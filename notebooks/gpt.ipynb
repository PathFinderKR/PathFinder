{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "36b2fa8101c30390"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:25.655663Z",
     "start_time": "2025-06-07T02:12:19.022133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from src.utils import set_seed, load_text, split_text, speedometer\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.tokenizer import CharTokenizer\n",
    "from models.GPT import GPT\n",
    "from src.train import Trainer"
   ],
   "id": "a38a265df86e4231",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/miniconda3/envs/torch-env/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:25.789482Z",
     "start_time": "2025-06-07T02:12:25.784006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")"
   ],
   "id": "70df4b9194f3773f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pathfinder/projects/PathFinder\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "1b632728e9a730d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:25.895059Z",
     "start_time": "2025-06-07T02:12:25.840560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=-1,\n",
    "    max_seq_len=128,\n",
    "    d_embed=128,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    d_head=32,\n",
    "    rank=32,\n",
    "    d_ff=512,\n",
    "    d_ff_multiple_of=64,\n",
    "    beta_min=1/2,\n",
    "    beta_max=4\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    model_name=\"nanoGPT\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    gradient_accumulation_steps=512 // 512,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-3,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=True,\n",
    "    matmul_precision=\"high\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    top_k=50\n",
    ")"
   ],
   "id": "7321faac2fc60c61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:26.903990Z",
     "start_time": "2025-06-07T02:12:25.947312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "78c21c905da1750e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/pathfinder/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "a5aea71474607392"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "1a5ca12f86ee8ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:26.926907Z",
     "start_time": "2025-06-07T02:12:26.920146Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "1aee2436650dc3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "4d7ae71c3abae1ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:27.204443Z",
     "start_time": "2025-06-07T02:12:27.030242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores\n",
    "print(f\"MatMul Precision: {train_config.matmul_precision}\")"
   ],
   "id": "6fd52c7cdf9b9fff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4080 SUPER\n",
      "MatMul Precision: high\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "881b3d83e0f9a02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:27.232529Z",
     "start_time": "2025-06-07T02:12:27.225122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/shakespeare.txt\")\n",
    "shakespeare_text = load_text(dataset_path)"
   ],
   "id": "3fb5b3cb4c7caae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /home/pathfinder/projects/PathFinder/datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:27.327009Z",
     "start_time": "2025-06-07T02:12:27.302935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    subset_shakespeare_text = shakespeare_text[:10000]\n",
    "    print(subset_shakespeare_text)\n",
    "    shakespeare_text = subset_shakespeare_text"
   ],
   "id": "891a7456f7efefb8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "73942bc06caadbf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:27.433083Z",
     "start_time": "2025-06-07T02:12:27.419171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "vocab_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/vocab.json\")\n",
    "char_tokenizer.save_vocab(vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "1422132939291d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n",
      "Vocabulary saved to /home/pathfinder/projects/PathFinder/datasets/Shakespeare/vocab.json.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:27.568921Z",
     "start_time": "2025-06-07T02:12:27.563727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "28702519b26b8516",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "3e19cdb93ce1cca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:27.711657Z",
     "start_time": "2025-06-07T02:12:27.704483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "369661fd35cc5408",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 1003854 characters\n",
      "Validation text length: 111540 characters\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:28.008708Z",
     "start_time": "2025-06-07T02:12:27.833444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "ade0a4f3f06bc47e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:28.042196Z",
     "start_time": "2025-06-07T02:12:28.035058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][0]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][0]}\")"
   ],
   "id": "bd0090591cadc29e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c7426d2c0a176973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:12:32.937756Z",
     "start_time": "2025-06-07T02:12:28.077923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_params() / 1e6:.2f}M\")"
   ],
   "id": "2d25545316ca49d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (token_embedding): Embedding(69, 128)\n",
      "    (positional_encoding): Embedding(128, 128)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (Wkv_down): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (Wkv_up): Linear(in_features=32, out_features=256, bias=False)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (lm_head): Linear(in_features=128, out_features=69, bias=False)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 0.72M\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "bbe1c76409605d58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:13:00.251141Z",
     "start_time": "2025-06-07T02:12:32.989223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "df06c866b6549d28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/pathfinder/projects/PathFinder/wandb/run-20250607_111232-3mcjl7eb</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/3mcjl7eb' target=\"_blank\">nanoGPT-2025-06-07_11-12-23</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/3mcjl7eb' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/3mcjl7eb</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 75/981 [00:25<00:50, 17.91it/s, epoch=1, grad_norm=0.2170, loss=2.4977, lr=0.003878] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      1\u001B[39m trainer = Trainer(\n\u001B[32m      2\u001B[39m     model=model,\n\u001B[32m      3\u001B[39m     train_config=train_config,\n\u001B[32m   (...)\u001B[39m\u001B[32m      7\u001B[39m     master_process=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m      8\u001B[39m )\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m trainer.train()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/PathFinder/src/train.py:95\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     91\u001B[39m step += \u001B[32m1\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.master_process:\n\u001B[32m     94\u001B[39m     wandb.log({\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mTrain Loss\u001B[39m\u001B[33m\"\u001B[39m: loss.item() * \u001B[38;5;28mself\u001B[39m.train_config.gradient_accumulation_steps,\n\u001B[32m     96\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mLearning Rate\u001B[39m\u001B[33m\"\u001B[39m: scheduler.get_last_lr()[\u001B[32m0\u001B[39m],\n\u001B[32m     97\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mGrad Norm\u001B[39m\u001B[33m\"\u001B[39m: grad_norm\n\u001B[32m     98\u001B[39m     })\n\u001B[32m     99\u001B[39m     progress_bar.set_postfix(\n\u001B[32m    100\u001B[39m         loss = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss.item()\u001B[38;5;250m \u001B[39m*\u001B[38;5;250m \u001B[39m\u001B[38;5;28mself\u001B[39m.train_config.gradient_accumulation_steps\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    101\u001B[39m         lr = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mscheduler.get_last_lr()[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    102\u001B[39m         grad_norm = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrad_norm\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    103\u001B[39m         epoch = epoch + \u001B[32m1\u001B[39m,\n\u001B[32m    104\u001B[39m     )\n\u001B[32m    105\u001B[39m     progress_bar.update(\u001B[32m1\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "df43a98cae309849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:13:00.400949200Z",
     "start_time": "2025-06-07T01:51:26.644899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save model locally\n",
    "if not train_config.debug:\n",
    "    output_dir = os.path.join(PROJECT_ROOT, \"checkpoints\", train_config.model_name, train_config.run_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        model.save_pretrained(\n",
    "            output_dir,\n",
    "            safe_serialization=True\n",
    "        )\n",
    "        print(\"Model saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    # Push to Hugging Face Hub\n",
    "    #model.push_to_hub(\n",
    "    #    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    #    private=True,\n",
    "    #    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    #)\n",
    "    #print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{train_config.model_name}-{train_config.run_name}\")"
   ],
   "id": "a587e8a215674e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:13:00.426453900Z",
     "start_time": "2025-06-07T01:51:26.692383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT(model_config)\n",
    "# model = model.from_pretrained(output_dir).to(device)"
   ],
   "id": "f3cad31e5a514fc0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "1ed82e9a71b4c9bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:13:00.482599200Z",
     "start_time": "2025-06-07T01:51:26.712312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k,\n",
    "    tokenizer=char_tokenizer\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "847724c2f13f7a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s,\n",
      "And yet was without a ready grantage, if we\n",
      "has either should the father.\n",
      "\n",
      "CORIOLANUSResetting KV cache\n",
      "IR MENENIUS:\n",
      "Go, good mother, give me, break upon't, I know,\n",
      "Were to his death commission me, be neared\n",
      "that I should persuive tResetting KV cache\n",
      ", my good are with in thee,\n",
      "And when and gives mine accounted of gentlemen,\n",
      "In drunk i' the favour honour in me.\n",
      "But I fear a goResetting KV cache\n",
      "ld hand an earth presence of he king,\n",
      "Some thought is the state, with a unbawd by his sacred:\n",
      "Where thou from wilt dead, and in Resetting KV cache\n",
      "prince a ktes, and yet some-envy\n",
      "Or silvers her state witness in unhonour,\n",
      "And then dike a holy of it eyes:\n",
      "'Twere secondle whicResetting KV cache\n",
      "ommitted\n",
      "To seek the close.\n",
      "\n",
      "Boy:\n",
      "Peace, this what is constinuated abothed\n",
      "With this boldness make thee to princely you,\n",
      "And balResetting KV cache\n",
      "oth and blessing evertains, to Berkeley,\n",
      "That seem hearts of night.\n",
      "\n",
      "LUCIO:\n",
      "And it is appeared, is not the court.\n",
      "\n",
      "LEONTES:\n",
      "GoodResetting KV cache\n",
      "ferous brother.\n",
      "Come, staypp'd your sand soul and his breast.\n",
      "\n",
      "CORIOLANUS:\n",
      "If thou dissenselenged wilt or a discomple,\n",
      "Your majeResetting KV cache\n",
      "r high\n",
      "Of this s"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:13:00.486384800Z",
     "start_time": "2025-06-07T01:51:31.955358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"🤖 Model Response:\")\n",
    "print(response)"
   ],
   "id": "1ea28687151601ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User prompt: \n",
      "To be, or not to be, that is the question\n",
      "--------------------------------------------------\n",
      "🤖 Model Response:\n",
      "To be, or not to be, that is the questions,\n",
      "And yet was without a ready grantage, if we\n",
      "has either should the father.\n",
      "\n",
      "CORIOLANUSIR MENENIUS:\n",
      "Go, good mother, give me, break upon't, I know,\n",
      "Were to his death commission me, be neared\n",
      "that I should persuive t, my good are with in thee,\n",
      "And when and gives mine accounted of gentlemen,\n",
      "In drunk i' the favour honour in me.\n",
      "But I fear a gold hand an earth presence of he king,\n",
      "Some thought is the state, with a unbawd by his sacred:\n",
      "Where thou from wilt dead, and in prince a ktes, and yet some-envy\n",
      "Or silvers her state witness in unhonour,\n",
      "And then dike a holy of it eyes:\n",
      "'Twere secondle whicommitted\n",
      "To seek the close.\n",
      "\n",
      "Boy:\n",
      "Peace, this what is constinuated abothed\n",
      "With this boldness make thee to princely you,\n",
      "And baloth and blessing evertains, to Berkeley,\n",
      "That seem hearts of night.\n",
      "\n",
      "LUCIO:\n",
      "And it is appeared, is not the court.\n",
      "\n",
      "LEONTES:\n",
      "Goodferous brother.\n",
      "Come, staypp'd your sand soul and his breast.\n",
      "\n",
      "CORIOLANUS:\n",
      "If thou dissenselenged wilt or a discomple,\n",
      "Your majer high\n",
      "Of this s\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speedometer",
   "id": "85c7993c75040a06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:13:00.489388900Z",
     "start_time": "2025-06-07T01:51:31.981592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=True,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "b2ce78011b613db2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV cache enabled: True\n",
      "Latency (Time per token): 3.25 ms\n",
      "Throughput (Tokens per second): 307.92\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:13:00.490391300Z",
     "start_time": "2025-06-07T01:51:33.944346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=False,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "297cc7c8a09ae162",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV cache enabled: False\n",
      "Latency (Time per token): 3.91 ms\n",
      "Throughput (Tokens per second): 255.63\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
