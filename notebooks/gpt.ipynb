{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "9c5839c9a3cf64fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@triton.jit\n",
    "def flash_decoding_stage_1(\n",
    "    Q, K, V,\n",
    "    FIXMAX, ACC, SUM,\n",
    "    stride_q_b, stride_q_h, stride_q_t, stride_q_d,\n",
    "    stride_k_b, stride_k_h, stride_k_t, stride_k_d,\n",
    "    stride_v_b, stride_v_h, stride_v_t, stride_v_d,\n",
    "    stride_m_b, stride_m_h,\n",
    "    stride_acc_b, stride_acc_h, stride_acc_d,\n",
    "    stride_sum_b, stride_sum_h,\n",
    "    B, H, T, D,\n",
    "    BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n",
    "):\n",
    "    pid_bh = tl.program_id(0)\n",
    "    pid_t  = tl.program_id(1)\n",
    "\n",
    "    off_h = pid_bh % H\n",
    "    off_z = pid_bh // H\n",
    "\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    offs_d = tl.arange(0, BLOCK_D)\n",
    "\n",
    "    q_ptrs = Q + off_z*stride_q_b + off_h*stride_q_h + offs_d*stride_q_d\n",
    "    k_base = K + off_z*stride_k_b + off_h*stride_k_h\n",
    "    v_base = V + off_z*stride_v_b + off_h*stride_v_h\n",
    "\n",
    "    # fixed max\n",
    "    max_ptr = FIXMAX + off_z*stride_m_b + off_h*stride_m_h\n",
    "    fixed_max = tl.load(max_ptr).to(tl.float32)\n",
    "\n",
    "    # tile range on T\n",
    "    start_n = pid_t * BLOCK_N\n",
    "    remain = T - start_n\n",
    "    block_size = tl.maximum(0, tl.minimum(remain, BLOCK_N))\n",
    "    mask_n = offs_n < block_size\n",
    "\n",
    "    # load q (fp32)\n",
    "    q = tl.load(q_ptrs, mask=offs_d < D, other=0.0).to(tl.float32)\n",
    "\n",
    "    # scale\n",
    "    scale = 1.0 / tl.sqrt(tl.full((), D, tl.float32))\n",
    "\n",
    "    # partial accumulators\n",
    "    part_acc = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    part_sum = tl.zeros((), dtype=tl.float32)\n",
    "\n",
    "    # load k,v tile (fp32)\n",
    "    k = tl.load(\n",
    "        k_base + (start_n + offs_n)[:, None]*stride_k_t + offs_d[None, :]*stride_k_d,\n",
    "        mask=mask_n[:, None] & (offs_d[None, :] < D),\n",
    "        other=0.0,\n",
    "    ).to(tl.float32)\n",
    "    v = tl.load(\n",
    "        v_base + (start_n + offs_n)[:, None]*stride_v_t + offs_d[None, :]*stride_v_d,\n",
    "        mask=mask_n[:, None] & (offs_d[None, :] < D),\n",
    "        other=0.0,\n",
    "    ).to(tl.float32)\n",
    "\n",
    "    # scores\n",
    "    scores = tl.sum(k * q[None, :], axis=1) * scale\n",
    "    neg_inf = tl.full(scores.shape, -float(\"inf\"), scores.dtype)\n",
    "    scores = tl.where(mask_n, scores, neg_inf)\n",
    "\n",
    "    # probs with fixed max\n",
    "    probs = tl.exp(scores - fixed_max)\n",
    "    probs = tl.where(mask_n, probs, 0.0)\n",
    "\n",
    "    # partial sums\n",
    "    part_sum += tl.sum(probs, axis=0)\n",
    "    part_acc += tl.sum(probs[:, None] * v, axis=0)\n",
    "\n",
    "    # atomic accumulate into ACC[B,H,D] and SUM[B,H]\n",
    "    acc_ptrs = ACC + off_z*stride_acc_b + off_h*stride_acc_h + offs_d*stride_acc_d\n",
    "    sum_ptr  = SUM + off_z*stride_sum_b + off_h*stride_sum_h\n",
    "\n",
    "    # only valid D range\n",
    "    tl.atomic_add(acc_ptrs, part_acc, mask=offs_d < D)\n",
    "    tl.atomic_add(sum_ptr,  part_sum)\n",
    "\n",
    "@triton.jit\n",
    "def flash_decoding_stage_2(\n",
    "    ACC, SUM,\n",
    "    O,\n",
    "    stride_acc_b, stride_acc_h, stride_acc_d,\n",
    "    stride_sum_b, stride_sum_h,\n",
    "    stride_o_b, stride_o_h, stride_o_t, stride_o_d,\n",
    "    B, H, D,\n",
    "    BLOCK_D: tl.constexpr\n",
    "):\n",
    "    pid_bh = tl.program_id(0)\n",
    "    off_h = pid_bh % H\n",
    "    off_z = pid_bh // H\n",
    "\n",
    "    offs_d = tl.arange(0, BLOCK_D)\n",
    "\n",
    "    acc_ptrs = ACC + off_z*stride_acc_b + off_h*stride_acc_h + offs_d*stride_acc_d\n",
    "    sum_ptr  = SUM + off_z*stride_sum_b + off_h*stride_sum_h\n",
    "    o_ptrs   = O   + off_z*stride_o_b   + off_h*stride_o_h   + offs_d*stride_o_d  # t=0\n",
    "\n",
    "    acc = tl.load(acc_ptrs, mask=offs_d < D, other=0.0)\n",
    "    s   = tl.load(sum_ptr)\n",
    "    eps = tl.full((), 1e-20, tl.float32)\n",
    "    denom = tl.maximum(s, eps)\n",
    "    out = acc / denom\n",
    "\n",
    "    tl.store(o_ptrs, out.to(O.dtype.element_ty), mask=offs_d < D)\n",
    "\n",
    "def flash_decoding_2(q, k, v, fixmax: float = 10):\n",
    "    B, H, _, D = q.shape\n",
    "    T = k.shape[2]\n",
    "    n_tiles: int = (T + BLOCK_N - 1) // BLOCK_N\n",
    "\n",
    "    ACC = torch.zeros((B, H, D), dtype=torch.float32, device=q.device)\n",
    "    SUM = torch.zeros((B, H),    dtype=torch.float32, device=q.device)\n",
    "\n",
    "    grid_partial = (B*H, n_tiles)\n",
    "    flash_decoding_stage_1[grid_partial](\n",
    "        q, k, v, fixmax, ACC, SUM,\n",
    "        # strides ... (ACC/SUM Ìè¨Ìï®),\n",
    "        B, H, T, D,\n",
    "        BLOCK_N=BLOCK_N, BLOCK_D=BLOCK_D,\n",
    "        num_warps=4, num_stages=2\n",
    "    )\n",
    "\n",
    "    o = torch.empty_like(q)\n",
    "    grid_finalize = (B*H,)\n",
    "    flash_decoding_stage_2[grid_finalize](\n",
    "        ACC, SUM, o,\n",
    "        # strides ...,\n",
    "        B, H, D,\n",
    "        BLOCK_D=BLOCK_D,\n",
    "        num_warps=1\n",
    "    )\n",
    "    return o\n",
    "\n",
    "@triton.jit\n",
    "def flash_decoding_2_kernel(\n",
    "        Q,     # [B, H, 1, D]\n",
    "        K, V,  # [B, H, T, D]\n",
    "        O,     # [B, H, 1, D]\n",
    "        FIXMAX,\n",
    "        stride_q_b, stride_q_h, stride_q_t, stride_q_d,\n",
    "        stride_k_b, stride_k_h, stride_k_t, stride_k_d,\n",
    "        stride_v_b, stride_v_h, stride_v_t, stride_v_d,\n",
    "        stride_o_b, stride_o_h, stride_o_t, stride_o_d,\n",
    "        stride_m_b, stride_m_h,\n",
    "        B, H, T, D,\n",
    "        BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n",
    "):\n",
    "    # Program IDs\n",
    "    off_hz = tl.program_id(0)\n",
    "    off_h = off_hz % H\n",
    "    off_z = off_hz // H\n",
    "\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    offs_d = tl.arange(0, BLOCK_D)\n",
    "\n",
    "    # Initialize pointers for this batch and head\n",
    "    q_ptrs = Q + off_z * stride_q_b + off_h * stride_q_h + offs_d * stride_q_d\n",
    "    k_ptrs = K + off_z * stride_k_b + off_h * stride_k_h + offs_n[:, None] * stride_k_t + offs_d[None, :] * stride_k_d\n",
    "    v_ptrs = V + off_z * stride_v_b + off_h * stride_v_h + offs_n[:, None] * stride_v_t + offs_d[None, :] * stride_v_d\n",
    "    o_ptrs = O + off_z * stride_o_b + off_h * stride_o_h + offs_d * stride_o_d\n",
    "\n",
    "    #\n",
    "    max_ptr = FIXMAX + off_z * stride_m_b + off_h * stride_m_h\n",
    "    fixed_max = tl.load(max_ptr, mask=tl.full((), True, tl.int1), other=0.0).to(tl.float32)\n",
    "\n",
    "    # Load single query vector\n",
    "    q = tl.load(q_ptrs, mask=offs_d < D, other=0.0).to(tl.float32)  # [BLOCK_D]\n",
    "\n",
    "    # Initialize output accumulator and softmax statistics\n",
    "    acc = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    sum_exp = tl.zeros((), dtype=tl.float32)\n",
    "\n",
    "    # Scale\n",
    "    scale = 1.0 / tl.sqrt(tl.full((), D, tl.float32))\n",
    "\n",
    "    # Loop over K, V blocks\n",
    "    start_n = 0\n",
    "    while start_n < T:\n",
    "        # Calculate current block bounds\n",
    "        remain = T - start_n\n",
    "        block_size = tl.minimum(remain, BLOCK_N)\n",
    "        mask_n = offs_n < block_size\n",
    "\n",
    "        # Load K, V blocks\n",
    "        k = tl.load(\n",
    "            k_ptrs + start_n * stride_k_t,\n",
    "            mask=mask_n[:, None] & (offs_d[None, :] < D),\n",
    "            other=0.0,\n",
    "        )  # [BLOCK_N, BLOCK_D]\n",
    "        v = tl.load(\n",
    "            v_ptrs + start_n * stride_v_t,\n",
    "            mask=mask_n[:, None] & (offs_d[None, :] < D),\n",
    "            other=0.0,\n",
    "        )  # [BLOCK_N, BLOCK_D]\n",
    "\n",
    "        # Compute attention scores: q @ K^T\n",
    "        scores = tl.sum(k * q[None, :], axis=1) * scale  # [BLOCK_N]\n",
    "\n",
    "        # Apply causal mask if needed (for decoding, usually all positions are valid)\n",
    "        scores = tl.where(mask_n, scores, -float('inf'))\n",
    "        neg_inf = tl.full(scores.shape, -float(\"inf\"), scores.dtype)\n",
    "        scores = tl.where(mask_n, scores, neg_inf)\n",
    "\n",
    "        # Compute probabilities for current block\n",
    "        probs = tl.exp(scores - fixed_max)  # [BLOCK_N]\n",
    "        probs = tl.where(mask_n, probs, 0.0)\n",
    "\n",
    "        # Update accumulator and sum\n",
    "        sum_exp += tl.sum(probs, axis=0)\n",
    "        acc += tl.sum(probs[:, None] * v, axis=0)  # [BLOCK_D]\n",
    "\n",
    "        start_n += BLOCK_N\n",
    "\n",
    "    # Final normalization\n",
    "    eps = tl.full((), 1e-20, tl.float32)\n",
    "    denom = tl.maximum(sum_exp, eps)\n",
    "    out = acc / denom\n",
    "\n",
    "    # Store output\n",
    "    tl.store(o_ptrs, out.to(O.dtype.element_ty), mask=offs_d < D)\n",
    "\n",
    "def flash_decoding_23(q, k, v, fixmax: float = 10):\n",
    "    \"\"\"\n",
    "    Flash Decoding 2 Triton Kernel\n",
    "\n",
    "    Args:\n",
    "        q: Query tensor [batch, n_heads, 1, d_head]\n",
    "        k: Key tensor   [batch, n_heads, seq_len_k, d_head]\n",
    "        v: Value tensor [batch, n_heads, seq_len_v, d_head]\n",
    "        fixmax:\n",
    "\n",
    "    Returns:\n",
    "        o: Output tensor [batch, n_heads, 1, d_head]\n",
    "    \"\"\"\n",
    "    assert q.ndim == 4 and k.ndim == 4 and v.ndim == 4\n",
    "    batch_size, n_heads, q_seq_len, d_head = q.shape\n",
    "    assert q_seq_len == 1, \"This kernel assumes q_len=1 (decode step).\"\n",
    "    kv_seq_len = k.shape[2]\n",
    "\n",
    "    q = q.contiguous()\n",
    "    k = k.contiguous()\n",
    "    v = v.contiguous()\n",
    "    fixmax = torch.full((batch_size, n_heads), float(fixmax), dtype=torch.float32, device=q.device)\n",
    "\n",
    "    o = torch.empty_like(q)\n",
    "    grid = (batch_size * n_heads,)\n",
    "\n",
    "    flash_decoding_2_kernel[grid](\n",
    "        q, k, v, o,\n",
    "        fixmax,\n",
    "        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  # Q strides [B, H, 1, D]\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  # K strides [B, H, N, D]\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  # V strides [B, H, N, D]\n",
    "        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  # O strides [B, H, 1, D]\n",
    "        fixmax.stride(0), fixmax.stride(1),\n",
    "        batch_size, n_heads, kv_seq_len, d_head,\n",
    "        BLOCK_N=BLOCK_N, BLOCK_D=BLOCK_D,\n",
    "        num_warps=4, num_stages=2\n",
    "    )\n",
    "    return o\n",
    "\n",
    "@triton.jit\n",
    "def flash_decoding_2_kernel(\n",
    "        Q,     # [B, H, 1, D]\n",
    "        K, V,  # [B, H, T, D]\n",
    "        O,     # [B, H, 1, D]\n",
    "        FIXMAX,\n",
    "        stride_q_b, stride_q_h, stride_q_t, stride_q_d,\n",
    "        stride_k_b, stride_k_h, stride_k_t, stride_k_d,\n",
    "        stride_v_b, stride_v_h, stride_v_t, stride_v_d,\n",
    "        stride_o_b, stride_o_h, stride_o_t, stride_o_d,\n",
    "        stride_m_b, stride_m_h,\n",
    "        B, H, T, D,\n",
    "        BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n",
    "):\n",
    "    # Program IDs\n",
    "    off_hz = tl.program_id(0)\n",
    "    off_h = off_hz % H\n",
    "    off_z = off_hz // H\n",
    "\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    offs_d = tl.arange(0, BLOCK_D)\n",
    "\n",
    "    # Initialize pointers for this batch and head\n",
    "    q_ptrs = Q + off_z * stride_q_b + off_h * stride_q_h + offs_d * stride_q_d\n",
    "    k_ptrs = K + off_z * stride_k_b + off_h * stride_k_h + offs_n[:, None] * stride_k_t + offs_d[None, :] * stride_k_d\n",
    "    v_ptrs = V + off_z * stride_v_b + off_h * stride_v_h + offs_n[:, None] * stride_v_t + offs_d[None, :] * stride_v_d\n",
    "    o_ptrs = O + off_z * stride_o_b + off_h * stride_o_h + offs_d * stride_o_d\n",
    "\n",
    "    #\n",
    "    max_ptr = FIXMAX + off_z * stride_m_b + off_h * stride_m_h\n",
    "    fixed_max = tl.load(max_ptr, mask=tl.full((), True, tl.int1), other=0.0).to(tl.float32)\n",
    "\n",
    "    # Load single query vector\n",
    "    q = tl.load(q_ptrs, mask=offs_d < D, other=0.0).to(tl.float32)  # [BLOCK_D]\n",
    "\n",
    "    # Initialize output accumulator and softmax statistics\n",
    "    acc = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    sum_exp = tl.zeros((), dtype=tl.float32)\n",
    "\n",
    "    # Scale\n",
    "    scale = 1.0 / tl.sqrt(tl.full((), D, tl.float32))\n",
    "\n",
    "    # Loop over K, V blocks\n",
    "    start_n = 0\n",
    "    while start_n < T:\n",
    "        # Calculate current block bounds\n",
    "        remain = T - start_n\n",
    "        block_size = tl.minimum(remain, BLOCK_N)\n",
    "        mask_n = offs_n < block_size\n",
    "\n",
    "        # Load K, V blocks\n",
    "        k = tl.load(\n",
    "            k_ptrs + start_n * stride_k_t,\n",
    "            mask=mask_n[:, None] & (offs_d[None, :] < D),\n",
    "            other=0.0,\n",
    "        )  # [BLOCK_N, BLOCK_D]\n",
    "        v = tl.load(\n",
    "            v_ptrs + start_n * stride_v_t,\n",
    "            mask=mask_n[:, None] & (offs_d[None, :] < D),\n",
    "            other=0.0,\n",
    "        )  # [BLOCK_N, BLOCK_D]\n",
    "\n",
    "        # Compute attention scores: q @ K^T\n",
    "        scores = tl.sum(k * q[None, :], axis=1) * scale  # [BLOCK_N]\n",
    "\n",
    "        # Apply causal mask if needed (for decoding, usually all positions are valid)\n",
    "        scores = tl.where(mask_n, scores, -float('inf'))\n",
    "        neg_inf = tl.full(scores.shape, -float(\"inf\"), scores.dtype)\n",
    "        scores = tl.where(mask_n, scores, neg_inf)\n",
    "\n",
    "        # Compute probabilities for current block\n",
    "        probs = tl.exp(scores - fixed_max)  # [BLOCK_N]\n",
    "        probs = tl.where(mask_n, probs, 0.0)\n",
    "\n",
    "        # Update accumulator and sum\n",
    "        sum_exp += tl.sum(probs, axis=0)\n",
    "        acc += tl.sum(probs[:, None] * v, axis=0)  # [BLOCK_D]\n",
    "\n",
    "        start_n += BLOCK_N\n",
    "\n",
    "    # Final normalization\n",
    "    eps = tl.full((), 1e-20, tl.float32)\n",
    "    denom = tl.maximum(sum_exp, eps)\n",
    "    out = acc / denom\n",
    "\n",
    "    # Store output\n",
    "    tl.store(o_ptrs, out.to(O.dtype.element_ty), mask=offs_d < D)\n",
    "\n",
    "def flash_decoding_2(q, k, v, fixmax: float = 10):\n",
    "    \"\"\"\n",
    "    Flash Decoding 2 Triton Kernel\n",
    "\n",
    "    Args:\n",
    "        q: Query tensor [batch, n_heads, 1, d_head]\n",
    "        k: Key tensor   [batch, n_heads, seq_len_k, d_head]\n",
    "        v: Value tensor [batch, n_heads, seq_len_v, d_head]\n",
    "        fixmax:\n",
    "\n",
    "    Returns:\n",
    "        o: Output tensor [batch, n_heads, 1, d_head]\n",
    "    \"\"\"\n",
    "    assert q.ndim == 4 and k.ndim == 4 and v.ndim == 4\n",
    "    batch_size, n_heads, q_seq_len, d_head = q.shape\n",
    "    assert q_seq_len == 1, \"This kernel assumes q_len=1 (decode step).\"\n",
    "    kv_seq_len = k.shape[2]\n",
    "\n",
    "    q = q.contiguous()\n",
    "    k = k.contiguous()\n",
    "    v = v.contiguous()\n",
    "    fixmax = torch.full((batch_size, n_heads), float(fixmax), dtype=torch.float32, device=q.device)\n",
    "\n",
    "    o = torch.empty_like(q)\n",
    "    grid = (batch_size * n_heads,)\n",
    "\n",
    "    flash_decoding_2_kernel[grid](\n",
    "        q, k, v, o,\n",
    "        fixmax,\n",
    "        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  # Q strides [B, H, 1, D]\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  # K strides [B, H, N, D]\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  # V strides [B, H, N, D]\n",
    "        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  # O strides [B, H, 1, D]\n",
    "        fixmax.stride(0), fixmax.stride(1),\n",
    "        batch_size, n_heads, kv_seq_len, d_head,\n",
    "        BLOCK_N=BLOCK_N, BLOCK_D=BLOCK_D,\n",
    "        num_warps=4, num_stages=2\n",
    "    )\n",
    "    return o"
   ],
   "id": "686169cc785c69bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:49.762471Z",
     "start_time": "2025-10-24T07:40:46.464456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.utils import set_seed, load_text, split_text\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.train import Trainer\n",
    "from tokenizer.tokenizer import CharTokenizer\n",
    "from models.GPT import GPT"
   ],
   "id": "46f01dcbb360429a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:49.770554Z",
     "start_time": "2025-10-24T07:40:49.766988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + \"/..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")"
   ],
   "id": "8d59d28379b9627c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pathfinder/projects/PathFinder\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "e1db9708d016dbff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:49.827335Z",
     "start_time": "2025-10-24T07:40:49.823408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=-1,\n",
    "    max_seq_len=128,\n",
    "    d_embed=256,\n",
    "    n_layers=4,\n",
    "    attn_type=\"MHA\",\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    attn_bias=False,\n",
    "    d_ff=1024,\n",
    "    mlp_bias=False,\n",
    "    flash=True,\n",
    "    flash_decode=True,\n",
    "    cla=False\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    model_name=\"nanoGPT\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    gradient_accumulation_steps=512 // 512,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    attn_decay=0.5,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=True,\n",
    "    matmul_precision=\"high\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    top_k=50\n",
    ")"
   ],
   "id": "40c1b49185fe925c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "c58f70d689c96eb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "f274b536c84b9fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:49.840370Z",
     "start_time": "2025-10-24T07:40:49.831105Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "589c9877d81b1ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "41576077c1495528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.043219Z",
     "start_time": "2025-10-24T07:40:49.901942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores\n",
    "print(f\"MatMul Precision: {train_config.matmul_precision}\")"
   ],
   "id": "4480ebea1f838033",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4080 SUPER\n",
      "MatMul Precision: high\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "3ce6a33cb70f30bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.050592Z",
     "start_time": "2025-10-24T07:40:50.047009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/shakespeare.txt\")\n",
    "shakespeare_text = load_text(dataset_path)"
   ],
   "id": "c8b140fbb518dd26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /home/pathfinder/projects/PathFinder/datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.115034Z",
     "start_time": "2025-10-24T07:40:50.112004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    subset_shakespeare_text = shakespeare_text[:10000]\n",
    "    print(subset_shakespeare_text)\n",
    "    shakespeare_text = subset_shakespeare_text"
   ],
   "id": "2d80cd5188573c48",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "8d61430be1c7c5eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.134526Z",
     "start_time": "2025-10-24T07:40:50.118304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "vocab_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/vocab.json\")\n",
    "char_tokenizer.save_vocab(vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "432c5fbe09cb4f09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n",
      "Vocabulary saved to /home/pathfinder/projects/PathFinder/datasets/Shakespeare/vocab.json.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.141315Z",
     "start_time": "2025-10-24T07:40:50.138538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "11c6e1486b065782",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "ca72c158757f0704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.149401Z",
     "start_time": "2025-10-24T07:40:50.146262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "9c950bd3950a08db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 1003854 characters\n",
      "Validation text length: 111540 characters\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.326717Z",
     "start_time": "2025-10-24T07:40:50.212262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        #\"attention_mask\": attention_mask,\n",
    "        \"target_ids\": target_ids\n",
    "    }\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "620a9b685b7bca7c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:50.333633Z",
     "start_time": "2025-10-24T07:40:50.330870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][0]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][0]}\")"
   ],
   "id": "a6a326932c2034e1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d680f9ad3de3eefb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:40:51.657607Z",
     "start_time": "2025-10-24T07:40:50.336568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.get_num_params() / 1e6:.2f}M\")"
   ],
   "id": "2a82b103029ecc60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (token_embedding): Embedding(69, 256)\n",
      "    (positional_encoding): Embedding(128, 256)\n",
      "    (dropout): Dropout(p=0.01, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (qkv_proj): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.01, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.01, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (lm_head): Linear(in_features=256, out_features=69, bias=False)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 3.20M\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "d016b0c6fe5f3be0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:42:29.636375Z",
     "start_time": "2025-10-24T07:40:51.752395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "57885df1d97b3537",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/pathfinder/projects/PathFinder/wandb/run-20251024_164053-jciah0p3</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/jciah0p3' target=\"_blank\">nanoGPT-2025-10-24_16-40-49</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/jciah0p3' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/jciah0p3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 981/981 [01:34<00:00, 10.41it/s, epoch=1, grad_norm=0.3982, loss=1.7068, lr=0.000000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "f694effe3a856723648b8c5192b863c6"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Learning Rate</td><td>‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Val Loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Val Perplexity</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>0.3982</td></tr><tr><td>Learning Rate</td><td>0</td></tr><tr><td>Train Loss</td><td>1.70679</td></tr><tr><td>Val Loss</td><td>1.82508</td></tr><tr><td>Val Perplexity</td><td>6.20332</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nanoGPT-2025-10-24_16-40-49</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/jciah0p3' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/jciah0p3</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>/home/pathfinder/projects/PathFinder/wandb/run-20251024_164053-jciah0p3/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "5fb3b0ae98408329"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:42:29.713542Z",
     "start_time": "2025-10-24T07:42:29.710388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not train_config.debug:\n",
    "    pass\n",
    "    #output_dir = os.path.join(PROJECT_ROOT, \"checkpoints\", train_config.model_name, train_config.run_name)\n",
    "    #os.makedirs(output_dir, exist_ok=True)\n",
    "    #try:\n",
    "    #    model.save_pretrained(\n",
    "    #        output_dir,\n",
    "    #        safe_serialization=True\n",
    "    #    )\n",
    "    #    print(\"Model saved successfully\")\n",
    "    #xcept Exception as e:\n",
    "    #    print(f\"Error saving model: {e}\")\n",
    "    # Push to Hugging Face Hub\n",
    "    #model.push_to_hub(\n",
    "    #    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    #    private=True,\n",
    "    #    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    #)\n",
    "    #print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{train_config.model_name}-{train_config.run_name}\")"
   ],
   "id": "aa7fc811417bd46",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:42:29.720317Z",
     "start_time": "2025-10-24T07:42:29.717492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT.from_pretrained(output_dir).to(device)"
   ],
   "id": "189947e80319f23e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "754c9ac783ec7ce0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:42:31.910843Z",
     "start_time": "2025-10-24T07:42:29.723451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k,\n",
    "    tokenizer=char_tokenizer\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "51651a0e0120012d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s,\n",
      "And your to firtend, if Sencer heaven is well.\n",
      "\n",
      "KING RICHORD III:\n",
      "Pain of the shall t\u001B[91mResetting KV cache\u001B[0m\n",
      " Rew he you cance ouble them;\n",
      "Was habjournd duder'sway raw, have it worn!\n",
      "\n",
      "Second Bussween:\n",
      "Duke I heads ware will suckers the d\u001B[91mResetting KV cache\u001B[0m\n",
      ", my to, you to at iny one she\n",
      "ould man; gim twith one the flom of his slackes drusp.\n",
      "\n",
      "GLOUCESTER:\n",
      "Are is kidagn! now heas your \u001B[91mResetting KV cache\u001B[0m\n",
      "line I'll and\n",
      "To prince. Boklie know, of you jicedince,\n",
      "Has thou was parn of my you, hearth follow;\n",
      "Or murnriently, choreding bo\u001B[91mResetting KV cache\u001B[0m\n",
      " Rihchard,\n",
      "With all sake thy his leasial marre'ds as a trands,\n",
      "Shull from foiticy nighter forget oald eyes:\n",
      "Wise with threow him\u001B[91mResetting KV cache\u001B[0m\n",
      "be compys,\n",
      "And work'd yours dayars titlent import,\n",
      "Not be timadgernable in will that his to came,\n",
      "Stand the sinchens own detuers\u001B[91mResetting KV cache\u001B[0m\n",
      " that Comeeliings! Andlif, who fiel!\n",
      "\n",
      "HENRY EDWARD ICK:\n",
      "Now me, my I everak deysitying preatent is suckes,\n",
      "On like it be slevoug\u001B[91mResetting KV cache\u001B[0m\n",
      "ue or hand, there movest tist\n",
      "Or be sand soul and of cure straior.\n",
      "\n",
      "YORK:\n",
      "I dist allive Leflend ut, goines, but so begieves\n",
      "One \u001B[91mResetting KV cache\u001B[0m\n",
      "for give mistres"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:42:31.980790Z",
     "start_time": "2025-10-24T07:42:31.977659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"ü§ñ Model Response:\")\n",
    "print(response)"
   ],
   "id": "6e66a6c36144f057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User prompt: \n",
      "To be, or not to be, that is the question\n",
      "--------------------------------------------------\n",
      "ü§ñ Model Response:\n",
      "re movest tist\n",
      "Or be sand soul and of cure straior.\n",
      "\n",
      "YORK:\n",
      "I dist allive Leflend ut, goines, but so begieves\n",
      "One for give mistres\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Profiling",
   "id": "df64b918376b3b29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    input_ids = torch.randint(0, model_config.vocab_size, (1, model_config.max_seq_len), device=device)\n",
    "    with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            model(input_ids)\n",
    "    print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cuda_time_total\", row_limit=20))"
   ],
   "id": "acc60cff49785962",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
