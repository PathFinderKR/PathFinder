{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "36b2fa8101c30390"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:29.100807Z",
     "start_time": "2025-06-13T11:00:25.469354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import wandb\n",
    "from src.utils import set_seed, load_text, split_text, speedometer\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.tokenizer import CharTokenizer\n",
    "from models.GPT import GPT\n",
    "from src.train import Trainer"
   ],
   "id": "a38a265df86e4231",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:29.241008Z",
     "start_time": "2025-06-13T11:00:29.238081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + \"/..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")"
   ],
   "id": "70df4b9194f3773f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /workspace/PathFinder\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "1b632728e9a730d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:29.446039Z",
     "start_time": "2025-06-13T11:00:29.440566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=-1,\n",
    "    max_seq_len=128,\n",
    "    d_embed=128,\n",
    "    n_layers=4,\n",
    "    flash=True,\n",
    "    n_heads=4,\n",
    "    d_head=32,\n",
    "    scale=32 ** -0.5,\n",
    "    #rank=16,\n",
    "    d_ff=512,\n",
    "    #d_ff_multiple_of=64,\n",
    "    #beta_min=1/2,\n",
    "    #beta_max=8\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    model_name=\"PathFinder-nano\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    gradient_accumulation_steps=512 // 512,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-3,\n",
    "    attn_decay=0.5,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=True,\n",
    "    matmul_precision=\"high\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    top_k=50\n",
    ")"
   ],
   "id": "7321faac2fc60c61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:30.383428Z",
     "start_time": "2025-06-13T11:00:29.621354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "78c21c905da1750e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "a5aea71474607392"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "1a5ca12f86ee8ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:30.743453Z",
     "start_time": "2025-06-13T11:00:30.738377Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "1aee2436650dc3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "4d7ae71c3abae1ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:30.923289Z",
     "start_time": "2025-06-13T11:00:30.808285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores\n",
    "print(f\"MatMul Precision: {train_config.matmul_precision}\")"
   ],
   "id": "6fd52c7cdf9b9fff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA RTX A6000\n",
      "MatMul Precision: high\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "881b3d83e0f9a02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:30.949655Z",
     "start_time": "2025-06-13T11:00:30.945237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/shakespeare.txt\")\n",
    "shakespeare_text = load_text(dataset_path)"
   ],
   "id": "3fb5b3cb4c7caae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /workspace/PathFinder/datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:31.019021Z",
     "start_time": "2025-06-13T11:00:31.016664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    subset_shakespeare_text = shakespeare_text[:10000]\n",
    "    print(subset_shakespeare_text)\n",
    "    shakespeare_text = subset_shakespeare_text"
   ],
   "id": "891a7456f7efefb8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "73942bc06caadbf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:31.190085Z",
     "start_time": "2025-06-13T11:00:31.180473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "vocab_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/vocab.json\")\n",
    "char_tokenizer.save_vocab(vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "1422132939291d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n",
      "Vocabulary saved to /workspace/PathFinder/datasets/Shakespeare/vocab.json.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:31.263694Z",
     "start_time": "2025-06-13T11:00:31.258800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "28702519b26b8516",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "3e19cdb93ce1cca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:31.340455Z",
     "start_time": "2025-06-13T11:00:31.334474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "369661fd35cc5408",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 1003854 characters\n",
      "Validation text length: 111540 characters\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:31.591814Z",
     "start_time": "2025-06-13T11:00:31.395896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "ade0a4f3f06bc47e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:00:31.716767Z",
     "start_time": "2025-06-13T11:00:31.711062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][0]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][0]}\")"
   ],
   "id": "bd0090591cadc29e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c7426d2c0a176973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:01:30.230016Z",
     "start_time": "2025-06-13T11:01:30.180693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "#model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_params() / 1e6:.2f}M\")"
   ],
   "id": "2d25545316ca49d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (token_embedding): Embedding(69, 128)\n",
      "  (positional_encoding): Embedding(128, 128)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiHeadAttention(\n",
      "        (qkv_proj): Linear(in_features=128, out_features=384, bias=False)\n",
      "        (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): FeedForward(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=128, out_features=69, bias=False)\n",
      ")\n",
      "Number of parameters: 0.81M\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "bbe1c76409605d58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:02:55.468398Z",
     "start_time": "2025-06-13T11:01:31.240344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "df06c866b6549d28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PathFinder-nano-2025-06-13_11-00-29</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/ce68zeq2' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/ce68zeq2</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>/workspace/PathFinder/wandb/run-20250613_110034-ce68zeq2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/PathFinder/wandb/run-20250613_110131-p07avk57</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/p07avk57' target=\"_blank\">PathFinder-nano-2025-06-13_11-00-29</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/p07avk57' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/p07avk57</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/981 [00:00<?, ?it/s]\u001B[A\n",
      "Training:   0%|          | 0/981 [00:01<?, ?it/s, epoch=1, grad_norm=1.6928, loss=4.7471, lr=0.000051]\u001B[A\n",
      "Training:   0%|          | 1/981 [00:01<19:28,  1.19s/it, epoch=1, grad_norm=1.6928, loss=4.7471, lr=0.000051]\u001B[A\n",
      "Training:   0%|          | 1/981 [00:01<19:28,  1.19s/it, epoch=1, grad_norm=1.6993, loss=4.7461, lr=0.000102]\u001B[A\n",
      "Training:   0%|          | 2/981 [00:01<19:27,  1.19s/it, epoch=1, grad_norm=1.7096, loss=4.7156, lr=0.000153]\u001B[A\n",
      "Training:   0%|          | 3/981 [00:01<05:44,  2.84it/s, epoch=1, grad_norm=1.7096, loss=4.7156, lr=0.000153]\u001B[A\n",
      "Training:   0%|          | 3/981 [00:01<05:44,  2.84it/s, epoch=1, grad_norm=1.7052, loss=4.6548, lr=0.000204]\u001B[A\n",
      "Training:   0%|          | 4/981 [00:01<05:44,  2.84it/s, epoch=1, grad_norm=1.7226, loss=4.5574, lr=0.000255]\u001B[A\n",
      "Training:   1%|          | 5/981 [00:01<03:14,  5.02it/s, epoch=1, grad_norm=1.7226, loss=4.5574, lr=0.000255]\u001B[A\n",
      "Training:   1%|          | 5/981 [00:01<03:14,  5.02it/s, epoch=1, grad_norm=1.7410, loss=4.4273, lr=0.000306]\u001B[A\n",
      "Training:   1%|          | 6/981 [00:01<03:14,  5.02it/s, epoch=1, grad_norm=1.7117, loss=4.2616, lr=0.000357]\u001B[A\n",
      "Training:   1%|          | 7/981 [00:01<02:14,  7.23it/s, epoch=1, grad_norm=1.7117, loss=4.2616, lr=0.000357]\u001B[A\n",
      "Training:   1%|          | 7/981 [00:01<02:14,  7.23it/s, epoch=1, grad_norm=1.6410, loss=4.0705, lr=0.000408]\u001B[A\n",
      "Training:   1%|          | 8/981 [00:01<02:14,  7.23it/s, epoch=1, grad_norm=1.4412, loss=3.8792, lr=0.000459]\u001B[A\n",
      "Training:   1%|          | 9/981 [00:01<01:43,  9.37it/s, epoch=1, grad_norm=1.4412, loss=3.8792, lr=0.000459]\u001B[A\n",
      "Training:   1%|          | 9/981 [00:01<01:43,  9.37it/s, epoch=1, grad_norm=1.2115, loss=3.7210, lr=0.000510]\u001B[A\n",
      "Training:   1%|          | 10/981 [00:01<01:43,  9.37it/s, epoch=1, grad_norm=1.0701, loss=3.5969, lr=0.000561]\u001B[A\n",
      "Training:   1%|          | 11/981 [00:01<01:25, 11.28it/s, epoch=1, grad_norm=1.0701, loss=3.5969, lr=0.000561]\u001B[A\n",
      "Training:   1%|          | 11/981 [00:01<01:25, 11.28it/s, epoch=1, grad_norm=0.9294, loss=3.5131, lr=0.000612]\u001B[A\n",
      "Training:   1%|          | 12/981 [00:01<01:25, 11.28it/s, epoch=1, grad_norm=0.7548, loss=3.4583, lr=0.000663]\u001B[A\n",
      "Training:   1%|▏         | 13/981 [00:01<01:14, 12.92it/s, epoch=1, grad_norm=0.7548, loss=3.4583, lr=0.000663]\u001B[A\n",
      "Training:   1%|▏         | 13/981 [00:01<01:14, 12.92it/s, epoch=1, grad_norm=0.6669, loss=3.4094, lr=0.000714]\u001B[A\n",
      "Training:   1%|▏         | 14/981 [00:01<01:14, 12.92it/s, epoch=1, grad_norm=0.6434, loss=3.3837, lr=0.000765]\u001B[A\n",
      "Training:   2%|▏         | 15/981 [00:01<01:07, 14.25it/s, epoch=1, grad_norm=0.6434, loss=3.3837, lr=0.000765]\u001B[A\n",
      "Training:   2%|▏         | 15/981 [00:02<01:07, 14.25it/s, epoch=1, grad_norm=0.6094, loss=3.3742, lr=0.000816]\u001B[A\n",
      "Training:   2%|▏         | 16/981 [00:02<01:07, 14.25it/s, epoch=1, grad_norm=0.5114, loss=3.3593, lr=0.000867]\u001B[A\n",
      "Training:   2%|▏         | 17/981 [00:02<01:03, 15.29it/s, epoch=1, grad_norm=0.5114, loss=3.3593, lr=0.000867]\u001B[A\n",
      "Training:   2%|▏         | 17/981 [00:02<01:03, 15.29it/s, epoch=1, grad_norm=0.3808, loss=3.3625, lr=0.000918]\u001B[A\n",
      "Training:   2%|▏         | 18/981 [00:02<01:02, 15.29it/s, epoch=1, grad_norm=0.2992, loss=3.3472, lr=0.000969]\u001B[A\n",
      "Training:   2%|▏         | 19/981 [00:02<00:59, 16.09it/s, epoch=1, grad_norm=0.2992, loss=3.3472, lr=0.000969]\u001B[A\n",
      "Training:   2%|▏         | 19/981 [00:02<00:59, 16.09it/s, epoch=1, grad_norm=0.3225, loss=3.3385, lr=0.001020]\u001B[A\n",
      "Training:   2%|▏         | 20/981 [00:02<00:59, 16.09it/s, epoch=1, grad_norm=0.3827, loss=3.3361, lr=0.001071]\u001B[A\n",
      "Training:   2%|▏         | 21/981 [00:02<00:57, 16.67it/s, epoch=1, grad_norm=0.3827, loss=3.3361, lr=0.001071]\u001B[A\n",
      "Training:   2%|▏         | 21/981 [00:02<00:57, 16.67it/s, epoch=1, grad_norm=0.3159, loss=3.3305, lr=0.001122]\u001B[A\n",
      "Training:   2%|▏         | 22/981 [00:02<00:57, 16.67it/s, epoch=1, grad_norm=0.2453, loss=3.3379, lr=0.001173]\u001B[A\n",
      "Training:   2%|▏         | 23/981 [00:02<00:55, 17.11it/s, epoch=1, grad_norm=0.2453, loss=3.3379, lr=0.001173]\u001B[A\n",
      "Training:   2%|▏         | 23/981 [00:02<00:55, 17.11it/s, epoch=1, grad_norm=0.2581, loss=3.3228, lr=0.001224]\u001B[A\n",
      "Training:   2%|▏         | 24/981 [00:02<00:55, 17.11it/s, epoch=1, grad_norm=0.2841, loss=3.3281, lr=0.001276]\u001B[A\n",
      "Training:   3%|▎         | 25/981 [00:02<00:54, 17.43it/s, epoch=1, grad_norm=0.2841, loss=3.3281, lr=0.001276]\u001B[A\n",
      "Training:   3%|▎         | 25/981 [00:02<00:54, 17.43it/s, epoch=1, grad_norm=0.2477, loss=3.3147, lr=0.001327]\u001B[A\n",
      "Training:   3%|▎         | 26/981 [00:02<00:54, 17.43it/s, epoch=1, grad_norm=0.1891, loss=3.3094, lr=0.001378]\u001B[A\n",
      "Training:   3%|▎         | 27/981 [00:02<00:54, 17.65it/s, epoch=1, grad_norm=0.1891, loss=3.3094, lr=0.001378]\u001B[A\n",
      "Training:   3%|▎         | 27/981 [00:02<00:54, 17.65it/s, epoch=1, grad_norm=0.2215, loss=3.3033, lr=0.001429]\u001B[A\n",
      "Training:   3%|▎         | 28/981 [00:02<00:53, 17.65it/s, epoch=1, grad_norm=0.2746, loss=3.3108, lr=0.001480]\u001B[A\n",
      "Training:   3%|▎         | 29/981 [00:02<00:53, 17.81it/s, epoch=1, grad_norm=0.2746, loss=3.3108, lr=0.001480]\u001B[A\n",
      "Training:   3%|▎         | 29/981 [00:02<00:53, 17.81it/s, epoch=1, grad_norm=0.2242, loss=3.2787, lr=0.001531]\u001B[A\n",
      "Training:   3%|▎         | 30/981 [00:02<00:53, 17.81it/s, epoch=1, grad_norm=0.3008, loss=3.2557, lr=0.001582]\u001B[A\n",
      "Training:   3%|▎         | 31/981 [00:02<00:52, 17.93it/s, epoch=1, grad_norm=0.3008, loss=3.2557, lr=0.001582]\u001B[A\n",
      "Training:   3%|▎         | 31/981 [00:02<00:52, 17.93it/s, epoch=1, grad_norm=0.4732, loss=3.2045, lr=0.001633]\u001B[A\n",
      "Training:   3%|▎         | 32/981 [00:02<00:52, 17.93it/s, epoch=1, grad_norm=0.8070, loss=3.1178, lr=0.001684]\u001B[A\n",
      "Training:   3%|▎         | 33/981 [00:02<00:52, 18.01it/s, epoch=1, grad_norm=0.8070, loss=3.1178, lr=0.001684]\u001B[A\n",
      "Training:   3%|▎         | 33/981 [00:03<00:52, 18.01it/s, epoch=1, grad_norm=0.9006, loss=3.0918, lr=0.001735]\u001B[A\n",
      "Training:   3%|▎         | 34/981 [00:03<00:52, 18.01it/s, epoch=1, grad_norm=0.6444, loss=3.0499, lr=0.001786]\u001B[A\n",
      "Training:   4%|▎         | 35/981 [00:03<00:52, 18.06it/s, epoch=1, grad_norm=0.6444, loss=3.0499, lr=0.001786]\u001B[A\n",
      "Training:   4%|▎         | 35/981 [00:03<00:52, 18.06it/s, epoch=1, grad_norm=0.7847, loss=3.0052, lr=0.001837]\u001B[A\n",
      "Training:   4%|▎         | 36/981 [00:03<00:52, 18.06it/s, epoch=1, grad_norm=0.5372, loss=2.9516, lr=0.001888]\u001B[A\n",
      "Training:   4%|▍         | 37/981 [00:03<00:52, 18.10it/s, epoch=1, grad_norm=0.5372, loss=2.9516, lr=0.001888]\u001B[A\n",
      "Training:   4%|▍         | 37/981 [00:03<00:52, 18.10it/s, epoch=1, grad_norm=0.6282, loss=2.9251, lr=0.001939]\u001B[A\n",
      "Training:   4%|▍         | 38/981 [00:03<00:52, 18.10it/s, epoch=1, grad_norm=0.5207, loss=2.8833, lr=0.001990]\u001B[A\n",
      "Training:   4%|▍         | 39/981 [00:03<00:51, 18.13it/s, epoch=1, grad_norm=0.5207, loss=2.8833, lr=0.001990]\u001B[A\n",
      "Training:   4%|▍         | 39/981 [00:03<00:51, 18.13it/s, epoch=1, grad_norm=0.3677, loss=2.8414, lr=0.002041]\u001B[A\n",
      "Training:   4%|▍         | 40/981 [00:03<00:51, 18.13it/s, epoch=1, grad_norm=0.4332, loss=2.8172, lr=0.002092]\u001B[A\n",
      "Training:   4%|▍         | 41/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.4332, loss=2.8172, lr=0.002092]\u001B[A\n",
      "Training:   4%|▍         | 41/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.3605, loss=2.7876, lr=0.002143]\u001B[A\n",
      "Training:   4%|▍         | 42/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.4300, loss=2.7656, lr=0.002194]\u001B[A\n",
      "Training:   4%|▍         | 43/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.4300, loss=2.7656, lr=0.002194]\u001B[A\n",
      "Training:   4%|▍         | 43/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.3683, loss=2.7377, lr=0.002245]\u001B[A\n",
      "Training:   4%|▍         | 44/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.2882, loss=2.7252, lr=0.002296]\u001B[A\n",
      "Training:   5%|▍         | 45/981 [00:03<00:51, 18.16it/s, epoch=1, grad_norm=0.2882, loss=2.7252, lr=0.002296]\u001B[A\n",
      "Training:   5%|▍         | 45/981 [00:03<00:51, 18.16it/s, epoch=1, grad_norm=0.2846, loss=2.6990, lr=0.002347]\u001B[A\n",
      "Training:   5%|▍         | 46/981 [00:03<00:51, 18.16it/s, epoch=1, grad_norm=0.2330, loss=2.6832, lr=0.002398]\u001B[A\n",
      "Training:   5%|▍         | 47/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.2330, loss=2.6832, lr=0.002398]\u001B[A\n",
      "Training:   5%|▍         | 47/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.3128, loss=2.6675, lr=0.002449]\u001B[A\n",
      "Training:   5%|▍         | 48/981 [00:03<00:51, 18.15it/s, epoch=1, grad_norm=0.2160, loss=2.6555, lr=0.002500]\u001B[A\n",
      "Training:   5%|▍         | 49/981 [00:03<00:51, 18.16it/s, epoch=1, grad_norm=0.2160, loss=2.6555, lr=0.002500]\u001B[A\n",
      "Training:   5%|▍         | 49/981 [00:03<00:51, 18.16it/s, epoch=1, grad_norm=0.2499, loss=2.6506, lr=0.002551]\u001B[A\n",
      "Training:   5%|▌         | 50/981 [00:03<00:51, 18.16it/s, epoch=1, grad_norm=0.1988, loss=2.6326, lr=0.002602]\u001B[A\n",
      "Training:   5%|▌         | 51/981 [00:03<00:51, 18.16it/s, epoch=1, grad_norm=0.1988, loss=2.6326, lr=0.002602]\u001B[A\n",
      "Training:   5%|▌         | 51/981 [00:04<00:51, 18.16it/s, epoch=1, grad_norm=0.2606, loss=2.6253, lr=0.002653]\u001B[A\n",
      "Training:   5%|▌         | 52/981 [00:04<00:51, 18.16it/s, epoch=1, grad_norm=0.1983, loss=2.6131, lr=0.002704]\u001B[A\n",
      "Training:   5%|▌         | 53/981 [00:04<00:51, 18.16it/s, epoch=1, grad_norm=0.1983, loss=2.6131, lr=0.002704]\u001B[A\n",
      "Training:   5%|▌         | 53/981 [00:04<00:51, 18.16it/s, epoch=1, grad_norm=0.2347, loss=2.6043, lr=0.002755]\u001B[A\n",
      "Training:   6%|▌         | 54/981 [00:04<00:51, 18.16it/s, epoch=1, grad_norm=0.1641, loss=2.5959, lr=0.002806]\u001B[A\n",
      "Training:   6%|▌         | 55/981 [00:04<00:51, 17.96it/s, epoch=1, grad_norm=0.1641, loss=2.5959, lr=0.002806]\u001B[A\n",
      "Training:   6%|▌         | 55/981 [00:04<00:51, 17.96it/s, epoch=1, grad_norm=0.2095, loss=2.5882, lr=0.002857]\u001B[A\n",
      "Training:   6%|▌         | 56/981 [00:04<00:51, 17.96it/s, epoch=1, grad_norm=0.1739, loss=2.5789, lr=0.002908]\u001B[A\n",
      "Training:   6%|▌         | 57/981 [00:04<00:51, 17.80it/s, epoch=1, grad_norm=0.1739, loss=2.5789, lr=0.002908]\u001B[A\n",
      "Training:   6%|▌         | 57/981 [00:04<00:51, 17.80it/s, epoch=1, grad_norm=0.1706, loss=2.5763, lr=0.002959]\u001B[A\n",
      "Training:   6%|▌         | 58/981 [00:04<00:51, 17.80it/s, epoch=1, grad_norm=0.2007, loss=2.5635, lr=0.003010]\u001B[A\n",
      "Training:   6%|▌         | 59/981 [00:04<00:52, 17.72it/s, epoch=1, grad_norm=0.2007, loss=2.5635, lr=0.003010]\u001B[A\n",
      "Training:   6%|▌         | 59/981 [00:04<00:52, 17.72it/s, epoch=1, grad_norm=0.1394, loss=2.5615, lr=0.003061]\u001B[A\n",
      "Training:   6%|▌         | 60/981 [00:04<00:51, 17.72it/s, epoch=1, grad_norm=0.1814, loss=2.5530, lr=0.003112]\u001B[A\n",
      "Training:   6%|▌         | 61/981 [00:04<00:52, 17.64it/s, epoch=1, grad_norm=0.1814, loss=2.5530, lr=0.003112]\u001B[A\n",
      "Training:   6%|▌         | 61/981 [00:04<00:52, 17.64it/s, epoch=1, grad_norm=0.1890, loss=2.5543, lr=0.003163]\u001B[A\n",
      "Training:   6%|▋         | 62/981 [00:04<00:52, 17.64it/s, epoch=1, grad_norm=0.1571, loss=2.5467, lr=0.003214]\u001B[A\n",
      "Training:   6%|▋         | 63/981 [00:04<00:52, 17.54it/s, epoch=1, grad_norm=0.1571, loss=2.5467, lr=0.003214]\u001B[A\n",
      "Training:   6%|▋         | 63/981 [00:04<00:52, 17.54it/s, epoch=1, grad_norm=0.1376, loss=2.5396, lr=0.003265]\u001B[A\n",
      "Training:   7%|▋         | 64/981 [00:04<00:52, 17.54it/s, epoch=1, grad_norm=0.2064, loss=2.5273, lr=0.003316]\u001B[A\n",
      "Training:   7%|▋         | 65/981 [00:04<00:52, 17.47it/s, epoch=1, grad_norm=0.2064, loss=2.5273, lr=0.003316]\u001B[A\n",
      "Training:   7%|▋         | 65/981 [00:04<00:52, 17.47it/s, epoch=1, grad_norm=0.3217, loss=2.5224, lr=0.003367]\u001B[A\n",
      "Training:   7%|▋         | 66/981 [00:04<00:52, 17.47it/s, epoch=1, grad_norm=0.3729, loss=2.5245, lr=0.003418]\u001B[A\n",
      "Training:   7%|▋         | 67/981 [00:04<00:52, 17.45it/s, epoch=1, grad_norm=0.3729, loss=2.5245, lr=0.003418]\u001B[A\n",
      "Training:   7%|▋         | 67/981 [00:04<00:52, 17.45it/s, epoch=1, grad_norm=0.3101, loss=2.5249, lr=0.003469]\u001B[A\n",
      "Training:   7%|▋         | 68/981 [00:04<00:52, 17.45it/s, epoch=1, grad_norm=0.1862, loss=2.5050, lr=0.003520]\u001B[A\n",
      "Training:   7%|▋         | 69/981 [00:04<00:52, 17.46it/s, epoch=1, grad_norm=0.1862, loss=2.5050, lr=0.003520]\u001B[A\n",
      "Training:   7%|▋         | 69/981 [00:05<00:52, 17.46it/s, epoch=1, grad_norm=0.2229, loss=2.5035, lr=0.003571]\u001B[A\n",
      "Training:   7%|▋         | 70/981 [00:05<00:52, 17.46it/s, epoch=1, grad_norm=0.3637, loss=2.5225, lr=0.003622]\u001B[A\n",
      "Training:   7%|▋         | 71/981 [00:05<00:52, 17.43it/s, epoch=1, grad_norm=0.3637, loss=2.5225, lr=0.003622]\u001B[A\n",
      "Training:   7%|▋         | 71/981 [00:05<00:52, 17.43it/s, epoch=1, grad_norm=0.2598, loss=2.5143, lr=0.003673]\u001B[A\n",
      "Training:   7%|▋         | 72/981 [00:05<00:52, 17.43it/s, epoch=1, grad_norm=0.2508, loss=2.4918, lr=0.003724]\u001B[A\n",
      "Training:   7%|▋         | 73/981 [00:05<00:51, 17.46it/s, epoch=1, grad_norm=0.2508, loss=2.4918, lr=0.003724]\u001B[A\n",
      "Training:   7%|▋         | 73/981 [00:05<00:51, 17.46it/s, epoch=1, grad_norm=0.3644, loss=2.5006, lr=0.003776]\u001B[A\n",
      "Training:   8%|▊         | 74/981 [00:05<00:51, 17.46it/s, epoch=1, grad_norm=0.2778, loss=2.4953, lr=0.003827]\u001B[A\n",
      "Training:   8%|▊         | 75/981 [00:05<00:51, 17.46it/s, epoch=1, grad_norm=0.2778, loss=2.4953, lr=0.003827]\u001B[A\n",
      "Training:   8%|▊         | 75/981 [00:05<00:51, 17.46it/s, epoch=1, grad_norm=0.2203, loss=2.4918, lr=0.003878]\u001B[A\n",
      "Training:   8%|▊         | 76/981 [00:05<00:51, 17.46it/s, epoch=1, grad_norm=0.1385, loss=2.4726, lr=0.003929]\u001B[A\n",
      "Training:   8%|▊         | 77/981 [00:05<00:51, 17.43it/s, epoch=1, grad_norm=0.1385, loss=2.4726, lr=0.003929]\u001B[A\n",
      "Training:   8%|▊         | 77/981 [00:05<00:51, 17.43it/s, epoch=1, grad_norm=0.2598, loss=2.4829, lr=0.003980]\u001B[A\n",
      "Training:   8%|▊         | 78/981 [00:05<00:51, 17.43it/s, epoch=1, grad_norm=0.4635, loss=2.4784, lr=0.004031]\u001B[A\n",
      "Training:   8%|▊         | 79/981 [00:05<00:51, 17.39it/s, epoch=1, grad_norm=0.4635, loss=2.4784, lr=0.004031]\u001B[A\n",
      "Training:   8%|▊         | 79/981 [00:05<00:51, 17.39it/s, epoch=1, grad_norm=0.5250, loss=2.4771, lr=0.004082]\u001B[A\n",
      "Training:   8%|▊         | 80/981 [00:05<00:51, 17.39it/s, epoch=1, grad_norm=0.3678, loss=2.4704, lr=0.004133]\u001B[A\n",
      "Training:   8%|▊         | 81/981 [00:05<00:51, 17.36it/s, epoch=1, grad_norm=0.3678, loss=2.4704, lr=0.004133]\u001B[A\n",
      "Training:   8%|▊         | 81/981 [00:05<00:51, 17.36it/s, epoch=1, grad_norm=0.3167, loss=2.4703, lr=0.004184]\u001B[A\n",
      "Training:   8%|▊         | 82/981 [00:05<00:51, 17.36it/s, epoch=1, grad_norm=0.2909, loss=2.4620, lr=0.004235]\u001B[A\n",
      "Training:   8%|▊         | 83/981 [00:05<00:51, 17.36it/s, epoch=1, grad_norm=0.2909, loss=2.4620, lr=0.004235]\u001B[A\n",
      "Training:   8%|▊         | 83/981 [00:05<00:51, 17.36it/s, epoch=1, grad_norm=0.2340, loss=2.4557, lr=0.004286]\u001B[A\n",
      "Training:   9%|▊         | 84/981 [00:05<00:51, 17.36it/s, epoch=1, grad_norm=0.2381, loss=2.4561, lr=0.004337]\u001B[A\n",
      "Training:   9%|▊         | 85/981 [00:05<00:51, 17.42it/s, epoch=1, grad_norm=0.2381, loss=2.4561, lr=0.004337]\u001B[A\n",
      "Training:   9%|▊         | 85/981 [00:05<00:51, 17.42it/s, epoch=1, grad_norm=0.2504, loss=2.4450, lr=0.004388]\u001B[A\n",
      "Training:   9%|▉         | 86/981 [00:06<00:51, 17.42it/s, epoch=1, grad_norm=0.2467, loss=2.4410, lr=0.004439]\u001B[A\n",
      "Training:   9%|▉         | 87/981 [00:06<00:51, 17.48it/s, epoch=1, grad_norm=0.2467, loss=2.4410, lr=0.004439]\u001B[A\n",
      "Training:   9%|▉         | 87/981 [00:06<00:51, 17.48it/s, epoch=1, grad_norm=0.2814, loss=2.4476, lr=0.004490]\u001B[A\n",
      "Training:   9%|▉         | 88/981 [00:06<00:51, 17.48it/s, epoch=1, grad_norm=0.1972, loss=2.4305, lr=0.004541]\u001B[A\n",
      "Training:   9%|▉         | 89/981 [00:06<00:51, 17.45it/s, epoch=1, grad_norm=0.1972, loss=2.4305, lr=0.004541]\u001B[A\n",
      "Training:   9%|▉         | 89/981 [00:06<00:51, 17.45it/s, epoch=1, grad_norm=0.2418, loss=2.4263, lr=0.004592]\u001B[A\n",
      "Training:   9%|▉         | 90/981 [00:06<00:51, 17.45it/s, epoch=1, grad_norm=0.4323, loss=2.4259, lr=0.004643]\u001B[A\n",
      "Training:   9%|▉         | 91/981 [00:06<00:50, 17.47it/s, epoch=1, grad_norm=0.4323, loss=2.4259, lr=0.004643]\u001B[A\n",
      "Training:   9%|▉         | 91/981 [00:06<00:50, 17.47it/s, epoch=1, grad_norm=1.1996, loss=2.5049, lr=0.004694]\u001B[A\n",
      "Training:   9%|▉         | 92/981 [00:06<00:50, 17.47it/s, epoch=1, grad_norm=0.7145, loss=2.4567, lr=0.004745]\u001B[A\n",
      "Training:   9%|▉         | 93/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.7145, loss=2.4567, lr=0.004745]\u001B[A\n",
      "Training:   9%|▉         | 93/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.5098, loss=2.4474, lr=0.004796]\u001B[A\n",
      "Training:  10%|▉         | 94/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.4790, loss=2.4367, lr=0.004847]\u001B[A\n",
      "Training:  10%|▉         | 95/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.4790, loss=2.4367, lr=0.004847]\u001B[A\n",
      "Training:  10%|▉         | 95/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.4149, loss=2.4334, lr=0.004898]\u001B[A\n",
      "Training:  10%|▉         | 96/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.2766, loss=2.4274, lr=0.004949]\u001B[A\n",
      "Training:  10%|▉         | 97/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.2766, loss=2.4274, lr=0.004949]\u001B[A\n",
      "Training:  10%|▉         | 97/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.3659, loss=2.4215, lr=0.005000]\u001B[A\n",
      "Training:  10%|▉         | 98/981 [00:06<00:50, 17.45it/s, epoch=1, grad_norm=0.2773, loss=2.4201, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 99/981 [00:06<00:50, 17.47it/s, epoch=1, grad_norm=0.2773, loss=2.4201, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 99/981 [00:06<00:50, 17.47it/s, epoch=1, grad_norm=0.2413, loss=2.4128, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 100/981 [00:09<00:50, 17.47it/s, epoch=1, grad_norm=0.2736, loss=2.4148, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 101/981 [00:09<05:54,  2.48it/s, epoch=1, grad_norm=0.2736, loss=2.4148, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 101/981 [00:09<05:54,  2.48it/s, epoch=1, grad_norm=0.1973, loss=2.3963, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 102/981 [00:09<05:54,  2.48it/s, epoch=1, grad_norm=0.2932, loss=2.3876, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 103/981 [00:09<04:23,  3.33it/s, epoch=1, grad_norm=0.2932, loss=2.3876, lr=0.005000]\u001B[A\n",
      "Training:  10%|█         | 103/981 [00:09<04:23,  3.33it/s, epoch=1, grad_norm=0.2822, loss=2.3905, lr=0.004999]\u001B[A\n",
      "Training:  11%|█         | 104/981 [00:09<04:23,  3.33it/s, epoch=1, grad_norm=0.2520, loss=2.3790, lr=0.004999]\u001B[A\n",
      "Training:  11%|█         | 105/981 [00:09<03:19,  4.40it/s, epoch=1, grad_norm=0.2520, loss=2.3790, lr=0.004999]\u001B[A\n",
      "Training:  11%|█         | 105/981 [00:09<03:19,  4.40it/s, epoch=1, grad_norm=0.3740, loss=2.3753, lr=0.004999]\u001B[A\n",
      "Training:  11%|█         | 106/981 [00:09<03:18,  4.40it/s, epoch=1, grad_norm=0.5350, loss=2.3836, lr=0.004999]\u001B[A\n",
      "Training:  11%|█         | 107/981 [00:09<02:33,  5.68it/s, epoch=1, grad_norm=0.5350, loss=2.3836, lr=0.004999]\u001B[A\n",
      "Training:  11%|█         | 107/981 [00:09<02:33,  5.68it/s, epoch=1, grad_norm=0.4520, loss=2.3586, lr=0.004998]\u001B[A\n",
      "Training:  11%|█         | 108/981 [00:09<02:33,  5.68it/s, epoch=1, grad_norm=0.3502, loss=2.3588, lr=0.004998]\u001B[A\n",
      "Training:  11%|█         | 109/981 [00:09<02:02,  7.13it/s, epoch=1, grad_norm=0.3502, loss=2.3588, lr=0.004998]\u001B[A\n",
      "Training:  11%|█         | 109/981 [00:09<02:02,  7.13it/s, epoch=1, grad_norm=0.1719, loss=2.3417, lr=0.004998]\u001B[A\n",
      "Training:  11%|█         | 110/981 [00:09<02:02,  7.13it/s, epoch=1, grad_norm=0.3511, loss=2.3381, lr=0.004997]\u001B[A\n",
      "Training:  11%|█▏        | 111/981 [00:09<01:40,  8.66it/s, epoch=1, grad_norm=0.3511, loss=2.3381, lr=0.004997]\u001B[A\n",
      "Training:  11%|█▏        | 111/981 [00:09<01:40,  8.66it/s, epoch=1, grad_norm=0.6219, loss=2.3380, lr=0.004997]\u001B[A\n",
      "Training:  11%|█▏        | 112/981 [00:09<01:40,  8.66it/s, epoch=1, grad_norm=0.9493, loss=2.3424, lr=0.004996]\u001B[A\n",
      "Training:  12%|█▏        | 113/981 [00:09<01:25, 10.20it/s, epoch=1, grad_norm=0.9493, loss=2.3424, lr=0.004996]\u001B[A\n",
      "Training:  12%|█▏        | 113/981 [00:09<01:25, 10.20it/s, epoch=1, grad_norm=0.6065, loss=2.3552, lr=0.004996]\u001B[A\n",
      "Training:  12%|█▏        | 114/981 [00:09<01:24, 10.20it/s, epoch=1, grad_norm=0.4896, loss=2.3403, lr=0.004995]\u001B[A\n",
      "Training:  12%|█▏        | 115/981 [00:09<01:14, 11.66it/s, epoch=1, grad_norm=0.4896, loss=2.3403, lr=0.004995]\u001B[A\n",
      "Training:  12%|█▏        | 115/981 [00:09<01:14, 11.66it/s, epoch=1, grad_norm=0.5306, loss=2.3387, lr=0.004995]\u001B[A\n",
      "Training:  12%|█▏        | 116/981 [00:10<01:14, 11.66it/s, epoch=1, grad_norm=0.5165, loss=2.3207, lr=0.004994]\u001B[A\n",
      "Training:  12%|█▏        | 117/981 [00:10<01:06, 12.97it/s, epoch=1, grad_norm=0.5165, loss=2.3207, lr=0.004994]\u001B[A\n",
      "Training:  12%|█▏        | 117/981 [00:10<01:06, 12.97it/s, epoch=1, grad_norm=0.4349, loss=2.3199, lr=0.004994]\u001B[A\n",
      "Training:  12%|█▏        | 118/981 [00:10<01:06, 12.97it/s, epoch=1, grad_norm=0.3087, loss=2.3095, lr=0.004993]\u001B[A\n",
      "Training:  12%|█▏        | 119/981 [00:10<01:01, 14.06it/s, epoch=1, grad_norm=0.3087, loss=2.3095, lr=0.004993]\u001B[A\n",
      "Training:  12%|█▏        | 119/981 [00:10<01:01, 14.06it/s, epoch=1, grad_norm=0.2753, loss=2.2966, lr=0.004992]\u001B[A\n",
      "Training:  12%|█▏        | 120/981 [00:10<01:01, 14.06it/s, epoch=1, grad_norm=0.2930, loss=2.2865, lr=0.004992]\u001B[A\n",
      "Training:  12%|█▏        | 121/981 [00:10<00:57, 14.96it/s, epoch=1, grad_norm=0.2930, loss=2.2865, lr=0.004992]\u001B[A\n",
      "Training:  12%|█▏        | 121/981 [00:10<00:57, 14.96it/s, epoch=1, grad_norm=0.3148, loss=2.2819, lr=0.004991]\u001B[A\n",
      "Training:  12%|█▏        | 122/981 [00:10<00:57, 14.96it/s, epoch=1, grad_norm=0.2575, loss=2.2703, lr=0.004990]\u001B[A\n",
      "Training:  13%|█▎        | 123/981 [00:10<00:54, 15.63it/s, epoch=1, grad_norm=0.2575, loss=2.2703, lr=0.004990]\u001B[A\n",
      "Training:  13%|█▎        | 123/981 [00:10<00:54, 15.63it/s, epoch=1, grad_norm=0.2256, loss=2.2596, lr=0.004989]\u001B[A\n",
      "Training:  13%|█▎        | 124/981 [00:10<00:54, 15.63it/s, epoch=1, grad_norm=0.2542, loss=2.2556, lr=0.004988]\u001B[A\n",
      "Training:  13%|█▎        | 125/981 [00:10<00:53, 16.15it/s, epoch=1, grad_norm=0.2542, loss=2.2556, lr=0.004988]\u001B[A\n",
      "Training:  13%|█▎        | 125/981 [00:10<00:53, 16.15it/s, epoch=1, grad_norm=0.2869, loss=2.2461, lr=0.004988]\u001B[A\n",
      "Training:  13%|█▎        | 126/981 [00:10<00:52, 16.15it/s, epoch=1, grad_norm=0.3316, loss=2.2364, lr=0.004987]\u001B[A\n",
      "Training:  13%|█▎        | 127/981 [00:10<00:51, 16.49it/s, epoch=1, grad_norm=0.3316, loss=2.2364, lr=0.004987]\u001B[A\n",
      "Training:  13%|█▎        | 127/981 [00:10<00:51, 16.49it/s, epoch=1, grad_norm=0.3234, loss=2.2366, lr=0.004986]\u001B[A\n",
      "Training:  13%|█▎        | 128/981 [00:10<00:51, 16.49it/s, epoch=1, grad_norm=0.2895, loss=2.2195, lr=0.004985]\u001B[A\n",
      "Training:  13%|█▎        | 129/981 [00:10<00:50, 16.76it/s, epoch=1, grad_norm=0.2895, loss=2.2195, lr=0.004985]\u001B[A\n",
      "Training:  13%|█▎        | 129/981 [00:10<00:50, 16.76it/s, epoch=1, grad_norm=0.2191, loss=2.2222, lr=0.004984]\u001B[A\n",
      "Training:  13%|█▎        | 130/981 [00:10<00:50, 16.76it/s, epoch=1, grad_norm=0.2172, loss=2.2113, lr=0.004983]\u001B[A\n",
      "Training:  13%|█▎        | 131/981 [00:10<00:50, 16.99it/s, epoch=1, grad_norm=0.2172, loss=2.2113, lr=0.004983]\u001B[A\n",
      "Training:  13%|█▎        | 131/981 [00:10<00:50, 16.99it/s, epoch=1, grad_norm=0.3223, loss=2.2012, lr=0.004982]\u001B[A\n",
      "Training:  13%|█▎        | 132/981 [00:10<00:49, 16.99it/s, epoch=1, grad_norm=0.5406, loss=2.2133, lr=0.004981]\u001B[A\n",
      "Training:  14%|█▎        | 133/981 [00:10<00:49, 17.13it/s, epoch=1, grad_norm=0.5406, loss=2.2133, lr=0.004981]\u001B[A\n",
      "Training:  14%|█▎        | 133/981 [00:11<00:49, 17.13it/s, epoch=1, grad_norm=0.4953, loss=2.2167, lr=0.004980]\u001B[A\n",
      "Training:  14%|█▎        | 134/981 [00:11<00:49, 17.13it/s, epoch=1, grad_norm=0.5059, loss=2.1997, lr=0.004978]\u001B[A\n",
      "Training:  14%|█▍        | 135/981 [00:11<00:49, 17.26it/s, epoch=1, grad_norm=0.5059, loss=2.1997, lr=0.004978]\u001B[A\n",
      "Training:  14%|█▍        | 135/981 [00:11<00:49, 17.26it/s, epoch=1, grad_norm=0.5342, loss=2.1922, lr=0.004977]\u001B[A\n",
      "Training:  14%|█▍        | 136/981 [00:11<00:48, 17.26it/s, epoch=1, grad_norm=0.5236, loss=2.1873, lr=0.004976]\u001B[A\n",
      "Training:  14%|█▍        | 137/981 [00:11<00:48, 17.32it/s, epoch=1, grad_norm=0.5236, loss=2.1873, lr=0.004976]\u001B[A\n",
      "Training:  14%|█▍        | 137/981 [00:11<00:48, 17.32it/s, epoch=1, grad_norm=0.5183, loss=2.1760, lr=0.004975]\u001B[A\n",
      "Training:  14%|█▍        | 138/981 [00:11<00:48, 17.32it/s, epoch=1, grad_norm=0.4024, loss=2.1698, lr=0.004973]\u001B[A\n",
      "Training:  14%|█▍        | 139/981 [00:11<00:48, 17.33it/s, epoch=1, grad_norm=0.4024, loss=2.1698, lr=0.004973]\u001B[A\n",
      "Training:  14%|█▍        | 139/981 [00:11<00:48, 17.33it/s, epoch=1, grad_norm=0.3916, loss=2.1657, lr=0.004972]\u001B[A\n",
      "Training:  14%|█▍        | 140/981 [00:11<00:48, 17.33it/s, epoch=1, grad_norm=0.3250, loss=2.1600, lr=0.004971]\u001B[A\n",
      "Training:  14%|█▍        | 141/981 [00:11<00:48, 17.34it/s, epoch=1, grad_norm=0.3250, loss=2.1600, lr=0.004971]\u001B[A\n",
      "Training:  14%|█▍        | 141/981 [00:11<00:48, 17.34it/s, epoch=1, grad_norm=0.3328, loss=2.1488, lr=0.004969]\u001B[A\n",
      "Training:  14%|█▍        | 142/981 [00:11<00:48, 17.34it/s, epoch=1, grad_norm=0.3004, loss=2.1360, lr=0.004968]\u001B[A\n",
      "Training:  15%|█▍        | 143/981 [00:11<00:48, 17.35it/s, epoch=1, grad_norm=0.3004, loss=2.1360, lr=0.004968]\u001B[A\n",
      "Training:  15%|█▍        | 143/981 [00:11<00:48, 17.35it/s, epoch=1, grad_norm=0.3248, loss=2.1376, lr=0.004967]\u001B[A\n",
      "Training:  15%|█▍        | 144/981 [00:11<00:48, 17.35it/s, epoch=1, grad_norm=0.2761, loss=2.1289, lr=0.004965]\u001B[A\n",
      "Training:  15%|█▍        | 145/981 [00:11<00:48, 17.35it/s, epoch=1, grad_norm=0.2761, loss=2.1289, lr=0.004965]\u001B[A\n",
      "Training:  15%|█▍        | 145/981 [00:11<00:48, 17.35it/s, epoch=1, grad_norm=0.2228, loss=2.1184, lr=0.004964]\u001B[A\n",
      "Training:  15%|█▍        | 146/981 [00:11<00:48, 17.35it/s, epoch=1, grad_norm=0.2586, loss=2.1129, lr=0.004962]\u001B[A\n",
      "Training:  15%|█▍        | 147/981 [00:11<00:47, 17.38it/s, epoch=1, grad_norm=0.2586, loss=2.1129, lr=0.004962]\u001B[A\n",
      "Training:  15%|█▍        | 147/981 [00:11<00:47, 17.38it/s, epoch=1, grad_norm=0.1963, loss=2.0945, lr=0.004961]\u001B[A\n",
      "Training:  15%|█▌        | 148/981 [00:11<00:47, 17.38it/s, epoch=1, grad_norm=0.2413, loss=2.0875, lr=0.004959]\u001B[A\n",
      "Training:  15%|█▌        | 149/981 [00:11<00:47, 17.42it/s, epoch=1, grad_norm=0.2413, loss=2.0875, lr=0.004959]\u001B[A\n",
      "Training:  15%|█▌        | 149/981 [00:11<00:47, 17.42it/s, epoch=1, grad_norm=0.2025, loss=2.0833, lr=0.004957]\u001B[A\n",
      "Training:  15%|█▌        | 150/981 [00:11<00:47, 17.42it/s, epoch=1, grad_norm=0.2357, loss=2.0726, lr=0.004956]\u001B[A\n",
      "Training:  15%|█▌        | 151/981 [00:11<00:47, 17.40it/s, epoch=1, grad_norm=0.2357, loss=2.0726, lr=0.004956]\u001B[A\n",
      "Training:  15%|█▌        | 151/981 [00:12<00:47, 17.40it/s, epoch=1, grad_norm=0.3530, loss=2.0809, lr=0.004954]\u001B[A\n",
      "Training:  15%|█▌        | 152/981 [00:12<00:47, 17.40it/s, epoch=1, grad_norm=0.4891, loss=2.0783, lr=0.004952]\u001B[A\n",
      "Training:  16%|█▌        | 153/981 [00:12<00:47, 17.44it/s, epoch=1, grad_norm=0.4891, loss=2.0783, lr=0.004952]\u001B[A\n",
      "Training:  16%|█▌        | 153/981 [00:12<00:47, 17.44it/s, epoch=1, grad_norm=0.4384, loss=2.0626, lr=0.004951]\u001B[A\n",
      "Training:  16%|█▌        | 154/981 [00:12<00:47, 17.44it/s, epoch=1, grad_norm=0.3053, loss=2.0582, lr=0.004949]\u001B[A\n",
      "Training:  16%|█▌        | 155/981 [00:12<00:47, 17.40it/s, epoch=1, grad_norm=0.3053, loss=2.0582, lr=0.004949]\u001B[A\n",
      "Training:  16%|█▌        | 155/981 [00:12<00:47, 17.40it/s, epoch=1, grad_norm=0.3756, loss=2.0612, lr=0.004947]\u001B[A\n",
      "Training:  16%|█▌        | 156/981 [00:12<00:47, 17.40it/s, epoch=1, grad_norm=0.3864, loss=2.0555, lr=0.004945]\u001B[A\n",
      "Training:  16%|█▌        | 157/981 [00:12<00:47, 17.41it/s, epoch=1, grad_norm=0.3864, loss=2.0555, lr=0.004945]\u001B[A\n",
      "Training:  16%|█▌        | 157/981 [00:12<00:47, 17.41it/s, epoch=1, grad_norm=0.2683, loss=2.0421, lr=0.004943]\u001B[A\n",
      "Training:  16%|█▌        | 158/981 [00:12<00:47, 17.41it/s, epoch=1, grad_norm=0.3193, loss=2.0407, lr=0.004941]\u001B[A\n",
      "Training:  16%|█▌        | 159/981 [00:12<00:47, 17.38it/s, epoch=1, grad_norm=0.3193, loss=2.0407, lr=0.004941]\u001B[A\n",
      "Training:  16%|█▌        | 159/981 [00:12<00:47, 17.38it/s, epoch=1, grad_norm=0.1982, loss=2.0166, lr=0.004939]\u001B[A\n",
      "Training:  16%|█▋        | 160/981 [00:12<00:47, 17.38it/s, epoch=1, grad_norm=0.3202, loss=2.0159, lr=0.004937]\u001B[A\n",
      "Training:  16%|█▋        | 161/981 [00:12<00:47, 17.37it/s, epoch=1, grad_norm=0.3202, loss=2.0159, lr=0.004937]\u001B[A\n",
      "Training:  16%|█▋        | 161/981 [00:12<00:47, 17.37it/s, epoch=1, grad_norm=0.2202, loss=2.0099, lr=0.004935]\u001B[A\n",
      "Training:  17%|█▋        | 162/981 [00:12<00:47, 17.37it/s, epoch=1, grad_norm=0.2424, loss=2.0037, lr=0.004933]\u001B[A\n",
      "Training:  17%|█▋        | 163/981 [00:12<00:46, 17.42it/s, epoch=1, grad_norm=0.2424, loss=2.0037, lr=0.004933]\u001B[A\n",
      "Training:  17%|█▋        | 163/981 [00:12<00:46, 17.42it/s, epoch=1, grad_norm=0.2172, loss=2.0017, lr=0.004931]\u001B[A\n",
      "Training:  17%|█▋        | 164/981 [00:12<00:46, 17.42it/s, epoch=1, grad_norm=0.2236, loss=1.9859, lr=0.004929]\u001B[A\n",
      "Training:  17%|█▋        | 165/981 [00:12<00:46, 17.49it/s, epoch=1, grad_norm=0.2236, loss=1.9859, lr=0.004929]\u001B[A\n",
      "Training:  17%|█▋        | 165/981 [00:12<00:46, 17.49it/s, epoch=1, grad_norm=0.2061, loss=1.9874, lr=0.004927]\u001B[A\n",
      "Training:  17%|█▋        | 166/981 [00:12<00:46, 17.49it/s, epoch=1, grad_norm=0.2594, loss=1.9821, lr=0.004925]\u001B[A\n",
      "Training:  17%|█▋        | 167/981 [00:12<00:46, 17.44it/s, epoch=1, grad_norm=0.2594, loss=1.9821, lr=0.004925]\u001B[A\n",
      "Training:  17%|█▋        | 167/981 [00:12<00:46, 17.44it/s, epoch=1, grad_norm=0.3564, loss=1.9764, lr=0.004923]\u001B[A\n",
      "Training:  17%|█▋        | 168/981 [00:13<00:46, 17.44it/s, epoch=1, grad_norm=0.4977, loss=1.9834, lr=0.004921]\u001B[A\n",
      "Training:  17%|█▋        | 169/981 [00:13<00:46, 17.48it/s, epoch=1, grad_norm=0.4977, loss=1.9834, lr=0.004921]\u001B[A\n",
      "Training:  17%|█▋        | 169/981 [00:13<00:46, 17.48it/s, epoch=1, grad_norm=0.3399, loss=1.9712, lr=0.004918]\u001B[A\n",
      "Training:  17%|█▋        | 170/981 [00:13<00:46, 17.48it/s, epoch=1, grad_norm=0.3369, loss=1.9647, lr=0.004916]\u001B[A\n",
      "Training:  17%|█▋        | 171/981 [00:13<00:46, 17.48it/s, epoch=1, grad_norm=0.3369, loss=1.9647, lr=0.004916]\u001B[A\n",
      "Training:  17%|█▋        | 171/981 [00:13<00:46, 17.48it/s, epoch=1, grad_norm=0.4435, loss=1.9688, lr=0.004914]\u001B[A\n",
      "Training:  18%|█▊        | 172/981 [00:13<00:46, 17.48it/s, epoch=1, grad_norm=0.4312, loss=1.9574, lr=0.004912]\u001B[A\n",
      "Training:  18%|█▊        | 173/981 [00:13<00:46, 17.45it/s, epoch=1, grad_norm=0.4312, loss=1.9574, lr=0.004912]\u001B[A\n",
      "Training:  18%|█▊        | 173/981 [00:13<00:46, 17.45it/s, epoch=1, grad_norm=0.3401, loss=1.9547, lr=0.004909]\u001B[A\n",
      "Training:  18%|█▊        | 174/981 [00:13<00:46, 17.45it/s, epoch=1, grad_norm=0.2650, loss=1.9319, lr=0.004907]\u001B[A\n",
      "Training:  18%|█▊        | 175/981 [00:13<00:46, 17.41it/s, epoch=1, grad_norm=0.2650, loss=1.9319, lr=0.004907]\u001B[A\n",
      "Training:  18%|█▊        | 175/981 [00:13<00:46, 17.41it/s, epoch=1, grad_norm=0.3478, loss=1.9388, lr=0.004904]\u001B[A\n",
      "Training:  18%|█▊        | 176/981 [00:13<00:46, 17.41it/s, epoch=1, grad_norm=0.2495, loss=1.9198, lr=0.004902]\u001B[A\n",
      "Training:  18%|█▊        | 177/981 [00:13<00:46, 17.43it/s, epoch=1, grad_norm=0.2495, loss=1.9198, lr=0.004902]\u001B[A\n",
      "Training:  18%|█▊        | 177/981 [00:13<00:46, 17.43it/s, epoch=1, grad_norm=0.2390, loss=1.9266, lr=0.004899]\u001B[A\n",
      "Training:  18%|█▊        | 178/981 [00:13<00:46, 17.43it/s, epoch=1, grad_norm=0.2390, loss=1.9183, lr=0.004897]\u001B[A\n",
      "Training:  18%|█▊        | 179/981 [00:13<00:45, 17.44it/s, epoch=1, grad_norm=0.2390, loss=1.9183, lr=0.004897]\u001B[A\n",
      "Training:  18%|█▊        | 179/981 [00:13<00:45, 17.44it/s, epoch=1, grad_norm=0.2166, loss=1.9112, lr=0.004894]\u001B[A\n",
      "Training:  18%|█▊        | 180/981 [00:13<00:45, 17.44it/s, epoch=1, grad_norm=0.2217, loss=1.9002, lr=0.004892]\u001B[A\n",
      "Training:  18%|█▊        | 181/981 [00:13<00:45, 17.42it/s, epoch=1, grad_norm=0.2217, loss=1.9002, lr=0.004892]\u001B[A\n",
      "Training:  18%|█▊        | 181/981 [00:13<00:45, 17.42it/s, epoch=1, grad_norm=0.2730, loss=1.8954, lr=0.004889]\u001B[A\n",
      "Training:  19%|█▊        | 182/981 [00:13<00:45, 17.42it/s, epoch=1, grad_norm=0.2552, loss=1.8932, lr=0.004887]\u001B[A\n",
      "Training:  19%|█▊        | 183/981 [00:13<00:45, 17.45it/s, epoch=1, grad_norm=0.2552, loss=1.8932, lr=0.004887]\u001B[A\n",
      "Training:  19%|█▊        | 183/981 [00:13<00:45, 17.45it/s, epoch=1, grad_norm=0.2044, loss=1.8880, lr=0.004884]\u001B[A\n",
      "Training:  19%|█▉        | 184/981 [00:13<00:45, 17.45it/s, epoch=1, grad_norm=0.1635, loss=1.8683, lr=0.004881]\u001B[A\n",
      "Training:  19%|█▉        | 185/981 [00:13<00:45, 17.44it/s, epoch=1, grad_norm=0.1635, loss=1.8683, lr=0.004881]\u001B[A\n",
      "Training:  19%|█▉        | 185/981 [00:13<00:45, 17.44it/s, epoch=1, grad_norm=0.2124, loss=1.8752, lr=0.004878]\u001B[A\n",
      "Training:  19%|█▉        | 186/981 [00:14<00:45, 17.44it/s, epoch=1, grad_norm=0.2131, loss=1.8559, lr=0.004876]\u001B[A\n",
      "Training:  19%|█▉        | 187/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.2131, loss=1.8559, lr=0.004876]\u001B[A\n",
      "Training:  19%|█▉        | 187/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.1962, loss=1.8512, lr=0.004873]\u001B[A\n",
      "Training:  19%|█▉        | 188/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.1881, loss=1.8531, lr=0.004870]\u001B[A\n",
      "Training:  19%|█▉        | 189/981 [00:14<00:45, 17.44it/s, epoch=1, grad_norm=0.1881, loss=1.8531, lr=0.004870]\u001B[A\n",
      "Training:  19%|█▉        | 189/981 [00:14<00:45, 17.44it/s, epoch=1, grad_norm=0.2272, loss=1.8490, lr=0.004867]\u001B[A\n",
      "Training:  19%|█▉        | 190/981 [00:14<00:45, 17.44it/s, epoch=1, grad_norm=0.3085, loss=1.8491, lr=0.004864]\u001B[A\n",
      "Training:  19%|█▉        | 191/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.3085, loss=1.8491, lr=0.004864]\u001B[A\n",
      "Training:  19%|█▉        | 191/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.4089, loss=1.8383, lr=0.004861]\u001B[A\n",
      "Training:  20%|█▉        | 192/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.3801, loss=1.8432, lr=0.004859]\u001B[A\n",
      "Training:  20%|█▉        | 193/981 [00:14<00:45, 17.43it/s, epoch=1, grad_norm=0.3801, loss=1.8432, lr=0.004859]\u001B[A\n",
      "Training:  20%|█▉        | 193/981 [00:14<00:45, 17.43it/s, epoch=1, grad_norm=0.3876, loss=1.8343, lr=0.004856]\u001B[A\n",
      "Training:  20%|█▉        | 194/981 [00:14<00:45, 17.43it/s, epoch=1, grad_norm=0.4494, loss=1.8348, lr=0.004853]\u001B[A\n",
      "Training:  20%|█▉        | 195/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.4494, loss=1.8348, lr=0.004853]\u001B[A\n",
      "Training:  20%|█▉        | 195/981 [00:14<00:45, 17.45it/s, epoch=1, grad_norm=0.2525, loss=1.8289, lr=0.004850]\u001B[A\n",
      "Training:  20%|█▉        | 196/981 [00:14<00:44, 17.45it/s, epoch=1, grad_norm=0.3152, loss=1.8293, lr=0.004847]\u001B[A\n",
      "Training:  20%|██        | 197/981 [00:14<00:45, 17.41it/s, epoch=1, grad_norm=0.3152, loss=1.8293, lr=0.004847]\u001B[A\n",
      "Training:  20%|██        | 197/981 [00:14<00:45, 17.41it/s, epoch=1, grad_norm=0.2509, loss=1.8174, lr=0.004843]\u001B[A\n",
      "Training:  20%|██        | 198/981 [00:14<00:44, 17.41it/s, epoch=1, grad_norm=0.2623, loss=1.8034, lr=0.004840]\u001B[A\n",
      "Training:  20%|██        | 199/981 [00:14<00:44, 17.44it/s, epoch=1, grad_norm=0.2623, loss=1.8034, lr=0.004840]\u001B[A\n",
      "Training:  20%|██        | 199/981 [00:14<00:44, 17.44it/s, epoch=1, grad_norm=0.2253, loss=1.8094, lr=0.004837]\u001B[A\n",
      "Training:  20%|██        | 200/981 [00:17<00:44, 17.44it/s, epoch=1, grad_norm=0.2070, loss=1.8039, lr=0.004834]\u001B[A\n",
      "Training:  20%|██        | 201/981 [00:17<05:07,  2.53it/s, epoch=1, grad_norm=0.2070, loss=1.8039, lr=0.004834]\u001B[A\n",
      "Training:  20%|██        | 201/981 [00:17<05:07,  2.53it/s, epoch=1, grad_norm=0.1976, loss=1.7899, lr=0.004831]\u001B[A\n",
      "Training:  21%|██        | 202/981 [00:17<05:07,  2.53it/s, epoch=1, grad_norm=0.2096, loss=1.7869, lr=0.004828]\u001B[A\n",
      "Training:  21%|██        | 203/981 [00:17<03:48,  3.41it/s, epoch=1, grad_norm=0.2096, loss=1.7869, lr=0.004828]\u001B[A\n",
      "Training:  21%|██        | 203/981 [00:17<03:48,  3.41it/s, epoch=1, grad_norm=0.2051, loss=1.7870, lr=0.004824]\u001B[A\n",
      "Training:  21%|██        | 204/981 [00:17<03:48,  3.41it/s, epoch=1, grad_norm=0.1936, loss=1.7791, lr=0.004821]\u001B[A\n",
      "Training:  21%|██        | 205/981 [00:17<02:52,  4.49it/s, epoch=1, grad_norm=0.1936, loss=1.7791, lr=0.004821]\u001B[A\n",
      "Training:  21%|██        | 205/981 [00:17<02:52,  4.49it/s, epoch=1, grad_norm=0.2115, loss=1.7763, lr=0.004818]\u001B[A\n",
      "Training:  21%|██        | 206/981 [00:17<02:52,  4.49it/s, epoch=1, grad_norm=0.2185, loss=1.7790, lr=0.004814]\u001B[A\n",
      "Training:  21%|██        | 207/981 [00:17<02:13,  5.78it/s, epoch=1, grad_norm=0.2185, loss=1.7790, lr=0.004814]\u001B[A\n",
      "Training:  21%|██        | 207/981 [00:17<02:13,  5.78it/s, epoch=1, grad_norm=0.2318, loss=1.7678, lr=0.004811]\u001B[A\n",
      "Training:  21%|██        | 208/981 [00:17<02:13,  5.78it/s, epoch=1, grad_norm=0.2035, loss=1.7580, lr=0.004808]\u001B[A\n",
      "Training:  21%|██▏       | 209/981 [00:17<01:46,  7.23it/s, epoch=1, grad_norm=0.2035, loss=1.7580, lr=0.004808]\u001B[A\n",
      "Training:  21%|██▏       | 209/981 [00:17<01:46,  7.23it/s, epoch=1, grad_norm=0.2114, loss=1.7588, lr=0.004804]\u001B[A\n",
      "Training:  21%|██▏       | 210/981 [00:17<01:46,  7.23it/s, epoch=1, grad_norm=0.1893, loss=1.7481, lr=0.004801]\u001B[A\n",
      "Training:  22%|██▏       | 211/981 [00:17<01:27,  8.77it/s, epoch=1, grad_norm=0.1893, loss=1.7481, lr=0.004801]\u001B[A\n",
      "Training:  22%|██▏       | 211/981 [00:17<01:27,  8.77it/s, epoch=1, grad_norm=0.2046, loss=1.7543, lr=0.004797]\u001B[A\n",
      "Training:  22%|██▏       | 212/981 [00:17<01:27,  8.77it/s, epoch=1, grad_norm=0.2254, loss=1.7439, lr=0.004794]\u001B[A\n",
      "Training:  22%|██▏       | 213/981 [00:17<01:14, 10.31it/s, epoch=1, grad_norm=0.2254, loss=1.7439, lr=0.004794]\u001B[A\n",
      "Training:  22%|██▏       | 213/981 [00:17<01:14, 10.31it/s, epoch=1, grad_norm=0.2524, loss=1.7456, lr=0.004790]\u001B[A\n",
      "Training:  22%|██▏       | 214/981 [00:17<01:14, 10.31it/s, epoch=1, grad_norm=0.2526, loss=1.7370, lr=0.004787]\u001B[A\n",
      "Training:  22%|██▏       | 215/981 [00:17<01:05, 11.72it/s, epoch=1, grad_norm=0.2526, loss=1.7370, lr=0.004787]\u001B[A\n",
      "Training:  22%|██▏       | 215/981 [00:17<01:05, 11.72it/s, epoch=1, grad_norm=0.2227, loss=1.7379, lr=0.004783]\u001B[A\n",
      "Training:  22%|██▏       | 216/981 [00:18<01:05, 11.72it/s, epoch=1, grad_norm=0.2204, loss=1.7352, lr=0.004779]\u001B[A\n",
      "Training:  22%|██▏       | 217/981 [00:18<00:58, 12.99it/s, epoch=1, grad_norm=0.2204, loss=1.7352, lr=0.004779]\u001B[A\n",
      "Training:  22%|██▏       | 217/981 [00:18<00:58, 12.99it/s, epoch=1, grad_norm=0.2603, loss=1.7247, lr=0.004776]\u001B[A\n",
      "Training:  22%|██▏       | 218/981 [00:18<00:58, 12.99it/s, epoch=1, grad_norm=0.1946, loss=1.7286, lr=0.004772]\u001B[A\n",
      "Training:  22%|██▏       | 219/981 [00:18<00:54, 14.07it/s, epoch=1, grad_norm=0.1946, loss=1.7286, lr=0.004772]\u001B[A\n",
      "Training:  22%|██▏       | 219/981 [00:18<00:54, 14.07it/s, epoch=1, grad_norm=0.2040, loss=1.7121, lr=0.004768]\u001B[A\n",
      "Training:  22%|██▏       | 220/981 [00:18<00:54, 14.07it/s, epoch=1, grad_norm=0.1914, loss=1.7273, lr=0.004764]\u001B[A\n",
      "Training:  23%|██▎       | 221/981 [00:18<00:50, 14.93it/s, epoch=1, grad_norm=0.1914, loss=1.7273, lr=0.004764]\u001B[A\n",
      "Training:  23%|██▎       | 221/981 [00:18<00:50, 14.93it/s, epoch=1, grad_norm=0.1719, loss=1.7074, lr=0.004761]\u001B[A\n",
      "Training:  23%|██▎       | 222/981 [00:18<00:50, 14.93it/s, epoch=1, grad_norm=0.2084, loss=1.7170, lr=0.004757]\u001B[A\n",
      "Training:  23%|██▎       | 223/981 [00:18<00:48, 15.63it/s, epoch=1, grad_norm=0.2084, loss=1.7170, lr=0.004757]\u001B[A\n",
      "Training:  23%|██▎       | 223/981 [00:18<00:48, 15.63it/s, epoch=1, grad_norm=0.2314, loss=1.7077, lr=0.004753]\u001B[A\n",
      "Training:  23%|██▎       | 224/981 [00:18<00:48, 15.63it/s, epoch=1, grad_norm=0.1945, loss=1.7017, lr=0.004749]\u001B[A\n",
      "Training:  23%|██▎       | 225/981 [00:18<00:46, 16.14it/s, epoch=1, grad_norm=0.1945, loss=1.7017, lr=0.004749]\u001B[A\n",
      "Training:  23%|██▎       | 225/981 [00:18<00:46, 16.14it/s, epoch=1, grad_norm=0.2515, loss=1.6898, lr=0.004745]\u001B[A\n",
      "Training:  23%|██▎       | 226/981 [00:18<00:46, 16.14it/s, epoch=1, grad_norm=0.2468, loss=1.6916, lr=0.004741]\u001B[A\n",
      "Training:  23%|██▎       | 227/981 [00:18<00:45, 16.46it/s, epoch=1, grad_norm=0.2468, loss=1.6916, lr=0.004741]\u001B[A\n",
      "Training:  23%|██▎       | 227/981 [00:18<00:45, 16.46it/s, epoch=1, grad_norm=0.2094, loss=1.6872, lr=0.004737]\u001B[A\n",
      "Training:  23%|██▎       | 228/981 [00:18<00:45, 16.46it/s, epoch=1, grad_norm=0.2014, loss=1.6879, lr=0.004733]\u001B[A\n",
      "Training:  23%|██▎       | 229/981 [00:18<00:44, 16.74it/s, epoch=1, grad_norm=0.2014, loss=1.6879, lr=0.004733]\u001B[A\n",
      "Training:  23%|██▎       | 229/981 [00:18<00:44, 16.74it/s, epoch=1, grad_norm=0.2325, loss=1.6924, lr=0.004729]\u001B[A\n",
      "Training:  23%|██▎       | 230/981 [00:18<00:44, 16.74it/s, epoch=1, grad_norm=0.2361, loss=1.6766, lr=0.004725]\u001B[A\n",
      "Training:  24%|██▎       | 231/981 [00:18<00:44, 16.94it/s, epoch=1, grad_norm=0.2361, loss=1.6766, lr=0.004725]\u001B[A\n",
      "Training:  24%|██▎       | 231/981 [00:18<00:44, 16.94it/s, epoch=1, grad_norm=0.2517, loss=1.6707, lr=0.004721]\u001B[A\n",
      "Training:  24%|██▎       | 232/981 [00:18<00:44, 16.94it/s, epoch=1, grad_norm=0.2197, loss=1.6749, lr=0.004717]\u001B[A\n",
      "Training:  24%|██▍       | 233/981 [00:18<00:43, 17.10it/s, epoch=1, grad_norm=0.2197, loss=1.6749, lr=0.004717]\u001B[A\n",
      "Training:  24%|██▍       | 233/981 [00:18<00:43, 17.10it/s, epoch=1, grad_norm=0.1981, loss=1.6762, lr=0.004713]\u001B[A\n",
      "Training:  24%|██▍       | 234/981 [00:19<00:43, 17.10it/s, epoch=1, grad_norm=0.2715, loss=1.6728, lr=0.004709]\u001B[A\n",
      "Training:  24%|██▍       | 235/981 [00:19<00:43, 17.15it/s, epoch=1, grad_norm=0.2715, loss=1.6728, lr=0.004709]\u001B[A\n",
      "Training:  24%|██▍       | 235/981 [00:19<00:43, 17.15it/s, epoch=1, grad_norm=0.4066, loss=1.6711, lr=0.004705]\u001B[A\n",
      "Training:  24%|██▍       | 236/981 [00:19<00:43, 17.15it/s, epoch=1, grad_norm=0.4537, loss=1.6895, lr=0.004700]\u001B[A\n",
      "Training:  24%|██▍       | 237/981 [00:19<00:43, 17.23it/s, epoch=1, grad_norm=0.4537, loss=1.6895, lr=0.004700]\u001B[A\n",
      "Training:  24%|██▍       | 237/981 [00:19<00:43, 17.23it/s, epoch=1, grad_norm=0.2478, loss=1.6653, lr=0.004696]\u001B[A\n",
      "Training:  24%|██▍       | 238/981 [00:19<00:43, 17.23it/s, epoch=1, grad_norm=0.3332, loss=1.6750, lr=0.004692]\u001B[A\n",
      "Training:  24%|██▍       | 239/981 [00:19<00:42, 17.30it/s, epoch=1, grad_norm=0.3332, loss=1.6750, lr=0.004692]\u001B[A\n",
      "Training:  24%|██▍       | 239/981 [00:19<00:42, 17.30it/s, epoch=1, grad_norm=0.2544, loss=1.6551, lr=0.004688]\u001B[A\n",
      "Training:  24%|██▍       | 240/981 [00:19<00:42, 17.30it/s, epoch=1, grad_norm=0.3230, loss=1.6600, lr=0.004683]\u001B[A\n",
      "Training:  25%|██▍       | 241/981 [00:19<00:42, 17.33it/s, epoch=1, grad_norm=0.3230, loss=1.6600, lr=0.004683]\u001B[A\n",
      "Training:  25%|██▍       | 241/981 [00:19<00:42, 17.33it/s, epoch=1, grad_norm=0.2320, loss=1.6614, lr=0.004679]\u001B[A\n",
      "Training:  25%|██▍       | 242/981 [00:19<00:42, 17.33it/s, epoch=1, grad_norm=0.2255, loss=1.6520, lr=0.004675]\u001B[A\n",
      "Training:  25%|██▍       | 243/981 [00:19<00:42, 17.39it/s, epoch=1, grad_norm=0.2255, loss=1.6520, lr=0.004675]\u001B[A\n",
      "Training:  25%|██▍       | 243/981 [00:19<00:42, 17.39it/s, epoch=1, grad_norm=0.2086, loss=1.6550, lr=0.004670]\u001B[A\n",
      "Training:  25%|██▍       | 244/981 [00:19<00:42, 17.39it/s, epoch=1, grad_norm=0.1728, loss=1.6491, lr=0.004666]\u001B[A\n",
      "Training:  25%|██▍       | 245/981 [00:19<00:42, 17.44it/s, epoch=1, grad_norm=0.1728, loss=1.6491, lr=0.004666]\u001B[A\n",
      "Training:  25%|██▍       | 245/981 [00:19<00:42, 17.44it/s, epoch=1, grad_norm=0.2152, loss=1.6487, lr=0.004661]\u001B[A\n",
      "Training:  25%|██▌       | 246/981 [00:19<00:42, 17.44it/s, epoch=1, grad_norm=0.1841, loss=1.6367, lr=0.004657]\u001B[A\n",
      "Training:  25%|██▌       | 247/981 [00:19<00:42, 17.45it/s, epoch=1, grad_norm=0.1841, loss=1.6367, lr=0.004657]\u001B[A\n",
      "Training:  25%|██▌       | 247/981 [00:19<00:42, 17.45it/s, epoch=1, grad_norm=0.1667, loss=1.6374, lr=0.004652]\u001B[A\n",
      "Training:  25%|██▌       | 248/981 [00:19<00:41, 17.45it/s, epoch=1, grad_norm=0.1794, loss=1.6340, lr=0.004648]\u001B[A\n",
      "Training:  25%|██▌       | 249/981 [00:19<00:41, 17.48it/s, epoch=1, grad_norm=0.1794, loss=1.6340, lr=0.004648]\u001B[A\n",
      "Training:  25%|██▌       | 249/981 [00:19<00:41, 17.48it/s, epoch=1, grad_norm=0.1651, loss=1.6167, lr=0.004643]\u001B[A\n",
      "Training:  25%|██▌       | 250/981 [00:19<00:41, 17.48it/s, epoch=1, grad_norm=0.1894, loss=1.6300, lr=0.004639]\u001B[A\n",
      "Training:  26%|██▌       | 251/981 [00:19<00:41, 17.48it/s, epoch=1, grad_norm=0.1894, loss=1.6300, lr=0.004639]\u001B[A\n",
      "Training:  26%|██▌       | 251/981 [00:20<00:41, 17.48it/s, epoch=1, grad_norm=0.1866, loss=1.6272, lr=0.004634]\u001B[A\n",
      "Training:  26%|██▌       | 252/981 [00:20<00:41, 17.48it/s, epoch=1, grad_norm=0.2308, loss=1.6213, lr=0.004629]\u001B[A\n",
      "Training:  26%|██▌       | 253/981 [00:20<00:41, 17.41it/s, epoch=1, grad_norm=0.2308, loss=1.6213, lr=0.004629]\u001B[A\n",
      "Training:  26%|██▌       | 253/981 [00:20<00:41, 17.41it/s, epoch=1, grad_norm=0.2500, loss=1.6243, lr=0.004625]\u001B[A\n",
      "Training:  26%|██▌       | 254/981 [00:20<00:41, 17.41it/s, epoch=1, grad_norm=0.2353, loss=1.6357, lr=0.004620]\u001B[A\n",
      "Training:  26%|██▌       | 255/981 [00:20<00:41, 17.44it/s, epoch=1, grad_norm=0.2353, loss=1.6357, lr=0.004620]\u001B[A\n",
      "Training:  26%|██▌       | 255/981 [00:20<00:41, 17.44it/s, epoch=1, grad_norm=0.1774, loss=1.6078, lr=0.004615]\u001B[A\n",
      "Training:  26%|██▌       | 256/981 [00:20<00:41, 17.44it/s, epoch=1, grad_norm=0.1855, loss=1.6190, lr=0.004611]\u001B[A\n",
      "Training:  26%|██▌       | 257/981 [00:20<00:41, 17.46it/s, epoch=1, grad_norm=0.1855, loss=1.6190, lr=0.004611]\u001B[A\n",
      "Training:  26%|██▌       | 257/981 [00:20<00:41, 17.46it/s, epoch=1, grad_norm=0.1673, loss=1.6066, lr=0.004606]\u001B[A\n",
      "Training:  26%|██▋       | 258/981 [00:20<00:41, 17.46it/s, epoch=1, grad_norm=0.1977, loss=1.6013, lr=0.004601]\u001B[A\n",
      "Training:  26%|██▋       | 259/981 [00:20<00:41, 17.51it/s, epoch=1, grad_norm=0.1977, loss=1.6013, lr=0.004601]\u001B[A\n",
      "Training:  26%|██▋       | 259/981 [00:20<00:41, 17.51it/s, epoch=1, grad_norm=0.2462, loss=1.6044, lr=0.004596]\u001B[A\n",
      "Training:  27%|██▋       | 260/981 [00:20<00:41, 17.51it/s, epoch=1, grad_norm=0.2192, loss=1.6011, lr=0.004591]\u001B[A\n",
      "Training:  27%|██▋       | 261/981 [00:20<00:41, 17.48it/s, epoch=1, grad_norm=0.2192, loss=1.6011, lr=0.004591]\u001B[A\n",
      "Training:  27%|██▋       | 261/981 [00:20<00:41, 17.48it/s, epoch=1, grad_norm=0.2387, loss=1.6070, lr=0.004586]\u001B[A\n",
      "Training:  27%|██▋       | 262/981 [00:20<00:41, 17.48it/s, epoch=1, grad_norm=0.1617, loss=1.6024, lr=0.004581]\u001B[A\n",
      "Training:  27%|██▋       | 263/981 [00:20<00:41, 17.49it/s, epoch=1, grad_norm=0.1617, loss=1.6024, lr=0.004581]\u001B[A\n",
      "Training:  27%|██▋       | 263/981 [00:20<00:41, 17.49it/s, epoch=1, grad_norm=0.1783, loss=1.5900, lr=0.004577]\u001B[A\n",
      "Training:  27%|██▋       | 264/981 [00:20<00:41, 17.49it/s, epoch=1, grad_norm=0.1429, loss=1.5976, lr=0.004572]\u001B[A\n",
      "Training:  27%|██▋       | 265/981 [00:20<00:40, 17.47it/s, epoch=1, grad_norm=0.1429, loss=1.5976, lr=0.004572]\u001B[A\n",
      "Training:  27%|██▋       | 265/981 [00:20<00:40, 17.47it/s, epoch=1, grad_norm=0.1848, loss=1.5935, lr=0.004567]\u001B[A\n",
      "Training:  27%|██▋       | 266/981 [00:20<00:40, 17.47it/s, epoch=1, grad_norm=0.1724, loss=1.5964, lr=0.004562]\u001B[A\n",
      "Training:  27%|██▋       | 267/981 [00:20<00:40, 17.47it/s, epoch=1, grad_norm=0.1724, loss=1.5964, lr=0.004562]\u001B[A\n",
      "Training:  27%|██▋       | 267/981 [00:20<00:40, 17.47it/s, epoch=1, grad_norm=0.1927, loss=1.5834, lr=0.004556]\u001B[A\n",
      "Training:  27%|██▋       | 268/981 [00:20<00:40, 17.47it/s, epoch=1, grad_norm=0.1519, loss=1.5797, lr=0.004551]\u001B[A\n",
      "Training:  27%|██▋       | 269/981 [00:20<00:40, 17.44it/s, epoch=1, grad_norm=0.1519, loss=1.5797, lr=0.004551]\u001B[A\n",
      "Training:  27%|██▋       | 269/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.1736, loss=1.5725, lr=0.004546]\u001B[A\n",
      "Training:  28%|██▊       | 270/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.1734, loss=1.5805, lr=0.004541]\u001B[A\n",
      "Training:  28%|██▊       | 271/981 [00:21<00:40, 17.45it/s, epoch=1, grad_norm=0.1734, loss=1.5805, lr=0.004541]\u001B[A\n",
      "Training:  28%|██▊       | 271/981 [00:21<00:40, 17.45it/s, epoch=1, grad_norm=0.1885, loss=1.5808, lr=0.004536]\u001B[A\n",
      "Training:  28%|██▊       | 272/981 [00:21<00:40, 17.45it/s, epoch=1, grad_norm=0.2267, loss=1.5653, lr=0.004531]\u001B[A\n",
      "Training:  28%|██▊       | 273/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.2267, loss=1.5653, lr=0.004531]\u001B[A\n",
      "Training:  28%|██▊       | 273/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.3150, loss=1.5760, lr=0.004526]\u001B[A\n",
      "Training:  28%|██▊       | 274/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.3436, loss=1.5729, lr=0.004520]\u001B[A\n",
      "Training:  28%|██▊       | 275/981 [00:21<00:40, 17.46it/s, epoch=1, grad_norm=0.3436, loss=1.5729, lr=0.004520]\u001B[A\n",
      "Training:  28%|██▊       | 275/981 [00:21<00:40, 17.46it/s, epoch=1, grad_norm=0.2190, loss=1.5798, lr=0.004515]\u001B[A\n",
      "Training:  28%|██▊       | 276/981 [00:21<00:40, 17.46it/s, epoch=1, grad_norm=0.2826, loss=1.5845, lr=0.004510]\u001B[A\n",
      "Training:  28%|██▊       | 277/981 [00:21<00:40, 17.48it/s, epoch=1, grad_norm=0.2826, loss=1.5845, lr=0.004510]\u001B[A\n",
      "Training:  28%|██▊       | 277/981 [00:21<00:40, 17.48it/s, epoch=1, grad_norm=0.3375, loss=1.5840, lr=0.004505]\u001B[A\n",
      "Training:  28%|██▊       | 278/981 [00:21<00:40, 17.48it/s, epoch=1, grad_norm=0.3201, loss=1.5761, lr=0.004499]\u001B[A\n",
      "Training:  28%|██▊       | 279/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.3201, loss=1.5761, lr=0.004499]\u001B[A\n",
      "Training:  28%|██▊       | 279/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.3521, loss=1.5759, lr=0.004494]\u001B[A\n",
      "Training:  29%|██▊       | 280/981 [00:21<00:40, 17.44it/s, epoch=1, grad_norm=0.2354, loss=1.5704, lr=0.004489]\u001B[A\n",
      "Training:  29%|██▊       | 281/981 [00:21<00:40, 17.41it/s, epoch=1, grad_norm=0.2354, loss=1.5704, lr=0.004489]\u001B[A\n",
      "Training:  29%|██▊       | 281/981 [00:21<00:40, 17.41it/s, epoch=1, grad_norm=0.2622, loss=1.5629, lr=0.004483]\u001B[A\n",
      "Training:  29%|██▊       | 282/981 [00:21<00:40, 17.41it/s, epoch=1, grad_norm=0.2059, loss=1.5634, lr=0.004478]\u001B[A\n",
      "Training:  29%|██▉       | 283/981 [00:21<00:40, 17.43it/s, epoch=1, grad_norm=0.2059, loss=1.5634, lr=0.004478]\u001B[A\n",
      "Training:  29%|██▉       | 283/981 [00:21<00:40, 17.43it/s, epoch=1, grad_norm=0.2208, loss=1.5639, lr=0.004472]\u001B[A\n",
      "Training:  29%|██▉       | 284/981 [00:21<00:39, 17.43it/s, epoch=1, grad_norm=0.1801, loss=1.5561, lr=0.004467]\u001B[A\n",
      "Training:  29%|██▉       | 285/981 [00:21<00:39, 17.46it/s, epoch=1, grad_norm=0.1801, loss=1.5561, lr=0.004467]\u001B[A\n",
      "Training:  29%|██▉       | 285/981 [00:21<00:39, 17.46it/s, epoch=1, grad_norm=0.1993, loss=1.5591, lr=0.004461]\u001B[A\n",
      "Training:  29%|██▉       | 286/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1948, loss=1.5547, lr=0.004456]\u001B[A\n",
      "Training:  29%|██▉       | 287/981 [00:22<00:39, 17.41it/s, epoch=1, grad_norm=0.1948, loss=1.5547, lr=0.004456]\u001B[A\n",
      "Training:  29%|██▉       | 287/981 [00:22<00:39, 17.41it/s, epoch=1, grad_norm=0.1695, loss=1.5471, lr=0.004450]\u001B[A\n",
      "Training:  29%|██▉       | 288/981 [00:22<00:39, 17.41it/s, epoch=1, grad_norm=0.2323, loss=1.5483, lr=0.004445]\u001B[A\n",
      "Training:  29%|██▉       | 289/981 [00:22<00:39, 17.47it/s, epoch=1, grad_norm=0.2323, loss=1.5483, lr=0.004445]\u001B[A\n",
      "Training:  29%|██▉       | 289/981 [00:22<00:39, 17.47it/s, epoch=1, grad_norm=0.1542, loss=1.5439, lr=0.004439]\u001B[A\n",
      "Training:  30%|██▉       | 290/981 [00:22<00:39, 17.47it/s, epoch=1, grad_norm=0.1855, loss=1.5411, lr=0.004433]\u001B[A\n",
      "Training:  30%|██▉       | 291/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1855, loss=1.5411, lr=0.004433]\u001B[A\n",
      "Training:  30%|██▉       | 291/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1853, loss=1.5400, lr=0.004428]\u001B[A\n",
      "Training:  30%|██▉       | 292/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1749, loss=1.5397, lr=0.004422]\u001B[A\n",
      "Training:  30%|██▉       | 293/981 [00:22<00:39, 17.48it/s, epoch=1, grad_norm=0.1749, loss=1.5397, lr=0.004422]\u001B[A\n",
      "Training:  30%|██▉       | 293/981 [00:22<00:39, 17.48it/s, epoch=1, grad_norm=0.1614, loss=1.5397, lr=0.004416]\u001B[A\n",
      "Training:  30%|██▉       | 294/981 [00:22<00:39, 17.48it/s, epoch=1, grad_norm=0.1733, loss=1.5423, lr=0.004411]\u001B[A\n",
      "Training:  30%|███       | 295/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1733, loss=1.5423, lr=0.004411]\u001B[A\n",
      "Training:  30%|███       | 295/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1813, loss=1.5292, lr=0.004405]\u001B[A\n",
      "Training:  30%|███       | 296/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1722, loss=1.5335, lr=0.004399]\u001B[A\n",
      "Training:  30%|███       | 297/981 [00:22<00:39, 17.45it/s, epoch=1, grad_norm=0.1722, loss=1.5335, lr=0.004399]\u001B[A\n",
      "Training:  30%|███       | 297/981 [00:22<00:39, 17.45it/s, epoch=1, grad_norm=0.1499, loss=1.5399, lr=0.004393]\u001B[A\n",
      "Training:  30%|███       | 298/981 [00:22<00:39, 17.45it/s, epoch=1, grad_norm=0.1545, loss=1.5237, lr=0.004388]\u001B[A\n",
      "Training:  30%|███       | 299/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1545, loss=1.5237, lr=0.004388]\u001B[A\n",
      "Training:  30%|███       | 299/981 [00:22<00:39, 17.46it/s, epoch=1, grad_norm=0.1873, loss=1.5347, lr=0.004382]\u001B[A\n",
      "Training:  31%|███       | 300/981 [00:25<00:39, 17.46it/s, epoch=1, grad_norm=0.1818, loss=1.5333, lr=0.004376]\u001B[A\n",
      "Training:  31%|███       | 301/981 [00:25<04:49,  2.35it/s, epoch=1, grad_norm=0.1818, loss=1.5333, lr=0.004376]\u001B[A\n",
      "Training:  31%|███       | 301/981 [00:25<04:49,  2.35it/s, epoch=1, grad_norm=0.1847, loss=1.5260, lr=0.004370]\u001B[A\n",
      "Training:  31%|███       | 302/981 [00:25<04:49,  2.35it/s, epoch=1, grad_norm=0.2267, loss=1.5248, lr=0.004364]\u001B[A\n",
      "Training:  31%|███       | 303/981 [00:25<03:34,  3.16it/s, epoch=1, grad_norm=0.2267, loss=1.5248, lr=0.004364]\u001B[A\n",
      "Training:  31%|███       | 303/981 [00:25<03:34,  3.16it/s, epoch=1, grad_norm=0.2866, loss=1.5401, lr=0.004358]\u001B[A\n",
      "Training:  31%|███       | 304/981 [00:25<03:34,  3.16it/s, epoch=1, grad_norm=0.2576, loss=1.5408, lr=0.004352]\u001B[A\n",
      "Training:  31%|███       | 305/981 [00:25<02:41,  4.18it/s, epoch=1, grad_norm=0.2576, loss=1.5408, lr=0.004352]\u001B[A\n",
      "Training:  31%|███       | 305/981 [00:25<02:41,  4.18it/s, epoch=1, grad_norm=0.2024, loss=1.5272, lr=0.004346]\u001B[A\n",
      "Training:  31%|███       | 306/981 [00:25<02:41,  4.18it/s, epoch=1, grad_norm=0.1927, loss=1.5236, lr=0.004340]\u001B[A\n",
      "Training:  31%|███▏      | 307/981 [00:25<02:04,  5.39it/s, epoch=1, grad_norm=0.1927, loss=1.5236, lr=0.004340]\u001B[A\n",
      "Training:  31%|███▏      | 307/981 [00:25<02:04,  5.39it/s, epoch=1, grad_norm=0.2012, loss=1.5135, lr=0.004334]\u001B[A\n",
      "Training:  31%|███▏      | 308/981 [00:25<02:04,  5.39it/s, epoch=1, grad_norm=0.1586, loss=1.5170, lr=0.004328]\u001B[A\n",
      "Training:  31%|███▏      | 309/981 [00:25<01:39,  6.77it/s, epoch=1, grad_norm=0.1586, loss=1.5170, lr=0.004328]\u001B[A\n",
      "Training:  31%|███▏      | 309/981 [00:25<01:39,  6.77it/s, epoch=1, grad_norm=0.1669, loss=1.5341, lr=0.004322]\u001B[A\n",
      "Training:  32%|███▏      | 310/981 [00:25<01:39,  6.77it/s, epoch=1, grad_norm=0.1526, loss=1.5072, lr=0.004316]\u001B[A\n",
      "Training:  32%|███▏      | 311/981 [00:25<01:21,  8.24it/s, epoch=1, grad_norm=0.1526, loss=1.5072, lr=0.004316]\u001B[A\n",
      "Training:  32%|███▏      | 311/981 [00:25<01:21,  8.24it/s, epoch=1, grad_norm=0.1821, loss=1.5158, lr=0.004310]\u001B[A\n",
      "Training:  32%|███▏      | 312/981 [00:26<01:21,  8.24it/s, epoch=1, grad_norm=0.1828, loss=1.5209, lr=0.004304]\u001B[A\n",
      "Training:  32%|███▏      | 313/981 [00:26<01:08,  9.71it/s, epoch=1, grad_norm=0.1828, loss=1.5209, lr=0.004304]\u001B[A\n",
      "Training:  32%|███▏      | 313/981 [00:26<01:08,  9.71it/s, epoch=1, grad_norm=0.2070, loss=1.5144, lr=0.004297]\u001B[A\n",
      "Training:  32%|███▏      | 314/981 [00:26<01:08,  9.71it/s, epoch=1, grad_norm=0.2418, loss=1.5149, lr=0.004291]\u001B[A\n",
      "Training:  32%|███▏      | 315/981 [00:26<00:59, 11.11it/s, epoch=1, grad_norm=0.2418, loss=1.5149, lr=0.004291]\u001B[A\n",
      "Training:  32%|███▏      | 315/981 [00:26<00:59, 11.11it/s, epoch=1, grad_norm=0.2490, loss=1.5071, lr=0.004285]\u001B[A\n",
      "Training:  32%|███▏      | 316/981 [00:26<00:59, 11.11it/s, epoch=1, grad_norm=0.1817, loss=1.5121, lr=0.004279]\u001B[A\n",
      "Training:  32%|███▏      | 317/981 [00:26<00:54, 12.29it/s, epoch=1, grad_norm=0.1817, loss=1.5121, lr=0.004279]\u001B[A\n",
      "Training:  32%|███▏      | 317/981 [00:26<00:54, 12.29it/s, epoch=1, grad_norm=0.1638, loss=1.4996, lr=0.004272]\u001B[A\n",
      "Training:  32%|███▏      | 318/981 [00:26<00:53, 12.29it/s, epoch=1, grad_norm=0.2145, loss=1.5057, lr=0.004266]\u001B[A\n",
      "Training:  33%|███▎      | 319/981 [00:26<00:50, 13.22it/s, epoch=1, grad_norm=0.2145, loss=1.5057, lr=0.004266]\u001B[A\n",
      "Training:  33%|███▎      | 319/981 [00:26<00:50, 13.22it/s, epoch=1, grad_norm=0.2364, loss=1.5126, lr=0.004260]\u001B[A\n",
      "Training:  33%|███▎      | 320/981 [00:26<00:50, 13.22it/s, epoch=1, grad_norm=0.2035, loss=1.4990, lr=0.004254]\u001B[A\n",
      "Training:  33%|███▎      | 321/981 [00:26<00:47, 14.02it/s, epoch=1, grad_norm=0.2035, loss=1.4990, lr=0.004254]\u001B[A\n",
      "Training:  33%|███▎      | 321/981 [00:26<00:47, 14.02it/s, epoch=1, grad_norm=0.1747, loss=1.5055, lr=0.004247]\u001B[A\n",
      "Training:  33%|███▎      | 322/981 [00:26<00:47, 14.02it/s, epoch=1, grad_norm=0.1772, loss=1.4936, lr=0.004241]\u001B[A\n",
      "Training:  33%|███▎      | 323/981 [00:26<00:44, 14.65it/s, epoch=1, grad_norm=0.1772, loss=1.4936, lr=0.004241]\u001B[A\n",
      "Training:  33%|███▎      | 323/981 [00:26<00:44, 14.65it/s, epoch=1, grad_norm=0.1836, loss=1.5055, lr=0.004234]\u001B[A\n",
      "Training:  33%|███▎      | 324/981 [00:26<00:44, 14.65it/s, epoch=1, grad_norm=0.1705, loss=1.5005, lr=0.004228]\u001B[A\n",
      "Training:  33%|███▎      | 325/981 [00:26<00:43, 15.13it/s, epoch=1, grad_norm=0.1705, loss=1.5005, lr=0.004228]\u001B[A\n",
      "Training:  33%|███▎      | 325/981 [00:26<00:43, 15.13it/s, epoch=1, grad_norm=0.1491, loss=1.5009, lr=0.004222]\u001B[A\n",
      "Training:  33%|███▎      | 326/981 [00:26<00:43, 15.13it/s, epoch=1, grad_norm=0.1681, loss=1.4924, lr=0.004215]\u001B[A\n",
      "Training:  33%|███▎      | 327/981 [00:26<00:42, 15.48it/s, epoch=1, grad_norm=0.1681, loss=1.4924, lr=0.004215]\u001B[A\n",
      "Training:  33%|███▎      | 327/981 [00:26<00:42, 15.48it/s, epoch=1, grad_norm=0.1623, loss=1.4969, lr=0.004209]\u001B[A\n",
      "Training:  33%|███▎      | 328/981 [00:26<00:42, 15.48it/s, epoch=1, grad_norm=0.1845, loss=1.4961, lr=0.004202]\u001B[A\n",
      "Training:  34%|███▎      | 329/981 [00:26<00:41, 15.73it/s, epoch=1, grad_norm=0.1845, loss=1.4961, lr=0.004202]\u001B[A\n",
      "Training:  34%|███▎      | 329/981 [00:27<00:41, 15.73it/s, epoch=1, grad_norm=0.2357, loss=1.4921, lr=0.004196]\u001B[A\n",
      "Training:  34%|███▎      | 330/981 [00:27<00:41, 15.73it/s, epoch=1, grad_norm=0.2092, loss=1.4924, lr=0.004189]\u001B[A\n",
      "Training:  34%|███▎      | 331/981 [00:27<00:40, 15.85it/s, epoch=1, grad_norm=0.2092, loss=1.4924, lr=0.004189]\u001B[A\n",
      "Training:  34%|███▎      | 331/981 [00:27<00:40, 15.85it/s, epoch=1, grad_norm=0.1839, loss=1.4857, lr=0.004182]\u001B[A\n",
      "Training:  34%|███▍      | 332/981 [00:27<00:40, 15.85it/s, epoch=1, grad_norm=0.1687, loss=1.4903, lr=0.004176]\u001B[A\n",
      "Training:  34%|███▍      | 333/981 [00:27<00:40, 16.01it/s, epoch=1, grad_norm=0.1687, loss=1.4903, lr=0.004176]\u001B[A\n",
      "Training:  34%|███▍      | 333/981 [00:27<00:40, 16.01it/s, epoch=1, grad_norm=0.1752, loss=1.4841, lr=0.004169]\u001B[A\n",
      "Training:  34%|███▍      | 334/981 [00:27<00:40, 16.01it/s, epoch=1, grad_norm=0.1569, loss=1.4843, lr=0.004163]\u001B[A\n",
      "Training:  34%|███▍      | 335/981 [00:27<00:39, 16.17it/s, epoch=1, grad_norm=0.1569, loss=1.4843, lr=0.004163]\u001B[A\n",
      "Training:  34%|███▍      | 335/981 [00:27<00:39, 16.17it/s, epoch=1, grad_norm=0.1783, loss=1.4789, lr=0.004156]\u001B[A\n",
      "Training:  34%|███▍      | 336/981 [00:27<00:39, 16.17it/s, epoch=1, grad_norm=0.1480, loss=1.4903, lr=0.004149]\u001B[A\n",
      "Training:  34%|███▍      | 337/981 [00:27<00:39, 16.27it/s, epoch=1, grad_norm=0.1480, loss=1.4903, lr=0.004149]\u001B[A\n",
      "Training:  34%|███▍      | 337/981 [00:27<00:39, 16.27it/s, epoch=1, grad_norm=0.1785, loss=1.4720, lr=0.004143]\u001B[A\n",
      "Training:  34%|███▍      | 338/981 [00:27<00:39, 16.27it/s, epoch=1, grad_norm=0.2260, loss=1.4871, lr=0.004136]\u001B[A\n",
      "Training:  35%|███▍      | 339/981 [00:27<00:39, 16.21it/s, epoch=1, grad_norm=0.2260, loss=1.4871, lr=0.004136]\u001B[A\n",
      "Training:  35%|███▍      | 339/981 [00:27<00:39, 16.21it/s, epoch=1, grad_norm=0.2996, loss=1.4841, lr=0.004129]\u001B[A\n",
      "Training:  35%|███▍      | 340/981 [00:27<00:39, 16.21it/s, epoch=1, grad_norm=0.2728, loss=1.4847, lr=0.004122]\u001B[A\n",
      "Training:  35%|███▍      | 341/981 [00:27<00:39, 16.18it/s, epoch=1, grad_norm=0.2728, loss=1.4847, lr=0.004122]\u001B[A\n",
      "Training:  35%|███▍      | 341/981 [00:27<00:39, 16.18it/s, epoch=1, grad_norm=0.1672, loss=1.4798, lr=0.004116]\u001B[A\n",
      "Training:  35%|███▍      | 342/981 [00:27<00:39, 16.18it/s, epoch=1, grad_norm=0.2369, loss=1.4785, lr=0.004109]\u001B[A\n",
      "Training:  35%|███▍      | 343/981 [00:27<00:39, 16.19it/s, epoch=1, grad_norm=0.2369, loss=1.4785, lr=0.004109]\u001B[A\n",
      "Training:  35%|███▍      | 343/981 [00:27<00:39, 16.19it/s, epoch=1, grad_norm=0.2270, loss=1.4813, lr=0.004102]\u001B[A\n",
      "Training:  35%|███▌      | 344/981 [00:27<00:39, 16.19it/s, epoch=1, grad_norm=0.1822, loss=1.4642, lr=0.004095]\u001B[A\n",
      "Training:  35%|███▌      | 345/981 [00:27<00:39, 16.30it/s, epoch=1, grad_norm=0.1822, loss=1.4642, lr=0.004095]\u001B[A\n",
      "Training:  35%|███▌      | 345/981 [00:28<00:39, 16.30it/s, epoch=1, grad_norm=0.2033, loss=1.4790, lr=0.004088]\u001B[A\n",
      "Training:  35%|███▌      | 346/981 [00:28<00:38, 16.30it/s, epoch=1, grad_norm=0.1768, loss=1.4731, lr=0.004081]\u001B[A\n",
      "Training:  35%|███▌      | 347/981 [00:28<00:38, 16.28it/s, epoch=1, grad_norm=0.1768, loss=1.4731, lr=0.004081]\u001B[A\n",
      "Training:  35%|███▌      | 347/981 [00:28<00:38, 16.28it/s, epoch=1, grad_norm=0.1760, loss=1.4661, lr=0.004075]\u001B[A\n",
      "Training:  35%|███▌      | 348/981 [00:28<00:38, 16.28it/s, epoch=1, grad_norm=0.1963, loss=1.4658, lr=0.004068]\u001B[A\n",
      "Training:  36%|███▌      | 349/981 [00:28<00:38, 16.22it/s, epoch=1, grad_norm=0.1963, loss=1.4658, lr=0.004068]\u001B[A\n",
      "Training:  36%|███▌      | 349/981 [00:28<00:38, 16.22it/s, epoch=1, grad_norm=0.1854, loss=1.4711, lr=0.004061]\u001B[A\n",
      "Training:  36%|███▌      | 350/981 [00:28<00:38, 16.22it/s, epoch=1, grad_norm=0.1812, loss=1.4665, lr=0.004054]\u001B[A\n",
      "Training:  36%|███▌      | 351/981 [00:28<00:38, 16.17it/s, epoch=1, grad_norm=0.1812, loss=1.4665, lr=0.004054]\u001B[A\n",
      "Training:  36%|███▌      | 351/981 [00:28<00:38, 16.17it/s, epoch=1, grad_norm=0.1706, loss=1.4713, lr=0.004047]\u001B[A\n",
      "Training:  36%|███▌      | 352/981 [00:28<00:38, 16.17it/s, epoch=1, grad_norm=0.1697, loss=1.4645, lr=0.004040]\u001B[A\n",
      "Training:  36%|███▌      | 353/981 [00:28<00:38, 16.20it/s, epoch=1, grad_norm=0.1697, loss=1.4645, lr=0.004040]\u001B[A\n",
      "Training:  36%|███▌      | 353/981 [00:28<00:38, 16.20it/s, epoch=1, grad_norm=0.1735, loss=1.4623, lr=0.004033]\u001B[A\n",
      "Training:  36%|███▌      | 354/981 [00:28<00:38, 16.20it/s, epoch=1, grad_norm=0.1476, loss=1.4760, lr=0.004026]\u001B[A\n",
      "Training:  36%|███▌      | 355/981 [00:28<00:38, 16.25it/s, epoch=1, grad_norm=0.1476, loss=1.4760, lr=0.004026]\u001B[A\n",
      "Training:  36%|███▌      | 355/981 [00:28<00:38, 16.25it/s, epoch=1, grad_norm=0.1497, loss=1.4689, lr=0.004019]\u001B[A\n",
      "Training:  36%|███▋      | 356/981 [00:28<00:38, 16.25it/s, epoch=1, grad_norm=0.1700, loss=1.4712, lr=0.004012]\u001B[A\n",
      "Training:  36%|███▋      | 357/981 [00:28<00:38, 16.26it/s, epoch=1, grad_norm=0.1700, loss=1.4712, lr=0.004012]\u001B[A\n",
      "Training:  36%|███▋      | 357/981 [00:28<00:38, 16.26it/s, epoch=1, grad_norm=0.2237, loss=1.4641, lr=0.004004]\u001B[A\n",
      "Training:  36%|███▋      | 358/981 [00:28<00:38, 16.26it/s, epoch=1, grad_norm=0.2701, loss=1.4670, lr=0.003997]\u001B[A\n",
      "Training:  37%|███▋      | 359/981 [00:28<00:38, 16.22it/s, epoch=1, grad_norm=0.2701, loss=1.4670, lr=0.003997]\u001B[A\n",
      "Training:  37%|███▋      | 359/981 [00:28<00:38, 16.22it/s, epoch=1, grad_norm=0.2975, loss=1.4633, lr=0.003990]\u001B[A\n",
      "Training:  37%|███▋      | 360/981 [00:28<00:38, 16.22it/s, epoch=1, grad_norm=0.2009, loss=1.4572, lr=0.003983]\u001B[A\n",
      "Training:  37%|███▋      | 361/981 [00:28<00:38, 16.22it/s, epoch=1, grad_norm=0.2009, loss=1.4572, lr=0.003983]\u001B[A\n",
      "Training:  37%|███▋      | 361/981 [00:29<00:38, 16.22it/s, epoch=1, grad_norm=0.1579, loss=1.4497, lr=0.003976]\u001B[A\n",
      "Training:  37%|███▋      | 362/981 [00:29<00:38, 16.22it/s, epoch=1, grad_norm=0.2251, loss=1.4635, lr=0.003969]\u001B[A\n",
      "Training:  37%|███▋      | 363/981 [00:29<00:38, 16.21it/s, epoch=1, grad_norm=0.2251, loss=1.4635, lr=0.003969]\u001B[A\n",
      "Training:  37%|███▋      | 363/981 [00:29<00:38, 16.21it/s, epoch=1, grad_norm=0.2137, loss=1.4641, lr=0.003962]\u001B[A\n",
      "Training:  37%|███▋      | 364/981 [00:29<00:38, 16.21it/s, epoch=1, grad_norm=0.1353, loss=1.4624, lr=0.003954]\u001B[A\n",
      "Training:  37%|███▋      | 365/981 [00:29<00:38, 16.17it/s, epoch=1, grad_norm=0.1353, loss=1.4624, lr=0.003954]\u001B[A\n",
      "Training:  37%|███▋      | 365/981 [00:29<00:38, 16.17it/s, epoch=1, grad_norm=0.1901, loss=1.4606, lr=0.003947]\u001B[A\n",
      "Training:  37%|███▋      | 366/981 [00:29<00:38, 16.17it/s, epoch=1, grad_norm=0.1629, loss=1.4497, lr=0.003940]\u001B[A\n",
      "Training:  37%|███▋      | 367/981 [00:29<00:37, 16.17it/s, epoch=1, grad_norm=0.1629, loss=1.4497, lr=0.003940]\u001B[A\n",
      "Training:  37%|███▋      | 367/981 [00:29<00:37, 16.17it/s, epoch=1, grad_norm=0.1420, loss=1.4536, lr=0.003933]\u001B[A\n",
      "Training:  38%|███▊      | 368/981 [00:29<00:37, 16.17it/s, epoch=1, grad_norm=0.1618, loss=1.4530, lr=0.003925]\u001B[A\n",
      "Training:  38%|███▊      | 369/981 [00:29<00:37, 16.19it/s, epoch=1, grad_norm=0.1618, loss=1.4530, lr=0.003925]\u001B[A\n",
      "Training:  38%|███▊      | 369/981 [00:29<00:37, 16.19it/s, epoch=1, grad_norm=0.1563, loss=1.4483, lr=0.003918]\u001B[A\n",
      "Training:  38%|███▊      | 370/981 [00:29<00:37, 16.19it/s, epoch=1, grad_norm=0.1355, loss=1.4656, lr=0.003911]\u001B[A\n",
      "Training:  38%|███▊      | 371/981 [00:29<00:37, 16.18it/s, epoch=1, grad_norm=0.1355, loss=1.4656, lr=0.003911]\u001B[A\n",
      "Training:  38%|███▊      | 371/981 [00:29<00:37, 16.18it/s, epoch=1, grad_norm=0.1347, loss=1.4497, lr=0.003903]\u001B[A\n",
      "Training:  38%|███▊      | 372/981 [00:29<00:37, 16.18it/s, epoch=1, grad_norm=0.1415, loss=1.4519, lr=0.003896]\u001B[A\n",
      "Training:  38%|███▊      | 373/981 [00:29<00:37, 16.24it/s, epoch=1, grad_norm=0.1415, loss=1.4519, lr=0.003896]\u001B[A\n",
      "Training:  38%|███▊      | 373/981 [00:29<00:37, 16.24it/s, epoch=1, grad_norm=0.1629, loss=1.4577, lr=0.003888]\u001B[A\n",
      "Training:  38%|███▊      | 374/981 [00:29<00:37, 16.24it/s, epoch=1, grad_norm=0.1806, loss=1.4428, lr=0.003881]\u001B[A\n",
      "Training:  38%|███▊      | 375/981 [00:29<00:37, 16.23it/s, epoch=1, grad_norm=0.1806, loss=1.4428, lr=0.003881]\u001B[A\n",
      "Training:  38%|███▊      | 375/981 [00:29<00:37, 16.23it/s, epoch=1, grad_norm=0.1868, loss=1.4502, lr=0.003874]\u001B[A\n",
      "Training:  38%|███▊      | 376/981 [00:29<00:37, 16.23it/s, epoch=1, grad_norm=0.1895, loss=1.4466, lr=0.003866]\u001B[A\n",
      "Training:  38%|███▊      | 377/981 [00:29<00:37, 16.28it/s, epoch=1, grad_norm=0.1895, loss=1.4466, lr=0.003866]\u001B[A\n",
      "Training:  38%|███▊      | 377/981 [00:30<00:37, 16.28it/s, epoch=1, grad_norm=0.1597, loss=1.4458, lr=0.003859]\u001B[A\n",
      "Training:  39%|███▊      | 378/981 [00:30<00:37, 16.28it/s, epoch=1, grad_norm=0.1687, loss=1.4470, lr=0.003851]\u001B[A\n",
      "Training:  39%|███▊      | 379/981 [00:30<00:37, 16.25it/s, epoch=1, grad_norm=0.1687, loss=1.4470, lr=0.003851]\u001B[A\n",
      "Training:  39%|███▊      | 379/981 [00:30<00:37, 16.25it/s, epoch=1, grad_norm=0.1494, loss=1.4357, lr=0.003844]\u001B[A\n",
      "Training:  39%|███▊      | 380/981 [00:30<00:36, 16.25it/s, epoch=1, grad_norm=0.1606, loss=1.4439, lr=0.003836]\u001B[A\n",
      "Training:  39%|███▉      | 381/981 [00:30<00:37, 16.19it/s, epoch=1, grad_norm=0.1606, loss=1.4439, lr=0.003836]\u001B[A\n",
      "Training:  39%|███▉      | 381/981 [00:30<00:37, 16.19it/s, epoch=1, grad_norm=0.1799, loss=1.4514, lr=0.003829]\u001B[A\n",
      "Training:  39%|███▉      | 382/981 [00:30<00:36, 16.19it/s, epoch=1, grad_norm=0.1858, loss=1.4453, lr=0.003821]\u001B[A\n",
      "Training:  39%|███▉      | 383/981 [00:30<00:36, 16.23it/s, epoch=1, grad_norm=0.1858, loss=1.4453, lr=0.003821]\u001B[A\n",
      "Training:  39%|███▉      | 383/981 [00:30<00:36, 16.23it/s, epoch=1, grad_norm=0.1577, loss=1.4409, lr=0.003814]\u001B[A\n",
      "Training:  39%|███▉      | 384/981 [00:30<00:36, 16.23it/s, epoch=1, grad_norm=0.1450, loss=1.4343, lr=0.003806]\u001B[A\n",
      "Training:  39%|███▉      | 385/981 [00:30<00:36, 16.19it/s, epoch=1, grad_norm=0.1450, loss=1.4343, lr=0.003806]\u001B[A\n",
      "Training:  39%|███▉      | 385/981 [00:30<00:36, 16.19it/s, epoch=1, grad_norm=0.1704, loss=1.4486, lr=0.003798]\u001B[A\n",
      "Training:  39%|███▉      | 386/981 [00:30<00:36, 16.19it/s, epoch=1, grad_norm=0.2029, loss=1.4326, lr=0.003791]\u001B[A\n",
      "Training:  39%|███▉      | 387/981 [00:30<00:36, 16.20it/s, epoch=1, grad_norm=0.2029, loss=1.4326, lr=0.003791]\u001B[A\n",
      "Training:  39%|███▉      | 387/981 [00:30<00:36, 16.20it/s, epoch=1, grad_norm=0.2380, loss=1.4359, lr=0.003783]\u001B[A\n",
      "Training:  40%|███▉      | 388/981 [00:30<00:36, 16.20it/s, epoch=1, grad_norm=0.2444, loss=1.4448, lr=0.003776]\u001B[A\n",
      "Training:  40%|███▉      | 389/981 [00:30<00:36, 16.20it/s, epoch=1, grad_norm=0.2444, loss=1.4448, lr=0.003776]\u001B[A\n",
      "Training:  40%|███▉      | 389/981 [00:30<00:36, 16.20it/s, epoch=1, grad_norm=0.1728, loss=1.4310, lr=0.003768]\u001B[A\n",
      "Training:  40%|███▉      | 390/981 [00:30<00:36, 16.20it/s, epoch=1, grad_norm=0.1864, loss=1.4430, lr=0.003760]\u001B[A\n",
      "Training:  40%|███▉      | 391/981 [00:30<00:36, 16.24it/s, epoch=1, grad_norm=0.1864, loss=1.4430, lr=0.003760]\u001B[A\n",
      "Training:  40%|███▉      | 391/981 [00:30<00:36, 16.24it/s, epoch=1, grad_norm=0.2046, loss=1.4343, lr=0.003753]\u001B[A\n",
      "Training:  40%|███▉      | 392/981 [00:30<00:36, 16.24it/s, epoch=1, grad_norm=0.1516, loss=1.4304, lr=0.003745]\u001B[A\n",
      "Training:  40%|████      | 393/981 [00:30<00:36, 16.25it/s, epoch=1, grad_norm=0.1516, loss=1.4304, lr=0.003745]\u001B[A\n",
      "Training:  40%|████      | 393/981 [00:30<00:36, 16.25it/s, epoch=1, grad_norm=0.1794, loss=1.4296, lr=0.003737]\u001B[A\n",
      "Training:  40%|████      | 394/981 [00:31<00:36, 16.25it/s, epoch=1, grad_norm=0.1601, loss=1.4397, lr=0.003729]\u001B[A\n",
      "Training:  40%|████      | 395/981 [00:31<00:36, 16.26it/s, epoch=1, grad_norm=0.1601, loss=1.4397, lr=0.003729]\u001B[A\n",
      "Training:  40%|████      | 395/981 [00:31<00:36, 16.26it/s, epoch=1, grad_norm=0.1651, loss=1.4328, lr=0.003722]\u001B[A\n",
      "Training:  40%|████      | 396/981 [00:31<00:35, 16.26it/s, epoch=1, grad_norm=0.1925, loss=1.4357, lr=0.003714]\u001B[A\n",
      "Training:  40%|████      | 397/981 [00:31<00:35, 16.24it/s, epoch=1, grad_norm=0.1925, loss=1.4357, lr=0.003714]\u001B[A\n",
      "Training:  40%|████      | 397/981 [00:31<00:35, 16.24it/s, epoch=1, grad_norm=0.2419, loss=1.4372, lr=0.003706]\u001B[A\n",
      "Training:  41%|████      | 398/981 [00:31<00:35, 16.24it/s, epoch=1, grad_norm=0.2697, loss=1.4469, lr=0.003698]\u001B[A\n",
      "Training:  41%|████      | 399/981 [00:31<00:35, 16.18it/s, epoch=1, grad_norm=0.2697, loss=1.4469, lr=0.003698]\u001B[A\n",
      "Training:  41%|████      | 399/981 [00:31<00:35, 16.18it/s, epoch=1, grad_norm=0.2118, loss=1.4402, lr=0.003690]\u001B[A\n",
      "Training:  41%|████      | 400/981 [00:33<00:35, 16.18it/s, epoch=1, grad_norm=0.1674, loss=1.4328, lr=0.003683]\u001B[A\n",
      "Training:  41%|████      | 401/981 [00:33<04:00,  2.41it/s, epoch=1, grad_norm=0.1674, loss=1.4328, lr=0.003683]\u001B[A\n",
      "Training:  41%|████      | 401/981 [00:33<04:00,  2.41it/s, epoch=1, grad_norm=0.1904, loss=1.4296, lr=0.003675]\u001B[A\n",
      "Training:  41%|████      | 402/981 [00:33<04:00,  2.41it/s, epoch=1, grad_norm=0.1951, loss=1.4270, lr=0.003667]\u001B[A\n",
      "Training:  41%|████      | 403/981 [00:33<02:57,  3.25it/s, epoch=1, grad_norm=0.1951, loss=1.4270, lr=0.003667]\u001B[A\n",
      "Training:  41%|████      | 403/981 [00:33<02:57,  3.25it/s, epoch=1, grad_norm=0.1675, loss=1.4286, lr=0.003659]\u001B[A\n",
      "Training:  41%|████      | 404/981 [00:34<02:57,  3.25it/s, epoch=1, grad_norm=0.1427, loss=1.4307, lr=0.003651]\u001B[A\n",
      "Training:  41%|████▏     | 405/981 [00:34<02:13,  4.30it/s, epoch=1, grad_norm=0.1427, loss=1.4307, lr=0.003651]\u001B[A\n",
      "Training:  41%|████▏     | 405/981 [00:34<02:13,  4.30it/s, epoch=1, grad_norm=0.1436, loss=1.4201, lr=0.003643]\u001B[A\n",
      "Training:  41%|████▏     | 406/981 [00:34<02:13,  4.30it/s, epoch=1, grad_norm=0.1540, loss=1.4250, lr=0.003635]\u001B[A\n",
      "Training:  41%|████▏     | 407/981 [00:34<01:43,  5.56it/s, epoch=1, grad_norm=0.1540, loss=1.4250, lr=0.003635]\u001B[A\n",
      "Training:  41%|████▏     | 407/981 [00:34<01:43,  5.56it/s, epoch=1, grad_norm=0.1362, loss=1.4152, lr=0.003627]\u001B[A\n",
      "Training:  42%|████▏     | 408/981 [00:34<01:43,  5.56it/s, epoch=1, grad_norm=0.1296, loss=1.4245, lr=0.003619]\u001B[A\n",
      "Training:  42%|████▏     | 409/981 [00:34<01:21,  6.98it/s, epoch=1, grad_norm=0.1296, loss=1.4245, lr=0.003619]\u001B[A\n",
      "Training:  42%|████▏     | 409/981 [00:34<01:21,  6.98it/s, epoch=1, grad_norm=0.1589, loss=1.4336, lr=0.003612]\u001B[A\n",
      "Training:  42%|████▏     | 410/981 [00:34<01:21,  6.98it/s, epoch=1, grad_norm=0.1925, loss=1.4205, lr=0.003604]\u001B[A\n",
      "Training:  42%|████▏     | 411/981 [00:34<01:06,  8.51it/s, epoch=1, grad_norm=0.1925, loss=1.4205, lr=0.003604]\u001B[A\n",
      "Training:  42%|████▏     | 411/981 [00:34<01:06,  8.51it/s, epoch=1, grad_norm=0.1979, loss=1.4224, lr=0.003596]\u001B[A\n",
      "Training:  42%|████▏     | 412/981 [00:34<01:06,  8.51it/s, epoch=1, grad_norm=0.1591, loss=1.4187, lr=0.003588]\u001B[A\n",
      "Training:  42%|████▏     | 413/981 [00:34<00:56, 10.05it/s, epoch=1, grad_norm=0.1591, loss=1.4187, lr=0.003588]\u001B[A\n",
      "Training:  42%|████▏     | 413/981 [00:34<00:56, 10.05it/s, epoch=1, grad_norm=0.1540, loss=1.4207, lr=0.003580]\u001B[A\n",
      "Training:  42%|████▏     | 414/981 [00:34<00:56, 10.05it/s, epoch=1, grad_norm=0.1707, loss=1.4160, lr=0.003572]\u001B[A\n",
      "Training:  42%|████▏     | 415/981 [00:34<00:49, 11.52it/s, epoch=1, grad_norm=0.1707, loss=1.4160, lr=0.003572]\u001B[A\n",
      "Training:  42%|████▏     | 415/981 [00:34<00:49, 11.52it/s, epoch=1, grad_norm=0.1934, loss=1.4128, lr=0.003563]\u001B[A\n",
      "Training:  42%|████▏     | 416/981 [00:34<00:49, 11.52it/s, epoch=1, grad_norm=0.2040, loss=1.4258, lr=0.003555]\u001B[A\n",
      "Training:  43%|████▎     | 417/981 [00:34<00:44, 12.80it/s, epoch=1, grad_norm=0.2040, loss=1.4258, lr=0.003555]\u001B[A\n",
      "Training:  43%|████▎     | 417/981 [00:34<00:44, 12.80it/s, epoch=1, grad_norm=0.1499, loss=1.4194, lr=0.003547]\u001B[A\n",
      "Training:  43%|████▎     | 418/981 [00:34<00:43, 12.80it/s, epoch=1, grad_norm=0.1467, loss=1.4165, lr=0.003539]\u001B[A\n",
      "Training:  43%|████▎     | 419/981 [00:34<00:40, 13.94it/s, epoch=1, grad_norm=0.1467, loss=1.4165, lr=0.003539]\u001B[A\n",
      "Training:  43%|████▎     | 419/981 [00:34<00:40, 13.94it/s, epoch=1, grad_norm=0.1643, loss=1.4160, lr=0.003531]\u001B[A\n",
      "Training:  43%|████▎     | 420/981 [00:34<00:40, 13.94it/s, epoch=1, grad_norm=0.1644, loss=1.4069, lr=0.003523]\u001B[A\n",
      "Training:  43%|████▎     | 421/981 [00:34<00:37, 14.81it/s, epoch=1, grad_norm=0.1644, loss=1.4069, lr=0.003523]\u001B[A\n",
      "Training:  43%|████▎     | 421/981 [00:34<00:37, 14.81it/s, epoch=1, grad_norm=0.1275, loss=1.4105, lr=0.003515]\u001B[A\n",
      "Training:  43%|████▎     | 422/981 [00:35<00:37, 14.81it/s, epoch=1, grad_norm=0.1550, loss=1.4153, lr=0.003507]\u001B[A\n",
      "Training:  43%|████▎     | 423/981 [00:35<00:35, 15.53it/s, epoch=1, grad_norm=0.1550, loss=1.4153, lr=0.003507]\u001B[A\n",
      "Training:  43%|████▎     | 423/981 [00:35<00:35, 15.53it/s, epoch=1, grad_norm=0.1891, loss=1.4139, lr=0.003499]\u001B[A\n",
      "Training:  43%|████▎     | 424/981 [00:35<00:35, 15.53it/s, epoch=1, grad_norm=0.1734, loss=1.4113, lr=0.003491]\u001B[A\n",
      "Training:  43%|████▎     | 425/981 [00:35<00:34, 16.05it/s, epoch=1, grad_norm=0.1734, loss=1.4113, lr=0.003491]\u001B[A\n",
      "Training:  43%|████▎     | 425/981 [00:35<00:34, 16.05it/s, epoch=1, grad_norm=0.1524, loss=1.4025, lr=0.003482]\u001B[A\n",
      "Training:  43%|████▎     | 426/981 [00:35<00:34, 16.05it/s, epoch=1, grad_norm=0.1462, loss=1.4232, lr=0.003474]\u001B[A\n",
      "Training:  44%|████▎     | 427/981 [00:35<00:33, 16.46it/s, epoch=1, grad_norm=0.1462, loss=1.4232, lr=0.003474]\u001B[A\n",
      "Training:  44%|████▎     | 427/981 [00:35<00:33, 16.46it/s, epoch=1, grad_norm=0.1528, loss=1.4191, lr=0.003466]\u001B[A\n",
      "Training:  44%|████▎     | 428/981 [00:35<00:33, 16.46it/s, epoch=1, grad_norm=0.1631, loss=1.4136, lr=0.003458]\u001B[A\n",
      "Training:  44%|████▎     | 429/981 [00:35<00:33, 16.72it/s, epoch=1, grad_norm=0.1631, loss=1.4136, lr=0.003458]\u001B[A\n",
      "Training:  44%|████▎     | 429/981 [00:35<00:33, 16.72it/s, epoch=1, grad_norm=0.1908, loss=1.4134, lr=0.003450]\u001B[A\n",
      "Training:  44%|████▍     | 430/981 [00:35<00:32, 16.72it/s, epoch=1, grad_norm=0.1738, loss=1.4148, lr=0.003441]\u001B[A\n",
      "Training:  44%|████▍     | 431/981 [00:35<00:32, 16.96it/s, epoch=1, grad_norm=0.1738, loss=1.4148, lr=0.003441]\u001B[A\n",
      "Training:  44%|████▍     | 431/981 [00:35<00:32, 16.96it/s, epoch=1, grad_norm=0.1534, loss=1.4092, lr=0.003433]\u001B[A\n",
      "Training:  44%|████▍     | 432/981 [00:35<00:32, 16.96it/s, epoch=1, grad_norm=0.1448, loss=1.4171, lr=0.003425]\u001B[A\n",
      "Training:  44%|████▍     | 433/981 [00:35<00:32, 17.08it/s, epoch=1, grad_norm=0.1448, loss=1.4171, lr=0.003425]\u001B[A\n",
      "Training:  44%|████▍     | 433/981 [00:35<00:32, 17.08it/s, epoch=1, grad_norm=0.1466, loss=1.3940, lr=0.003417]\u001B[A\n",
      "Training:  44%|████▍     | 434/981 [00:35<00:32, 17.08it/s, epoch=1, grad_norm=0.1772, loss=1.4100, lr=0.003408]\u001B[A\n",
      "Training:  44%|████▍     | 435/981 [00:35<00:31, 17.16it/s, epoch=1, grad_norm=0.1772, loss=1.4100, lr=0.003408]\u001B[A\n",
      "Training:  44%|████▍     | 435/981 [00:35<00:31, 17.16it/s, epoch=1, grad_norm=0.1738, loss=1.4008, lr=0.003400]\u001B[A\n",
      "Training:  44%|████▍     | 436/981 [00:35<00:31, 17.16it/s, epoch=1, grad_norm=0.1789, loss=1.4045, lr=0.003392]\u001B[A\n",
      "Training:  45%|████▍     | 437/981 [00:35<00:31, 17.18it/s, epoch=1, grad_norm=0.1789, loss=1.4045, lr=0.003392]\u001B[A\n",
      "Training:  45%|████▍     | 437/981 [00:35<00:31, 17.18it/s, epoch=1, grad_norm=0.1502, loss=1.4042, lr=0.003383]\u001B[A\n",
      "Training:  45%|████▍     | 438/981 [00:35<00:31, 17.18it/s, epoch=1, grad_norm=0.1673, loss=1.4095, lr=0.003375]\u001B[A\n",
      "Training:  45%|████▍     | 439/981 [00:35<00:31, 17.26it/s, epoch=1, grad_norm=0.1673, loss=1.4095, lr=0.003375]\u001B[A\n",
      "Training:  45%|████▍     | 439/981 [00:36<00:31, 17.26it/s, epoch=1, grad_norm=0.1754, loss=1.4053, lr=0.003367]\u001B[A\n",
      "Training:  45%|████▍     | 440/981 [00:36<00:31, 17.26it/s, epoch=1, grad_norm=0.1853, loss=1.3994, lr=0.003358]\u001B[A\n",
      "Training:  45%|████▍     | 441/981 [00:36<00:31, 17.32it/s, epoch=1, grad_norm=0.1853, loss=1.3994, lr=0.003358]\u001B[A\n",
      "Training:  45%|████▍     | 441/981 [00:36<00:31, 17.32it/s, epoch=1, grad_norm=0.1636, loss=1.4057, lr=0.003350]\u001B[A\n",
      "Training:  45%|████▌     | 442/981 [00:36<00:31, 17.32it/s, epoch=1, grad_norm=0.1559, loss=1.3960, lr=0.003342]\u001B[A\n",
      "Training:  45%|████▌     | 443/981 [00:36<00:30, 17.39it/s, epoch=1, grad_norm=0.1559, loss=1.3960, lr=0.003342]\u001B[A\n",
      "Training:  45%|████▌     | 443/981 [00:36<00:30, 17.39it/s, epoch=1, grad_norm=0.1492, loss=1.4028, lr=0.003333]\u001B[A\n",
      "Training:  45%|████▌     | 444/981 [00:36<00:30, 17.39it/s, epoch=1, grad_norm=0.1705, loss=1.4054, lr=0.003325]\u001B[A\n",
      "Training:  45%|████▌     | 445/981 [00:36<00:30, 17.40it/s, epoch=1, grad_norm=0.1705, loss=1.4054, lr=0.003325]\u001B[A\n",
      "Training:  45%|████▌     | 445/981 [00:36<00:30, 17.40it/s, epoch=1, grad_norm=0.1869, loss=1.4056, lr=0.003316]\u001B[A\n",
      "Training:  45%|████▌     | 446/981 [00:36<00:30, 17.40it/s, epoch=1, grad_norm=0.1874, loss=1.4032, lr=0.003308]\u001B[A\n",
      "Training:  46%|████▌     | 447/981 [00:36<00:30, 17.36it/s, epoch=1, grad_norm=0.1874, loss=1.4032, lr=0.003308]\u001B[A\n",
      "Training:  46%|████▌     | 447/981 [00:36<00:30, 17.36it/s, epoch=1, grad_norm=0.1729, loss=1.3977, lr=0.003300]\u001B[A\n",
      "Training:  46%|████▌     | 448/981 [00:36<00:30, 17.36it/s, epoch=1, grad_norm=0.1418, loss=1.3912, lr=0.003291]\u001B[A\n",
      "Training:  46%|████▌     | 449/981 [00:36<00:30, 17.33it/s, epoch=1, grad_norm=0.1418, loss=1.3912, lr=0.003291]\u001B[A\n",
      "Training:  46%|████▌     | 449/981 [00:36<00:30, 17.33it/s, epoch=1, grad_norm=0.1586, loss=1.3969, lr=0.003283]\u001B[A\n",
      "Training:  46%|████▌     | 450/981 [00:36<00:30, 17.33it/s, epoch=1, grad_norm=0.1354, loss=1.3962, lr=0.003274]\u001B[A\n",
      "Training:  46%|████▌     | 451/981 [00:36<00:30, 17.32it/s, epoch=1, grad_norm=0.1354, loss=1.3962, lr=0.003274]\u001B[A\n",
      "Training:  46%|████▌     | 451/981 [00:36<00:30, 17.32it/s, epoch=1, grad_norm=0.1549, loss=1.3947, lr=0.003266]\u001B[A\n",
      "Training:  46%|████▌     | 452/981 [00:36<00:30, 17.32it/s, epoch=1, grad_norm=0.1619, loss=1.4021, lr=0.003257]\u001B[A\n",
      "Training:  46%|████▌     | 453/981 [00:36<00:30, 17.36it/s, epoch=1, grad_norm=0.1619, loss=1.4021, lr=0.003257]\u001B[A\n",
      "Training:  46%|████▌     | 453/981 [00:36<00:30, 17.36it/s, epoch=1, grad_norm=0.1494, loss=1.3926, lr=0.003249]\u001B[A\n",
      "Training:  46%|████▋     | 454/981 [00:36<00:30, 17.36it/s, epoch=1, grad_norm=0.1523, loss=1.3907, lr=0.003240]\u001B[A\n",
      "Training:  46%|████▋     | 455/981 [00:36<00:30, 17.44it/s, epoch=1, grad_norm=0.1523, loss=1.3907, lr=0.003240]\u001B[A\n",
      "Training:  46%|████▋     | 455/981 [00:36<00:30, 17.44it/s, epoch=1, grad_norm=0.1385, loss=1.3935, lr=0.003232]\u001B[A\n",
      "Training:  46%|████▋     | 456/981 [00:36<00:30, 17.44it/s, epoch=1, grad_norm=0.1468, loss=1.3972, lr=0.003223]\u001B[A\n",
      "Training:  47%|████▋     | 457/981 [00:36<00:30, 17.43it/s, epoch=1, grad_norm=0.1468, loss=1.3972, lr=0.003223]\u001B[A\n",
      "Training:  47%|████▋     | 457/981 [00:37<00:30, 17.43it/s, epoch=1, grad_norm=0.1468, loss=1.3861, lr=0.003215]\u001B[A\n",
      "Training:  47%|████▋     | 458/981 [00:37<00:30, 17.43it/s, epoch=1, grad_norm=0.1606, loss=1.3946, lr=0.003206]\u001B[A\n",
      "Training:  47%|████▋     | 459/981 [00:37<00:29, 17.41it/s, epoch=1, grad_norm=0.1606, loss=1.3946, lr=0.003206]\u001B[A\n",
      "Training:  47%|████▋     | 459/981 [00:37<00:29, 17.41it/s, epoch=1, grad_norm=0.1921, loss=1.3911, lr=0.003198]\u001B[A\n",
      "Training:  47%|████▋     | 460/981 [00:37<00:29, 17.41it/s, epoch=1, grad_norm=0.1912, loss=1.3853, lr=0.003189]\u001B[A\n",
      "Training:  47%|████▋     | 461/981 [00:37<00:29, 17.39it/s, epoch=1, grad_norm=0.1912, loss=1.3853, lr=0.003189]\u001B[A\n",
      "Training:  47%|████▋     | 461/981 [00:37<00:29, 17.39it/s, epoch=1, grad_norm=0.2261, loss=1.3938, lr=0.003181]\u001B[A\n",
      "Training:  47%|████▋     | 462/981 [00:37<00:29, 17.39it/s, epoch=1, grad_norm=0.1878, loss=1.3882, lr=0.003172]\u001B[A\n",
      "Training:  47%|████▋     | 463/981 [00:37<00:29, 17.42it/s, epoch=1, grad_norm=0.1878, loss=1.3882, lr=0.003172]\u001B[A\n",
      "Training:  47%|████▋     | 463/981 [00:37<00:29, 17.42it/s, epoch=1, grad_norm=0.1401, loss=1.3951, lr=0.003163]\u001B[A\n",
      "Training:  47%|████▋     | 464/981 [00:37<00:29, 17.42it/s, epoch=1, grad_norm=0.1774, loss=1.3912, lr=0.003155]\u001B[A\n",
      "Training:  47%|████▋     | 465/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.1774, loss=1.3912, lr=0.003155]\u001B[A\n",
      "Training:  47%|████▋     | 465/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.2041, loss=1.4029, lr=0.003146]\u001B[A\n",
      "Training:  48%|████▊     | 466/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.1835, loss=1.3871, lr=0.003138]\u001B[A\n",
      "Training:  48%|████▊     | 467/981 [00:37<00:29, 17.36it/s, epoch=1, grad_norm=0.1835, loss=1.3871, lr=0.003138]\u001B[A\n",
      "Training:  48%|████▊     | 467/981 [00:37<00:29, 17.36it/s, epoch=1, grad_norm=0.1346, loss=1.3997, lr=0.003129]\u001B[A\n",
      "Training:  48%|████▊     | 468/981 [00:37<00:29, 17.36it/s, epoch=1, grad_norm=0.1580, loss=1.3850, lr=0.003121]\u001B[A\n",
      "Training:  48%|████▊     | 469/981 [00:37<00:29, 17.39it/s, epoch=1, grad_norm=0.1580, loss=1.3850, lr=0.003121]\u001B[A\n",
      "Training:  48%|████▊     | 469/981 [00:37<00:29, 17.39it/s, epoch=1, grad_norm=0.1846, loss=1.3999, lr=0.003112]\u001B[A\n",
      "Training:  48%|████▊     | 470/981 [00:37<00:29, 17.39it/s, epoch=1, grad_norm=0.1418, loss=1.3908, lr=0.003103]\u001B[A\n",
      "Training:  48%|████▊     | 471/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.1418, loss=1.3908, lr=0.003103]\u001B[A\n",
      "Training:  48%|████▊     | 471/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.1754, loss=1.3842, lr=0.003095]\u001B[A\n",
      "Training:  48%|████▊     | 472/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.2069, loss=1.3875, lr=0.003086]\u001B[A\n",
      "Training:  48%|████▊     | 473/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.2069, loss=1.3875, lr=0.003086]\u001B[A\n",
      "Training:  48%|████▊     | 473/981 [00:37<00:29, 17.37it/s, epoch=1, grad_norm=0.1506, loss=1.3761, lr=0.003077]\u001B[A\n",
      "Training:  48%|████▊     | 474/981 [00:38<00:29, 17.37it/s, epoch=1, grad_norm=0.1463, loss=1.3810, lr=0.003069]\u001B[A\n",
      "Training:  48%|████▊     | 475/981 [00:38<00:29, 17.38it/s, epoch=1, grad_norm=0.1463, loss=1.3810, lr=0.003069]\u001B[A\n",
      "Training:  48%|████▊     | 475/981 [00:38<00:29, 17.38it/s, epoch=1, grad_norm=0.1720, loss=1.3863, lr=0.003060]\u001B[A\n",
      "Training:  49%|████▊     | 476/981 [00:38<00:29, 17.38it/s, epoch=1, grad_norm=0.1780, loss=1.3828, lr=0.003051]\u001B[A\n",
      "Training:  49%|████▊     | 477/981 [00:38<00:28, 17.39it/s, epoch=1, grad_norm=0.1780, loss=1.3828, lr=0.003051]\u001B[A\n",
      "Training:  49%|████▊     | 477/981 [00:38<00:28, 17.39it/s, epoch=1, grad_norm=0.1280, loss=1.3853, lr=0.003043]\u001B[A\n",
      "Training:  49%|████▊     | 478/981 [00:38<00:28, 17.39it/s, epoch=1, grad_norm=0.1507, loss=1.3830, lr=0.003034]\u001B[A\n",
      "Training:  49%|████▉     | 479/981 [00:38<00:28, 17.43it/s, epoch=1, grad_norm=0.1507, loss=1.3830, lr=0.003034]\u001B[A\n",
      "Training:  49%|████▉     | 479/981 [00:38<00:28, 17.43it/s, epoch=1, grad_norm=0.1570, loss=1.3852, lr=0.003025]\u001B[A\n",
      "Training:  49%|████▉     | 480/981 [00:38<00:28, 17.43it/s, epoch=1, grad_norm=0.1582, loss=1.3904, lr=0.003017]\u001B[A\n",
      "Training:  49%|████▉     | 481/981 [00:38<00:28, 17.46it/s, epoch=1, grad_norm=0.1582, loss=1.3904, lr=0.003017]\u001B[A\n",
      "Training:  49%|████▉     | 481/981 [00:38<00:28, 17.46it/s, epoch=1, grad_norm=0.1458, loss=1.3784, lr=0.003008]\u001B[A\n",
      "Training:  49%|████▉     | 482/981 [00:38<00:28, 17.46it/s, epoch=1, grad_norm=0.1427, loss=1.3807, lr=0.002999]\u001B[A\n",
      "Training:  49%|████▉     | 483/981 [00:38<00:28, 17.46it/s, epoch=1, grad_norm=0.1427, loss=1.3807, lr=0.002999]\u001B[A\n",
      "Training:  49%|████▉     | 483/981 [00:38<00:28, 17.46it/s, epoch=1, grad_norm=0.1532, loss=1.3827, lr=0.002990]\u001B[A\n",
      "Training:  49%|████▉     | 484/981 [00:38<00:28, 17.46it/s, epoch=1, grad_norm=0.1587, loss=1.3834, lr=0.002982]\u001B[A\n",
      "Training:  49%|████▉     | 485/981 [00:38<00:28, 17.37it/s, epoch=1, grad_norm=0.1587, loss=1.3834, lr=0.002982]\u001B[A\n",
      "Training:  49%|████▉     | 485/981 [00:38<00:28, 17.37it/s, epoch=1, grad_norm=0.1483, loss=1.3805, lr=0.002973]\u001B[A\n",
      "Training:  50%|████▉     | 486/981 [00:38<00:28, 17.37it/s, epoch=1, grad_norm=0.1303, loss=1.3786, lr=0.002964]\u001B[A\n",
      "Training:  50%|████▉     | 487/981 [00:38<00:28, 17.36it/s, epoch=1, grad_norm=0.1303, loss=1.3786, lr=0.002964]\u001B[A\n",
      "Training:  50%|████▉     | 487/981 [00:38<00:28, 17.36it/s, epoch=1, grad_norm=0.1395, loss=1.3789, lr=0.002956]\u001B[A\n",
      "Training:  50%|████▉     | 488/981 [00:38<00:28, 17.36it/s, epoch=1, grad_norm=0.1474, loss=1.3754, lr=0.002947]\u001B[A\n",
      "Training:  50%|████▉     | 489/981 [00:38<00:28, 17.36it/s, epoch=1, grad_norm=0.1474, loss=1.3754, lr=0.002947]\u001B[A\n",
      "Training:  50%|████▉     | 489/981 [00:38<00:28, 17.36it/s, epoch=1, grad_norm=0.1455, loss=1.3790, lr=0.002938]\u001B[A\n",
      "Training:  50%|████▉     | 490/981 [00:38<00:28, 17.36it/s, epoch=1, grad_norm=0.1555, loss=1.3736, lr=0.002929]\u001B[A\n",
      "Training:  50%|█████     | 491/981 [00:38<00:28, 17.40it/s, epoch=1, grad_norm=0.1555, loss=1.3736, lr=0.002929]\u001B[A\n",
      "Training:  50%|█████     | 491/981 [00:39<00:28, 17.40it/s, epoch=1, grad_norm=0.1631, loss=1.3843, lr=0.002920]\u001B[A\n",
      "Training:  50%|█████     | 492/981 [00:39<00:28, 17.40it/s, epoch=1, grad_norm=0.1664, loss=1.3879, lr=0.002912]\u001B[A\n",
      "Training:  50%|█████     | 493/981 [00:39<00:28, 17.39it/s, epoch=1, grad_norm=0.1664, loss=1.3879, lr=0.002912]\u001B[A\n",
      "Training:  50%|█████     | 493/981 [00:39<00:28, 17.39it/s, epoch=1, grad_norm=0.1933, loss=1.3724, lr=0.002903]\u001B[A\n",
      "Training:  50%|█████     | 494/981 [00:39<00:28, 17.39it/s, epoch=1, grad_norm=0.2414, loss=1.3793, lr=0.002894]\u001B[A\n",
      "Training:  50%|█████     | 495/981 [00:39<00:27, 17.41it/s, epoch=1, grad_norm=0.2414, loss=1.3793, lr=0.002894]\u001B[A\n",
      "Training:  50%|█████     | 495/981 [00:39<00:27, 17.41it/s, epoch=1, grad_norm=0.2250, loss=1.3810, lr=0.002885]\u001B[A\n",
      "Training:  51%|█████     | 496/981 [00:39<00:27, 17.41it/s, epoch=1, grad_norm=0.1469, loss=1.3801, lr=0.002877]\u001B[A\n",
      "Training:  51%|█████     | 497/981 [00:39<00:27, 17.42it/s, epoch=1, grad_norm=0.1469, loss=1.3801, lr=0.002877]\u001B[A\n",
      "Training:  51%|█████     | 497/981 [00:39<00:27, 17.42it/s, epoch=1, grad_norm=0.1721, loss=1.3708, lr=0.002868]\u001B[A\n",
      "Training:  51%|█████     | 498/981 [00:39<00:27, 17.42it/s, epoch=1, grad_norm=0.1769, loss=1.3887, lr=0.002859]\u001B[A\n",
      "Training:  51%|█████     | 499/981 [00:39<00:27, 17.42it/s, epoch=1, grad_norm=0.1769, loss=1.3887, lr=0.002859]\u001B[A\n",
      "Training:  51%|█████     | 499/981 [00:39<00:27, 17.42it/s, epoch=1, grad_norm=0.1460, loss=1.3806, lr=0.002850]\u001B[A\n",
      "Training:  51%|█████     | 500/981 [00:41<00:27, 17.42it/s, epoch=1, grad_norm=0.1481, loss=1.3704, lr=0.002841]\u001B[A\n",
      "Training:  51%|█████     | 501/981 [00:41<03:14,  2.47it/s, epoch=1, grad_norm=0.1481, loss=1.3704, lr=0.002841]\u001B[A\n",
      "Training:  51%|█████     | 501/981 [00:41<03:14,  2.47it/s, epoch=1, grad_norm=0.1413, loss=1.3698, lr=0.002833]\u001B[A\n",
      "Training:  51%|█████     | 502/981 [00:41<03:13,  2.47it/s, epoch=1, grad_norm=0.1284, loss=1.3761, lr=0.002824]\u001B[A\n",
      "Training:  51%|█████▏    | 503/981 [00:41<02:23,  3.33it/s, epoch=1, grad_norm=0.1284, loss=1.3761, lr=0.002824]\u001B[A\n",
      "Training:  51%|█████▏    | 503/981 [00:42<02:23,  3.33it/s, epoch=1, grad_norm=0.1558, loss=1.3759, lr=0.002815]\u001B[A\n",
      "Training:  51%|█████▏    | 504/981 [00:42<02:23,  3.33it/s, epoch=1, grad_norm=0.1593, loss=1.3688, lr=0.002806]\u001B[A\n",
      "Training:  51%|█████▏    | 505/981 [00:42<01:48,  4.40it/s, epoch=1, grad_norm=0.1593, loss=1.3688, lr=0.002806]\u001B[A\n",
      "Training:  51%|█████▏    | 505/981 [00:42<01:48,  4.40it/s, epoch=1, grad_norm=0.1394, loss=1.3931, lr=0.002797]\u001B[A\n",
      "Training:  52%|█████▏    | 506/981 [00:42<01:47,  4.40it/s, epoch=1, grad_norm=0.1508, loss=1.3744, lr=0.002788]\u001B[A\n",
      "Training:  52%|█████▏    | 507/981 [00:42<01:23,  5.67it/s, epoch=1, grad_norm=0.1508, loss=1.3744, lr=0.002788]\u001B[A\n",
      "Training:  52%|█████▏    | 507/981 [00:42<01:23,  5.67it/s, epoch=1, grad_norm=0.1481, loss=1.3714, lr=0.002780]\u001B[A\n",
      "Training:  52%|█████▏    | 508/981 [00:42<01:23,  5.67it/s, epoch=1, grad_norm=0.1541, loss=1.3880, lr=0.002771]\u001B[A\n",
      "Training:  52%|█████▏    | 509/981 [00:42<01:06,  7.12it/s, epoch=1, grad_norm=0.1541, loss=1.3880, lr=0.002771]\u001B[A\n",
      "Training:  52%|█████▏    | 509/981 [00:42<01:06,  7.12it/s, epoch=1, grad_norm=0.1605, loss=1.3752, lr=0.002762]\u001B[A\n",
      "Training:  52%|█████▏    | 510/981 [00:42<01:06,  7.12it/s, epoch=1, grad_norm=0.1793, loss=1.3762, lr=0.002753]\u001B[A\n",
      "Training:  52%|█████▏    | 511/981 [00:42<00:54,  8.66it/s, epoch=1, grad_norm=0.1793, loss=1.3762, lr=0.002753]\u001B[A\n",
      "Training:  52%|█████▏    | 511/981 [00:42<00:54,  8.66it/s, epoch=1, grad_norm=0.1446, loss=1.3726, lr=0.002744]\u001B[A\n",
      "Training:  52%|█████▏    | 512/981 [00:42<00:54,  8.66it/s, epoch=1, grad_norm=0.1983, loss=1.3822, lr=0.002735]\u001B[A\n",
      "Training:  52%|█████▏    | 513/981 [00:42<00:45, 10.20it/s, epoch=1, grad_norm=0.1983, loss=1.3822, lr=0.002735]\u001B[A\n",
      "Training:  52%|█████▏    | 513/981 [00:42<00:45, 10.20it/s, epoch=1, grad_norm=0.1482, loss=1.3658, lr=0.002727]\u001B[A\n",
      "Training:  52%|█████▏    | 514/981 [00:42<00:45, 10.20it/s, epoch=1, grad_norm=0.1582, loss=1.3666, lr=0.002718]\u001B[A\n",
      "Training:  52%|█████▏    | 515/981 [00:42<00:39, 11.66it/s, epoch=1, grad_norm=0.1582, loss=1.3666, lr=0.002718]\u001B[A\n",
      "Training:  52%|█████▏    | 515/981 [00:42<00:39, 11.66it/s, epoch=1, grad_norm=0.1314, loss=1.3681, lr=0.002709]\u001B[A\n",
      "Training:  53%|█████▎    | 516/981 [00:42<00:39, 11.66it/s, epoch=1, grad_norm=0.1418, loss=1.3711, lr=0.002700]\u001B[A\n",
      "Training:  53%|█████▎    | 517/981 [00:42<00:35, 12.94it/s, epoch=1, grad_norm=0.1418, loss=1.3711, lr=0.002700]\u001B[A\n",
      "Training:  53%|█████▎    | 517/981 [00:42<00:35, 12.94it/s, epoch=1, grad_norm=0.1448, loss=1.3637, lr=0.002691]\u001B[A\n",
      "Training:  53%|█████▎    | 518/981 [00:42<00:35, 12.94it/s, epoch=1, grad_norm=0.1306, loss=1.3679, lr=0.002682]\u001B[A\n",
      "Training:  53%|█████▎    | 519/981 [00:42<00:32, 14.03it/s, epoch=1, grad_norm=0.1306, loss=1.3679, lr=0.002682]\u001B[A\n",
      "Training:  53%|█████▎    | 519/981 [00:42<00:32, 14.03it/s, epoch=1, grad_norm=0.1492, loss=1.3783, lr=0.002673]\u001B[A\n",
      "Training:  53%|█████▎    | 520/981 [00:42<00:32, 14.03it/s, epoch=1, grad_norm=0.1215, loss=1.3573, lr=0.002664]\u001B[A\n",
      "Training:  53%|█████▎    | 521/981 [00:42<00:30, 14.89it/s, epoch=1, grad_norm=0.1215, loss=1.3573, lr=0.002664]\u001B[A\n",
      "Training:  53%|█████▎    | 521/981 [00:43<00:30, 14.89it/s, epoch=1, grad_norm=0.1517, loss=1.3667, lr=0.002656]\u001B[A\n",
      "Training:  53%|█████▎    | 522/981 [00:43<00:30, 14.89it/s, epoch=1, grad_norm=0.1272, loss=1.3702, lr=0.002647]\u001B[A\n",
      "Training:  53%|█████▎    | 523/981 [00:43<00:29, 15.59it/s, epoch=1, grad_norm=0.1272, loss=1.3702, lr=0.002647]\u001B[A\n",
      "Training:  53%|█████▎    | 523/981 [00:43<00:29, 15.59it/s, epoch=1, grad_norm=0.1579, loss=1.3654, lr=0.002638]\u001B[A\n",
      "Training:  53%|█████▎    | 524/981 [00:43<00:29, 15.59it/s, epoch=1, grad_norm=0.1429, loss=1.3734, lr=0.002629]\u001B[A\n",
      "Training:  54%|█████▎    | 525/981 [00:43<00:28, 16.10it/s, epoch=1, grad_norm=0.1429, loss=1.3734, lr=0.002629]\u001B[A\n",
      "Training:  54%|█████▎    | 525/981 [00:43<00:28, 16.10it/s, epoch=1, grad_norm=0.1371, loss=1.3623, lr=0.002620]\u001B[A\n",
      "Training:  54%|█████▎    | 526/981 [00:43<00:28, 16.10it/s, epoch=1, grad_norm=0.1323, loss=1.3675, lr=0.002611]\u001B[A\n",
      "Training:  54%|█████▎    | 527/981 [00:43<00:27, 16.48it/s, epoch=1, grad_norm=0.1323, loss=1.3675, lr=0.002611]\u001B[A\n",
      "Training:  54%|█████▎    | 527/981 [00:43<00:27, 16.48it/s, epoch=1, grad_norm=0.1355, loss=1.3613, lr=0.002602]\u001B[A\n",
      "Training:  54%|█████▍    | 528/981 [00:43<00:27, 16.48it/s, epoch=1, grad_norm=0.1209, loss=1.3722, lr=0.002593]\u001B[A\n",
      "Training:  54%|█████▍    | 529/981 [00:43<00:26, 16.78it/s, epoch=1, grad_norm=0.1209, loss=1.3722, lr=0.002593]\u001B[A\n",
      "Training:  54%|█████▍    | 529/981 [00:43<00:26, 16.78it/s, epoch=1, grad_norm=0.1265, loss=1.3684, lr=0.002584]\u001B[A\n",
      "Training:  54%|█████▍    | 530/981 [00:43<00:26, 16.78it/s, epoch=1, grad_norm=0.1298, loss=1.3701, lr=0.002576]\u001B[A\n",
      "Training:  54%|█████▍    | 531/981 [00:43<00:26, 16.99it/s, epoch=1, grad_norm=0.1298, loss=1.3701, lr=0.002576]\u001B[A\n",
      "Training:  54%|█████▍    | 531/981 [00:43<00:26, 16.99it/s, epoch=1, grad_norm=0.1624, loss=1.3598, lr=0.002567]\u001B[A\n",
      "Training:  54%|█████▍    | 532/981 [00:43<00:26, 16.99it/s, epoch=1, grad_norm=0.1825, loss=1.3539, lr=0.002558]\u001B[A\n",
      "Training:  54%|█████▍    | 533/981 [00:43<00:26, 17.14it/s, epoch=1, grad_norm=0.1825, loss=1.3539, lr=0.002558]\u001B[A\n",
      "Training:  54%|█████▍    | 533/981 [00:43<00:26, 17.14it/s, epoch=1, grad_norm=0.1589, loss=1.3598, lr=0.002549]\u001B[A\n",
      "Training:  54%|█████▍    | 534/981 [00:43<00:26, 17.14it/s, epoch=1, grad_norm=0.1248, loss=1.3651, lr=0.002540]\u001B[A\n",
      "Training:  55%|█████▍    | 535/981 [00:43<00:25, 17.21it/s, epoch=1, grad_norm=0.1248, loss=1.3651, lr=0.002540]\u001B[A\n",
      "Training:  55%|█████▍    | 535/981 [00:43<00:25, 17.21it/s, epoch=1, grad_norm=0.1584, loss=1.3596, lr=0.002531]\u001B[A\n",
      "Training:  55%|█████▍    | 536/981 [00:43<00:25, 17.21it/s, epoch=1, grad_norm=0.1619, loss=1.3683, lr=0.002522]\u001B[A\n",
      "Training:  55%|█████▍    | 537/981 [00:43<00:25, 17.28it/s, epoch=1, grad_norm=0.1619, loss=1.3683, lr=0.002522]\u001B[A\n",
      "Training:  55%|█████▍    | 537/981 [00:43<00:25, 17.28it/s, epoch=1, grad_norm=0.1790, loss=1.3643, lr=0.002513]\u001B[A\n",
      "Training:  55%|█████▍    | 538/981 [00:44<00:25, 17.28it/s, epoch=1, grad_norm=0.1238, loss=1.3670, lr=0.002504]\u001B[A\n",
      "Training:  55%|█████▍    | 539/981 [00:44<00:25, 17.28it/s, epoch=1, grad_norm=0.1238, loss=1.3670, lr=0.002504]\u001B[A\n",
      "Training:  55%|█████▍    | 539/981 [00:44<00:25, 17.28it/s, epoch=1, grad_norm=0.1953, loss=1.3679, lr=0.002496]\u001B[A\n",
      "Training:  55%|█████▌    | 540/981 [00:44<00:25, 17.28it/s, epoch=1, grad_norm=0.2110, loss=1.3610, lr=0.002487]\u001B[A\n",
      "Training:  55%|█████▌    | 541/981 [00:44<00:25, 17.31it/s, epoch=1, grad_norm=0.2110, loss=1.3610, lr=0.002487]\u001B[A\n",
      "Training:  55%|█████▌    | 541/981 [00:44<00:25, 17.31it/s, epoch=1, grad_norm=0.1461, loss=1.3685, lr=0.002478]\u001B[A\n",
      "Training:  55%|█████▌    | 542/981 [00:44<00:25, 17.31it/s, epoch=1, grad_norm=0.1692, loss=1.3640, lr=0.002469]\u001B[A\n",
      "Training:  55%|█████▌    | 543/981 [00:44<00:25, 17.35it/s, epoch=1, grad_norm=0.1692, loss=1.3640, lr=0.002469]\u001B[A\n",
      "Training:  55%|█████▌    | 543/981 [00:44<00:25, 17.35it/s, epoch=1, grad_norm=0.1642, loss=1.3488, lr=0.002460]\u001B[A\n",
      "Training:  55%|█████▌    | 544/981 [00:44<00:25, 17.35it/s, epoch=1, grad_norm=0.1459, loss=1.3561, lr=0.002451]\u001B[A\n",
      "Training:  56%|█████▌    | 545/981 [00:44<00:25, 17.36it/s, epoch=1, grad_norm=0.1459, loss=1.3561, lr=0.002451]\u001B[A\n",
      "Training:  56%|█████▌    | 545/981 [00:44<00:25, 17.36it/s, epoch=1, grad_norm=0.1455, loss=1.3535, lr=0.002442]\u001B[A\n",
      "Training:  56%|█████▌    | 546/981 [00:44<00:25, 17.36it/s, epoch=1, grad_norm=0.1339, loss=1.3636, lr=0.002433]\u001B[A\n",
      "Training:  56%|█████▌    | 547/981 [00:44<00:24, 17.37it/s, epoch=1, grad_norm=0.1339, loss=1.3636, lr=0.002433]\u001B[A\n",
      "Training:  56%|█████▌    | 547/981 [00:44<00:24, 17.37it/s, epoch=1, grad_norm=0.1469, loss=1.3533, lr=0.002424]\u001B[A\n",
      "Training:  56%|█████▌    | 548/981 [00:44<00:24, 17.37it/s, epoch=1, grad_norm=0.1384, loss=1.3628, lr=0.002416]\u001B[A\n",
      "Training:  56%|█████▌    | 549/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1384, loss=1.3628, lr=0.002416]\u001B[A\n",
      "Training:  56%|█████▌    | 549/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1400, loss=1.3649, lr=0.002407]\u001B[A\n",
      "Training:  56%|█████▌    | 550/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1432, loss=1.3593, lr=0.002398]\u001B[A\n",
      "Training:  56%|█████▌    | 551/981 [00:44<00:24, 17.39it/s, epoch=1, grad_norm=0.1432, loss=1.3593, lr=0.002398]\u001B[A\n",
      "Training:  56%|█████▌    | 551/981 [00:44<00:24, 17.39it/s, epoch=1, grad_norm=0.1331, loss=1.3514, lr=0.002389]\u001B[A\n",
      "Training:  56%|█████▋    | 552/981 [00:44<00:24, 17.39it/s, epoch=1, grad_norm=0.1327, loss=1.3612, lr=0.002380]\u001B[A\n",
      "Training:  56%|█████▋    | 553/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1327, loss=1.3612, lr=0.002380]\u001B[A\n",
      "Training:  56%|█████▋    | 553/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1337, loss=1.3517, lr=0.002371]\u001B[A\n",
      "Training:  56%|█████▋    | 554/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1307, loss=1.3497, lr=0.002362]\u001B[A\n",
      "Training:  57%|█████▋    | 555/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1307, loss=1.3497, lr=0.002362]\u001B[A\n",
      "Training:  57%|█████▋    | 555/981 [00:44<00:24, 17.41it/s, epoch=1, grad_norm=0.1353, loss=1.3514, lr=0.002353]\u001B[A\n",
      "Training:  57%|█████▋    | 556/981 [00:45<00:24, 17.41it/s, epoch=1, grad_norm=0.1356, loss=1.3494, lr=0.002344]\u001B[A\n",
      "Training:  57%|█████▋    | 557/981 [00:45<00:24, 17.42it/s, epoch=1, grad_norm=0.1356, loss=1.3494, lr=0.002344]\u001B[A\n",
      "Training:  57%|█████▋    | 557/981 [00:45<00:24, 17.42it/s, epoch=1, grad_norm=0.1493, loss=1.3592, lr=0.002336]\u001B[A\n",
      "Training:  57%|█████▋    | 558/981 [00:45<00:24, 17.42it/s, epoch=1, grad_norm=0.1458, loss=1.3483, lr=0.002327]\u001B[A\n",
      "Training:  57%|█████▋    | 559/981 [00:45<00:24, 17.43it/s, epoch=1, grad_norm=0.1458, loss=1.3483, lr=0.002327]\u001B[A\n",
      "Training:  57%|█████▋    | 559/981 [00:45<00:24, 17.43it/s, epoch=1, grad_norm=0.1784, loss=1.3542, lr=0.002318]\u001B[A\n",
      "Training:  57%|█████▋    | 560/981 [00:45<00:24, 17.43it/s, epoch=1, grad_norm=0.1230, loss=1.3542, lr=0.002309]\u001B[A\n",
      "Training:  57%|█████▋    | 561/981 [00:45<00:24, 17.46it/s, epoch=1, grad_norm=0.1230, loss=1.3542, lr=0.002309]\u001B[A\n",
      "Training:  57%|█████▋    | 561/981 [00:45<00:24, 17.46it/s, epoch=1, grad_norm=0.1574, loss=1.3551, lr=0.002300]\u001B[A\n",
      "Training:  57%|█████▋    | 562/981 [00:45<00:23, 17.46it/s, epoch=1, grad_norm=0.1255, loss=1.3384, lr=0.002291]\u001B[A\n",
      "Training:  57%|█████▋    | 563/981 [00:45<00:23, 17.46it/s, epoch=1, grad_norm=0.1255, loss=1.3384, lr=0.002291]\u001B[A\n",
      "Training:  57%|█████▋    | 563/981 [00:45<00:23, 17.46it/s, epoch=1, grad_norm=0.1324, loss=1.3468, lr=0.002282]\u001B[A\n",
      "Training:  57%|█████▋    | 564/981 [00:45<00:23, 17.46it/s, epoch=1, grad_norm=0.1325, loss=1.3507, lr=0.002273]\u001B[A\n",
      "Training:  58%|█████▊    | 565/981 [00:45<00:23, 17.48it/s, epoch=1, grad_norm=0.1325, loss=1.3507, lr=0.002273]\u001B[A\n",
      "Training:  58%|█████▊    | 565/981 [00:45<00:23, 17.48it/s, epoch=1, grad_norm=0.1164, loss=1.3559, lr=0.002265]\u001B[A\n",
      "Training:  58%|█████▊    | 566/981 [00:45<00:23, 17.48it/s, epoch=1, grad_norm=0.1306, loss=1.3490, lr=0.002256]\u001B[A\n",
      "Training:  58%|█████▊    | 567/981 [00:45<00:23, 17.50it/s, epoch=1, grad_norm=0.1306, loss=1.3490, lr=0.002256]\u001B[A\n",
      "Training:  58%|█████▊    | 567/981 [00:45<00:23, 17.50it/s, epoch=1, grad_norm=0.1259, loss=1.3414, lr=0.002247]\u001B[A\n",
      "Training:  58%|█████▊    | 568/981 [00:45<00:23, 17.50it/s, epoch=1, grad_norm=0.1359, loss=1.3572, lr=0.002238]\u001B[A\n",
      "Training:  58%|█████▊    | 569/981 [00:45<00:23, 17.47it/s, epoch=1, grad_norm=0.1359, loss=1.3572, lr=0.002238]\u001B[A\n",
      "Training:  58%|█████▊    | 569/981 [00:45<00:23, 17.47it/s, epoch=1, grad_norm=0.1262, loss=1.3516, lr=0.002229]\u001B[A\n",
      "Training:  58%|█████▊    | 570/981 [00:45<00:23, 17.47it/s, epoch=1, grad_norm=0.1400, loss=1.3505, lr=0.002220]\u001B[A\n",
      "Training:  58%|█████▊    | 571/981 [00:45<00:23, 17.48it/s, epoch=1, grad_norm=0.1400, loss=1.3505, lr=0.002220]\u001B[A\n",
      "Training:  58%|█████▊    | 571/981 [00:45<00:23, 17.48it/s, epoch=1, grad_norm=0.1524, loss=1.3509, lr=0.002212]\u001B[A\n",
      "Training:  58%|█████▊    | 572/981 [00:45<00:23, 17.48it/s, epoch=1, grad_norm=0.1644, loss=1.3517, lr=0.002203]\u001B[A\n",
      "Training:  58%|█████▊    | 573/981 [00:45<00:23, 17.48it/s, epoch=1, grad_norm=0.1644, loss=1.3517, lr=0.002203]\u001B[A\n",
      "Training:  58%|█████▊    | 573/981 [00:46<00:23, 17.48it/s, epoch=1, grad_norm=0.1438, loss=1.3557, lr=0.002194]\u001B[A\n",
      "Training:  59%|█████▊    | 574/981 [00:46<00:23, 17.48it/s, epoch=1, grad_norm=0.1260, loss=1.3478, lr=0.002185]\u001B[A\n",
      "Training:  59%|█████▊    | 575/981 [00:46<00:23, 17.44it/s, epoch=1, grad_norm=0.1260, loss=1.3478, lr=0.002185]\u001B[A\n",
      "Training:  59%|█████▊    | 575/981 [00:46<00:23, 17.44it/s, epoch=1, grad_norm=0.1309, loss=1.3478, lr=0.002176]\u001B[A\n",
      "Training:  59%|█████▊    | 576/981 [00:46<00:23, 17.44it/s, epoch=1, grad_norm=0.1198, loss=1.3526, lr=0.002167]\u001B[A\n",
      "Training:  59%|█████▉    | 577/981 [00:46<00:23, 17.40it/s, epoch=1, grad_norm=0.1198, loss=1.3526, lr=0.002167]\u001B[A\n",
      "Training:  59%|█████▉    | 577/981 [00:46<00:23, 17.40it/s, epoch=1, grad_norm=0.1297, loss=1.3477, lr=0.002159]\u001B[A\n",
      "Training:  59%|█████▉    | 578/981 [00:46<00:23, 17.40it/s, epoch=1, grad_norm=0.1533, loss=1.3548, lr=0.002150]\u001B[A\n",
      "Training:  59%|█████▉    | 579/981 [00:46<00:23, 17.45it/s, epoch=1, grad_norm=0.1533, loss=1.3548, lr=0.002150]\u001B[A\n",
      "Training:  59%|█████▉    | 579/981 [00:46<00:23, 17.45it/s, epoch=1, grad_norm=0.1571, loss=1.3512, lr=0.002141]\u001B[A\n",
      "Training:  59%|█████▉    | 580/981 [00:46<00:22, 17.45it/s, epoch=1, grad_norm=0.1372, loss=1.3391, lr=0.002132]\u001B[A\n",
      "Training:  59%|█████▉    | 581/981 [00:46<00:22, 17.47it/s, epoch=1, grad_norm=0.1372, loss=1.3391, lr=0.002132]\u001B[A\n",
      "Training:  59%|█████▉    | 581/981 [00:46<00:22, 17.47it/s, epoch=1, grad_norm=0.1227, loss=1.3451, lr=0.002123]\u001B[A\n",
      "Training:  59%|█████▉    | 582/981 [00:46<00:22, 17.47it/s, epoch=1, grad_norm=0.1322, loss=1.3286, lr=0.002115]\u001B[A\n",
      "Training:  59%|█████▉    | 583/981 [00:46<00:22, 17.45it/s, epoch=1, grad_norm=0.1322, loss=1.3286, lr=0.002115]\u001B[A\n",
      "Training:  59%|█████▉    | 583/981 [00:46<00:22, 17.45it/s, epoch=1, grad_norm=0.1408, loss=1.3458, lr=0.002106]\u001B[A\n",
      "Training:  60%|█████▉    | 584/981 [00:46<00:22, 17.45it/s, epoch=1, grad_norm=0.1319, loss=1.3420, lr=0.002097]\u001B[A\n",
      "Training:  60%|█████▉    | 585/981 [00:46<00:22, 17.44it/s, epoch=1, grad_norm=0.1319, loss=1.3420, lr=0.002097]\u001B[A\n",
      "Training:  60%|█████▉    | 585/981 [00:46<00:22, 17.44it/s, epoch=1, grad_norm=0.1358, loss=1.3531, lr=0.002088]\u001B[A\n",
      "Training:  60%|█████▉    | 586/981 [00:46<00:22, 17.44it/s, epoch=1, grad_norm=0.1440, loss=1.3426, lr=0.002080]\u001B[A\n",
      "Training:  60%|█████▉    | 587/981 [00:46<00:22, 17.46it/s, epoch=1, grad_norm=0.1440, loss=1.3426, lr=0.002080]\u001B[A\n",
      "Training:  60%|█████▉    | 587/981 [00:46<00:22, 17.46it/s, epoch=1, grad_norm=0.1355, loss=1.3473, lr=0.002071]\u001B[A\n",
      "Training:  60%|█████▉    | 588/981 [00:46<00:22, 17.46it/s, epoch=1, grad_norm=0.1303, loss=1.3370, lr=0.002062]\u001B[A\n",
      "Training:  60%|██████    | 589/981 [00:46<00:22, 17.48it/s, epoch=1, grad_norm=0.1303, loss=1.3370, lr=0.002062]\u001B[A\n",
      "Training:  60%|██████    | 589/981 [00:46<00:22, 17.48it/s, epoch=1, grad_norm=0.1427, loss=1.3505, lr=0.002053]\u001B[A\n",
      "Training:  60%|██████    | 590/981 [00:46<00:22, 17.48it/s, epoch=1, grad_norm=0.1164, loss=1.3358, lr=0.002044]\u001B[A\n",
      "Training:  60%|██████    | 591/981 [00:46<00:22, 17.48it/s, epoch=1, grad_norm=0.1164, loss=1.3358, lr=0.002044]\u001B[A\n",
      "Training:  60%|██████    | 591/981 [00:47<00:22, 17.48it/s, epoch=1, grad_norm=0.1347, loss=1.3453, lr=0.002036]\u001B[A\n",
      "Training:  60%|██████    | 592/981 [00:47<00:22, 17.48it/s, epoch=1, grad_norm=0.1310, loss=1.3430, lr=0.002027]\u001B[A\n",
      "Training:  60%|██████    | 593/981 [00:47<00:22, 17.45it/s, epoch=1, grad_norm=0.1310, loss=1.3430, lr=0.002027]\u001B[A\n",
      "Training:  60%|██████    | 593/981 [00:47<00:22, 17.45it/s, epoch=1, grad_norm=0.1174, loss=1.3489, lr=0.002018]\u001B[A\n",
      "Training:  61%|██████    | 594/981 [00:47<00:22, 17.45it/s, epoch=1, grad_norm=0.1538, loss=1.3370, lr=0.002010]\u001B[A\n",
      "Training:  61%|██████    | 595/981 [00:47<00:22, 17.40it/s, epoch=1, grad_norm=0.1538, loss=1.3370, lr=0.002010]\u001B[A\n",
      "Training:  61%|██████    | 595/981 [00:47<00:22, 17.40it/s, epoch=1, grad_norm=0.1679, loss=1.3425, lr=0.002001]\u001B[A\n",
      "Training:  61%|██████    | 596/981 [00:47<00:22, 17.40it/s, epoch=1, grad_norm=0.1588, loss=1.3442, lr=0.001992]\u001B[A\n",
      "Training:  61%|██████    | 597/981 [00:47<00:22, 17.36it/s, epoch=1, grad_norm=0.1588, loss=1.3442, lr=0.001992]\u001B[A\n",
      "Training:  61%|██████    | 597/981 [00:47<00:22, 17.36it/s, epoch=1, grad_norm=0.1270, loss=1.3423, lr=0.001983]\u001B[A\n",
      "Training:  61%|██████    | 598/981 [00:47<00:22, 17.36it/s, epoch=1, grad_norm=0.1302, loss=1.3454, lr=0.001975]\u001B[A\n",
      "Training:  61%|██████    | 599/981 [00:47<00:21, 17.39it/s, epoch=1, grad_norm=0.1302, loss=1.3454, lr=0.001975]\u001B[A\n",
      "Training:  61%|██████    | 599/981 [00:47<00:21, 17.39it/s, epoch=1, grad_norm=0.1374, loss=1.3453, lr=0.001966]\u001B[A\n",
      "Training:  61%|██████    | 600/981 [00:49<00:21, 17.39it/s, epoch=1, grad_norm=0.1399, loss=1.3474, lr=0.001957]\u001B[A\n",
      "Training:  61%|██████▏   | 601/981 [00:49<02:30,  2.52it/s, epoch=1, grad_norm=0.1399, loss=1.3474, lr=0.001957]\u001B[A\n",
      "Training:  61%|██████▏   | 601/981 [00:49<02:30,  2.52it/s, epoch=1, grad_norm=0.1269, loss=1.3448, lr=0.001949]\u001B[A\n",
      "Training:  61%|██████▏   | 602/981 [00:49<02:30,  2.52it/s, epoch=1, grad_norm=0.1574, loss=1.3425, lr=0.001940]\u001B[A\n",
      "Training:  61%|██████▏   | 603/981 [00:49<01:51,  3.39it/s, epoch=1, grad_norm=0.1574, loss=1.3425, lr=0.001940]\u001B[A\n",
      "Training:  61%|██████▏   | 603/981 [00:49<01:51,  3.39it/s, epoch=1, grad_norm=0.1484, loss=1.3407, lr=0.001931]\u001B[A\n",
      "Training:  62%|██████▏   | 604/981 [00:50<01:51,  3.39it/s, epoch=1, grad_norm=0.1238, loss=1.3464, lr=0.001923]\u001B[A\n",
      "Training:  62%|██████▏   | 605/981 [00:50<01:24,  4.47it/s, epoch=1, grad_norm=0.1238, loss=1.3464, lr=0.001923]\u001B[A\n",
      "Training:  62%|██████▏   | 605/981 [00:50<01:24,  4.47it/s, epoch=1, grad_norm=0.1401, loss=1.3406, lr=0.001914]\u001B[A\n",
      "Training:  62%|██████▏   | 606/981 [00:50<01:23,  4.47it/s, epoch=1, grad_norm=0.1251, loss=1.3344, lr=0.001905]\u001B[A\n",
      "Training:  62%|██████▏   | 607/981 [00:50<01:05,  5.75it/s, epoch=1, grad_norm=0.1251, loss=1.3344, lr=0.001905]\u001B[A\n",
      "Training:  62%|██████▏   | 607/981 [00:50<01:05,  5.75it/s, epoch=1, grad_norm=0.1271, loss=1.3361, lr=0.001897]\u001B[A\n",
      "Training:  62%|██████▏   | 608/981 [00:50<01:04,  5.75it/s, epoch=1, grad_norm=0.1325, loss=1.3416, lr=0.001888]\u001B[A\n",
      "Training:  62%|██████▏   | 609/981 [00:50<00:51,  7.19it/s, epoch=1, grad_norm=0.1325, loss=1.3416, lr=0.001888]\u001B[A\n",
      "Training:  62%|██████▏   | 609/981 [00:50<00:51,  7.19it/s, epoch=1, grad_norm=0.1439, loss=1.3360, lr=0.001879]\u001B[A\n",
      "Training:  62%|██████▏   | 610/981 [00:50<00:51,  7.19it/s, epoch=1, grad_norm=0.1144, loss=1.3367, lr=0.001871]\u001B[A\n",
      "Training:  62%|██████▏   | 611/981 [00:50<00:42,  8.73it/s, epoch=1, grad_norm=0.1144, loss=1.3367, lr=0.001871]\u001B[A\n",
      "Training:  62%|██████▏   | 611/981 [00:50<00:42,  8.73it/s, epoch=1, grad_norm=0.1335, loss=1.3470, lr=0.001862]\u001B[A\n",
      "Training:  62%|██████▏   | 612/981 [00:50<00:42,  8.73it/s, epoch=1, grad_norm=0.1253, loss=1.3320, lr=0.001854]\u001B[A\n",
      "Training:  62%|██████▏   | 613/981 [00:50<00:35, 10.28it/s, epoch=1, grad_norm=0.1253, loss=1.3320, lr=0.001854]\u001B[A\n",
      "Training:  62%|██████▏   | 613/981 [00:50<00:35, 10.28it/s, epoch=1, grad_norm=0.1350, loss=1.3355, lr=0.001845]\u001B[A\n",
      "Training:  63%|██████▎   | 614/981 [00:50<00:35, 10.28it/s, epoch=1, grad_norm=0.1467, loss=1.3319, lr=0.001837]\u001B[A\n",
      "Training:  63%|██████▎   | 615/981 [00:50<00:31, 11.72it/s, epoch=1, grad_norm=0.1467, loss=1.3319, lr=0.001837]\u001B[A\n",
      "Training:  63%|██████▎   | 615/981 [00:50<00:31, 11.72it/s, epoch=1, grad_norm=0.1177, loss=1.3347, lr=0.001828]\u001B[A\n",
      "Training:  63%|██████▎   | 616/981 [00:50<00:31, 11.72it/s, epoch=1, grad_norm=0.1203, loss=1.3366, lr=0.001819]\u001B[A\n",
      "Training:  63%|██████▎   | 617/981 [00:50<00:28, 12.96it/s, epoch=1, grad_norm=0.1203, loss=1.3366, lr=0.001819]\u001B[A\n",
      "Training:  63%|██████▎   | 617/981 [00:50<00:28, 12.96it/s, epoch=1, grad_norm=0.1247, loss=1.3301, lr=0.001811]\u001B[A\n",
      "Training:  63%|██████▎   | 618/981 [00:50<00:27, 12.96it/s, epoch=1, grad_norm=0.1315, loss=1.3421, lr=0.001802]\u001B[A\n",
      "Training:  63%|██████▎   | 619/981 [00:50<00:25, 14.03it/s, epoch=1, grad_norm=0.1315, loss=1.3421, lr=0.001802]\u001B[A\n",
      "Training:  63%|██████▎   | 619/981 [00:50<00:25, 14.03it/s, epoch=1, grad_norm=0.1229, loss=1.3398, lr=0.001794]\u001B[A\n",
      "Training:  63%|██████▎   | 620/981 [00:50<00:25, 14.03it/s, epoch=1, grad_norm=0.1167, loss=1.3346, lr=0.001785]\u001B[A\n",
      "Training:  63%|██████▎   | 621/981 [00:50<00:24, 14.90it/s, epoch=1, grad_norm=0.1167, loss=1.3346, lr=0.001785]\u001B[A\n",
      "Training:  63%|██████▎   | 621/981 [00:51<00:24, 14.90it/s, epoch=1, grad_norm=0.1268, loss=1.3368, lr=0.001777]\u001B[A\n",
      "Training:  63%|██████▎   | 622/981 [00:51<00:24, 14.90it/s, epoch=1, grad_norm=0.1208, loss=1.3255, lr=0.001768]\u001B[A\n",
      "Training:  64%|██████▎   | 623/981 [00:51<00:22, 15.59it/s, epoch=1, grad_norm=0.1208, loss=1.3255, lr=0.001768]\u001B[A\n",
      "Training:  64%|██████▎   | 623/981 [00:51<00:22, 15.59it/s, epoch=1, grad_norm=0.1248, loss=1.3353, lr=0.001760]\u001B[A\n",
      "Training:  64%|██████▎   | 624/981 [00:51<00:22, 15.59it/s, epoch=1, grad_norm=0.1211, loss=1.3338, lr=0.001751]\u001B[A\n",
      "Training:  64%|██████▎   | 625/981 [00:51<00:22, 16.12it/s, epoch=1, grad_norm=0.1211, loss=1.3338, lr=0.001751]\u001B[A\n",
      "Training:  64%|██████▎   | 625/981 [00:51<00:22, 16.12it/s, epoch=1, grad_norm=0.1381, loss=1.3382, lr=0.001743]\u001B[A\n",
      "Training:  64%|██████▍   | 626/981 [00:51<00:22, 16.12it/s, epoch=1, grad_norm=0.1313, loss=1.3342, lr=0.001734]\u001B[A\n",
      "Training:  64%|██████▍   | 627/981 [00:51<00:21, 16.50it/s, epoch=1, grad_norm=0.1313, loss=1.3342, lr=0.001734]\u001B[A\n",
      "Training:  64%|██████▍   | 627/981 [00:51<00:21, 16.50it/s, epoch=1, grad_norm=0.1280, loss=1.3313, lr=0.001726]\u001B[A\n",
      "Training:  64%|██████▍   | 628/981 [00:51<00:21, 16.50it/s, epoch=1, grad_norm=0.1237, loss=1.3360, lr=0.001717]\u001B[A\n",
      "Training:  64%|██████▍   | 629/981 [00:51<00:21, 16.75it/s, epoch=1, grad_norm=0.1237, loss=1.3360, lr=0.001717]\u001B[A\n",
      "Training:  64%|██████▍   | 629/981 [00:51<00:21, 16.75it/s, epoch=1, grad_norm=0.1393, loss=1.3341, lr=0.001709]\u001B[A\n",
      "Training:  64%|██████▍   | 630/981 [00:51<00:20, 16.75it/s, epoch=1, grad_norm=0.1271, loss=1.3319, lr=0.001700]\u001B[A\n",
      "Training:  64%|██████▍   | 631/981 [00:51<00:20, 16.90it/s, epoch=1, grad_norm=0.1271, loss=1.3319, lr=0.001700]\u001B[A\n",
      "Training:  64%|██████▍   | 631/981 [00:51<00:20, 16.90it/s, epoch=1, grad_norm=0.1222, loss=1.3279, lr=0.001692]\u001B[A\n",
      "Training:  64%|██████▍   | 632/981 [00:51<00:20, 16.90it/s, epoch=1, grad_norm=0.1228, loss=1.3336, lr=0.001684]\u001B[A\n",
      "Training:  65%|██████▍   | 633/981 [00:51<00:20, 17.07it/s, epoch=1, grad_norm=0.1228, loss=1.3336, lr=0.001684]\u001B[A\n",
      "Training:  65%|██████▍   | 633/981 [00:51<00:20, 17.07it/s, epoch=1, grad_norm=0.1358, loss=1.3267, lr=0.001675]\u001B[A\n",
      "Training:  65%|██████▍   | 634/981 [00:51<00:20, 17.07it/s, epoch=1, grad_norm=0.1361, loss=1.3331, lr=0.001667]\u001B[A\n",
      "Training:  65%|██████▍   | 635/981 [00:51<00:20, 17.18it/s, epoch=1, grad_norm=0.1361, loss=1.3331, lr=0.001667]\u001B[A\n",
      "Training:  65%|██████▍   | 635/981 [00:51<00:20, 17.18it/s, epoch=1, grad_norm=0.1173, loss=1.3332, lr=0.001658]\u001B[A\n",
      "Training:  65%|██████▍   | 636/981 [00:51<00:20, 17.18it/s, epoch=1, grad_norm=0.1303, loss=1.3272, lr=0.001650]\u001B[A\n",
      "Training:  65%|██████▍   | 637/981 [00:51<00:20, 17.18it/s, epoch=1, grad_norm=0.1303, loss=1.3272, lr=0.001650]\u001B[A\n",
      "Training:  65%|██████▍   | 637/981 [00:51<00:20, 17.18it/s, epoch=1, grad_norm=0.1228, loss=1.3312, lr=0.001642]\u001B[A\n",
      "Training:  65%|██████▌   | 638/981 [00:52<00:19, 17.18it/s, epoch=1, grad_norm=0.1191, loss=1.3274, lr=0.001633]\u001B[A\n",
      "Training:  65%|██████▌   | 639/981 [00:52<00:19, 17.24it/s, epoch=1, grad_norm=0.1191, loss=1.3274, lr=0.001633]\u001B[A\n",
      "Training:  65%|██████▌   | 639/981 [00:52<00:19, 17.24it/s, epoch=1, grad_norm=0.1324, loss=1.3293, lr=0.001625]\u001B[A\n",
      "Training:  65%|██████▌   | 640/981 [00:52<00:19, 17.24it/s, epoch=1, grad_norm=0.1219, loss=1.3252, lr=0.001617]\u001B[A\n",
      "Training:  65%|██████▌   | 641/981 [00:52<00:19, 17.25it/s, epoch=1, grad_norm=0.1219, loss=1.3252, lr=0.001617]\u001B[A\n",
      "Training:  65%|██████▌   | 641/981 [00:52<00:19, 17.25it/s, epoch=1, grad_norm=0.1204, loss=1.3382, lr=0.001608]\u001B[A\n",
      "Training:  65%|██████▌   | 642/981 [00:52<00:19, 17.25it/s, epoch=1, grad_norm=0.1288, loss=1.3310, lr=0.001600]\u001B[A\n",
      "Training:  66%|██████▌   | 643/981 [00:52<00:19, 17.26it/s, epoch=1, grad_norm=0.1288, loss=1.3310, lr=0.001600]\u001B[A\n",
      "Training:  66%|██████▌   | 643/981 [00:52<00:19, 17.26it/s, epoch=1, grad_norm=0.1145, loss=1.3296, lr=0.001592]\u001B[A\n",
      "Training:  66%|██████▌   | 644/981 [00:52<00:19, 17.26it/s, epoch=1, grad_norm=0.1098, loss=1.3316, lr=0.001583]\u001B[A\n",
      "Training:  66%|██████▌   | 645/981 [00:52<00:19, 17.31it/s, epoch=1, grad_norm=0.1098, loss=1.3316, lr=0.001583]\u001B[A\n",
      "Training:  66%|██████▌   | 645/981 [00:52<00:19, 17.31it/s, epoch=1, grad_norm=0.1187, loss=1.3306, lr=0.001575]\u001B[A\n",
      "Training:  66%|██████▌   | 646/981 [00:52<00:19, 17.31it/s, epoch=1, grad_norm=0.1127, loss=1.3281, lr=0.001567]\u001B[A\n",
      "Training:  66%|██████▌   | 647/981 [00:52<00:19, 17.33it/s, epoch=1, grad_norm=0.1127, loss=1.3281, lr=0.001567]\u001B[A\n",
      "Training:  66%|██████▌   | 647/981 [00:52<00:19, 17.33it/s, epoch=1, grad_norm=0.1240, loss=1.3190, lr=0.001559]\u001B[A\n",
      "Training:  66%|██████▌   | 648/981 [00:52<00:19, 17.33it/s, epoch=1, grad_norm=0.1164, loss=1.3345, lr=0.001550]\u001B[A\n",
      "Training:  66%|██████▌   | 649/981 [00:52<00:19, 17.35it/s, epoch=1, grad_norm=0.1164, loss=1.3345, lr=0.001550]\u001B[A\n",
      "Training:  66%|██████▌   | 649/981 [00:52<00:19, 17.35it/s, epoch=1, grad_norm=0.1106, loss=1.3284, lr=0.001542]\u001B[A\n",
      "Training:  66%|██████▋   | 650/981 [00:52<00:19, 17.35it/s, epoch=1, grad_norm=0.1224, loss=1.3163, lr=0.001534]\u001B[A\n",
      "Training:  66%|██████▋   | 651/981 [00:52<00:19, 17.32it/s, epoch=1, grad_norm=0.1224, loss=1.3163, lr=0.001534]\u001B[A\n",
      "Training:  66%|██████▋   | 651/981 [00:52<00:19, 17.32it/s, epoch=1, grad_norm=0.1166, loss=1.3318, lr=0.001526]\u001B[A\n",
      "Training:  66%|██████▋   | 652/981 [00:52<00:18, 17.32it/s, epoch=1, grad_norm=0.1131, loss=1.3245, lr=0.001518]\u001B[A\n",
      "Training:  67%|██████▋   | 653/981 [00:52<00:18, 17.31it/s, epoch=1, grad_norm=0.1131, loss=1.3245, lr=0.001518]\u001B[A\n",
      "Training:  67%|██████▋   | 653/981 [00:52<00:18, 17.31it/s, epoch=1, grad_norm=0.1092, loss=1.3336, lr=0.001509]\u001B[A\n",
      "Training:  67%|██████▋   | 654/981 [00:52<00:18, 17.31it/s, epoch=1, grad_norm=0.1054, loss=1.3348, lr=0.001501]\u001B[A\n",
      "Training:  67%|██████▋   | 655/981 [00:52<00:18, 17.37it/s, epoch=1, grad_norm=0.1054, loss=1.3348, lr=0.001501]\u001B[A\n",
      "Training:  67%|██████▋   | 655/981 [00:52<00:18, 17.37it/s, epoch=1, grad_norm=0.1167, loss=1.3333, lr=0.001493]\u001B[A\n",
      "Training:  67%|██████▋   | 656/981 [00:53<00:18, 17.37it/s, epoch=1, grad_norm=0.1179, loss=1.3293, lr=0.001485]\u001B[A\n",
      "Training:  67%|██████▋   | 657/981 [00:53<00:18, 17.38it/s, epoch=1, grad_norm=0.1179, loss=1.3293, lr=0.001485]\u001B[A\n",
      "Training:  67%|██████▋   | 657/981 [00:53<00:18, 17.38it/s, epoch=1, grad_norm=0.1201, loss=1.3219, lr=0.001477]\u001B[A\n",
      "Training:  67%|██████▋   | 658/981 [00:53<00:18, 17.38it/s, epoch=1, grad_norm=0.1162, loss=1.3398, lr=0.001469]\u001B[A\n",
      "Training:  67%|██████▋   | 659/981 [00:53<00:18, 17.36it/s, epoch=1, grad_norm=0.1162, loss=1.3398, lr=0.001469]\u001B[A\n",
      "Training:  67%|██████▋   | 659/981 [00:53<00:18, 17.36it/s, epoch=1, grad_norm=0.1052, loss=1.3204, lr=0.001461]\u001B[A\n",
      "Training:  67%|██████▋   | 660/981 [00:53<00:18, 17.36it/s, epoch=1, grad_norm=0.1162, loss=1.3316, lr=0.001453]\u001B[A\n",
      "Training:  67%|██████▋   | 661/981 [00:53<00:18, 17.39it/s, epoch=1, grad_norm=0.1162, loss=1.3316, lr=0.001453]\u001B[A\n",
      "Training:  67%|██████▋   | 661/981 [00:53<00:18, 17.39it/s, epoch=1, grad_norm=0.1186, loss=1.3341, lr=0.001445]\u001B[A\n",
      "Training:  67%|██████▋   | 662/981 [00:53<00:18, 17.39it/s, epoch=1, grad_norm=0.1153, loss=1.3229, lr=0.001437]\u001B[A\n",
      "Training:  68%|██████▊   | 663/981 [00:53<00:18, 17.41it/s, epoch=1, grad_norm=0.1153, loss=1.3229, lr=0.001437]\u001B[A\n",
      "Training:  68%|██████▊   | 663/981 [00:53<00:18, 17.41it/s, epoch=1, grad_norm=0.1168, loss=1.3270, lr=0.001428]\u001B[A\n",
      "Training:  68%|██████▊   | 664/981 [00:53<00:18, 17.41it/s, epoch=1, grad_norm=0.1117, loss=1.3188, lr=0.001420]\u001B[A\n",
      "Training:  68%|██████▊   | 665/981 [00:53<00:18, 17.37it/s, epoch=1, grad_norm=0.1117, loss=1.3188, lr=0.001420]\u001B[A\n",
      "Training:  68%|██████▊   | 665/981 [00:53<00:18, 17.37it/s, epoch=1, grad_norm=0.1187, loss=1.3226, lr=0.001412]\u001B[A\n",
      "Training:  68%|██████▊   | 666/981 [00:53<00:18, 17.37it/s, epoch=1, grad_norm=0.1133, loss=1.3273, lr=0.001404]\u001B[A\n",
      "Training:  68%|██████▊   | 667/981 [00:53<00:18, 17.39it/s, epoch=1, grad_norm=0.1133, loss=1.3273, lr=0.001404]\u001B[A\n",
      "Training:  68%|██████▊   | 667/981 [00:53<00:18, 17.39it/s, epoch=1, grad_norm=0.1288, loss=1.3359, lr=0.001396]\u001B[A\n",
      "Training:  68%|██████▊   | 668/981 [00:53<00:17, 17.39it/s, epoch=1, grad_norm=0.1125, loss=1.3257, lr=0.001388]\u001B[A\n",
      "Training:  68%|██████▊   | 669/981 [00:53<00:17, 17.39it/s, epoch=1, grad_norm=0.1125, loss=1.3257, lr=0.001388]\u001B[A\n",
      "Training:  68%|██████▊   | 669/981 [00:53<00:17, 17.39it/s, epoch=1, grad_norm=0.1200, loss=1.3103, lr=0.001381]\u001B[A\n",
      "Training:  68%|██████▊   | 670/981 [00:53<00:17, 17.39it/s, epoch=1, grad_norm=0.1119, loss=1.3240, lr=0.001373]\u001B[A\n",
      "Training:  68%|██████▊   | 671/981 [00:53<00:17, 17.41it/s, epoch=1, grad_norm=0.1119, loss=1.3240, lr=0.001373]\u001B[A\n",
      "Training:  68%|██████▊   | 671/981 [00:53<00:17, 17.41it/s, epoch=1, grad_norm=0.1269, loss=1.3257, lr=0.001365]\u001B[A\n",
      "Training:  69%|██████▊   | 672/981 [00:53<00:17, 17.41it/s, epoch=1, grad_norm=0.1129, loss=1.3247, lr=0.001357]\u001B[A\n",
      "Training:  69%|██████▊   | 673/981 [00:53<00:17, 17.43it/s, epoch=1, grad_norm=0.1129, loss=1.3247, lr=0.001357]\u001B[A\n",
      "Training:  69%|██████▊   | 673/981 [00:54<00:17, 17.43it/s, epoch=1, grad_norm=0.1122, loss=1.3315, lr=0.001349]\u001B[A\n",
      "Training:  69%|██████▊   | 674/981 [00:54<00:17, 17.43it/s, epoch=1, grad_norm=0.1154, loss=1.3228, lr=0.001341]\u001B[A\n",
      "Training:  69%|██████▉   | 675/981 [00:54<00:17, 17.42it/s, epoch=1, grad_norm=0.1154, loss=1.3228, lr=0.001341]\u001B[A\n",
      "Training:  69%|██████▉   | 675/981 [00:54<00:17, 17.42it/s, epoch=1, grad_norm=0.1141, loss=1.3273, lr=0.001333]\u001B[A\n",
      "Training:  69%|██████▉   | 676/981 [00:54<00:17, 17.42it/s, epoch=1, grad_norm=0.1113, loss=1.3246, lr=0.001325]\u001B[A\n",
      "Training:  69%|██████▉   | 677/981 [00:54<00:17, 17.40it/s, epoch=1, grad_norm=0.1113, loss=1.3246, lr=0.001325]\u001B[A\n",
      "Training:  69%|██████▉   | 677/981 [00:54<00:17, 17.40it/s, epoch=1, grad_norm=0.1185, loss=1.3205, lr=0.001317]\u001B[A\n",
      "Training:  69%|██████▉   | 678/981 [00:54<00:17, 17.40it/s, epoch=1, grad_norm=0.1233, loss=1.3273, lr=0.001310]\u001B[A\n",
      "Training:  69%|██████▉   | 679/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1233, loss=1.3273, lr=0.001310]\u001B[A\n",
      "Training:  69%|██████▉   | 679/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1148, loss=1.3254, lr=0.001302]\u001B[A\n",
      "Training:  69%|██████▉   | 680/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1162, loss=1.3231, lr=0.001294]\u001B[A\n",
      "Training:  69%|██████▉   | 681/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1162, loss=1.3231, lr=0.001294]\u001B[A\n",
      "Training:  69%|██████▉   | 681/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1313, loss=1.3262, lr=0.001286]\u001B[A\n",
      "Training:  70%|██████▉   | 682/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1171, loss=1.3086, lr=0.001278]\u001B[A\n",
      "Training:  70%|██████▉   | 683/981 [00:54<00:17, 17.39it/s, epoch=1, grad_norm=0.1171, loss=1.3086, lr=0.001278]\u001B[A\n",
      "Training:  70%|██████▉   | 683/981 [00:54<00:17, 17.39it/s, epoch=1, grad_norm=0.1282, loss=1.3217, lr=0.001271]\u001B[A\n",
      "Training:  70%|██████▉   | 684/981 [00:54<00:17, 17.39it/s, epoch=1, grad_norm=0.1161, loss=1.3201, lr=0.001263]\u001B[A\n",
      "Training:  70%|██████▉   | 685/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1161, loss=1.3201, lr=0.001263]\u001B[A\n",
      "Training:  70%|██████▉   | 685/981 [00:54<00:17, 17.41it/s, epoch=1, grad_norm=0.1159, loss=1.3170, lr=0.001255]\u001B[A\n",
      "Training:  70%|██████▉   | 686/981 [00:54<00:16, 17.41it/s, epoch=1, grad_norm=0.1222, loss=1.3251, lr=0.001247]\u001B[A\n",
      "Training:  70%|███████   | 687/981 [00:54<00:16, 17.41it/s, epoch=1, grad_norm=0.1222, loss=1.3251, lr=0.001247]\u001B[A\n",
      "Training:  70%|███████   | 687/981 [00:54<00:16, 17.41it/s, epoch=1, grad_norm=0.1232, loss=1.3211, lr=0.001240]\u001B[A\n",
      "Training:  70%|███████   | 688/981 [00:54<00:16, 17.41it/s, epoch=1, grad_norm=0.1204, loss=1.3256, lr=0.001232]\u001B[A\n",
      "Training:  70%|███████   | 689/981 [00:54<00:16, 17.41it/s, epoch=1, grad_norm=0.1204, loss=1.3256, lr=0.001232]\u001B[A\n",
      "Training:  70%|███████   | 689/981 [00:54<00:16, 17.41it/s, epoch=1, grad_norm=0.1064, loss=1.3219, lr=0.001224]\u001B[A\n",
      "Training:  70%|███████   | 690/981 [00:55<00:16, 17.41it/s, epoch=1, grad_norm=0.1123, loss=1.3200, lr=0.001217]\u001B[A\n",
      "Training:  70%|███████   | 691/981 [00:55<00:16, 17.39it/s, epoch=1, grad_norm=0.1123, loss=1.3200, lr=0.001217]\u001B[A\n",
      "Training:  70%|███████   | 691/981 [00:55<00:16, 17.39it/s, epoch=1, grad_norm=0.1393, loss=1.3162, lr=0.001209]\u001B[A\n",
      "Training:  71%|███████   | 692/981 [00:55<00:16, 17.39it/s, epoch=1, grad_norm=0.1391, loss=1.3168, lr=0.001202]\u001B[A\n",
      "Training:  71%|███████   | 693/981 [00:55<00:16, 17.33it/s, epoch=1, grad_norm=0.1391, loss=1.3168, lr=0.001202]\u001B[A\n",
      "Training:  71%|███████   | 693/981 [00:55<00:16, 17.33it/s, epoch=1, grad_norm=0.1245, loss=1.3121, lr=0.001194]\u001B[A\n",
      "Training:  71%|███████   | 694/981 [00:55<00:16, 17.33it/s, epoch=1, grad_norm=0.1340, loss=1.3124, lr=0.001186]\u001B[A\n",
      "Training:  71%|███████   | 695/981 [00:55<00:16, 17.37it/s, epoch=1, grad_norm=0.1340, loss=1.3124, lr=0.001186]\u001B[A\n",
      "Training:  71%|███████   | 695/981 [00:55<00:16, 17.37it/s, epoch=1, grad_norm=0.1414, loss=1.3238, lr=0.001179]\u001B[A\n",
      "Training:  71%|███████   | 696/981 [00:55<00:16, 17.37it/s, epoch=1, grad_norm=0.1155, loss=1.3177, lr=0.001171]\u001B[A\n",
      "Training:  71%|███████   | 697/981 [00:55<00:16, 17.41it/s, epoch=1, grad_norm=0.1155, loss=1.3177, lr=0.001171]\u001B[A\n",
      "Training:  71%|███████   | 697/981 [00:55<00:16, 17.41it/s, epoch=1, grad_norm=0.1134, loss=1.3209, lr=0.001164]\u001B[A\n",
      "Training:  71%|███████   | 698/981 [00:55<00:16, 17.41it/s, epoch=1, grad_norm=0.1114, loss=1.3185, lr=0.001156]\u001B[A\n",
      "Training:  71%|███████▏  | 699/981 [00:55<00:16, 17.42it/s, epoch=1, grad_norm=0.1114, loss=1.3185, lr=0.001156]\u001B[A\n",
      "Training:  71%|███████▏  | 699/981 [00:55<00:16, 17.42it/s, epoch=1, grad_norm=0.1282, loss=1.3141, lr=0.001149]\u001B[A\n",
      "Training:  71%|███████▏  | 700/981 [00:57<00:16, 17.42it/s, epoch=1, grad_norm=0.1071, loss=1.3176, lr=0.001141]\u001B[A\n",
      "Training:  71%|███████▏  | 701/981 [00:57<01:51,  2.50it/s, epoch=1, grad_norm=0.1071, loss=1.3176, lr=0.001141]\u001B[A\n",
      "Training:  71%|███████▏  | 701/981 [00:57<01:51,  2.50it/s, epoch=1, grad_norm=0.1290, loss=1.3121, lr=0.001134]\u001B[A\n",
      "Training:  72%|███████▏  | 702/981 [00:57<01:51,  2.50it/s, epoch=1, grad_norm=0.1141, loss=1.3126, lr=0.001126]\u001B[A\n",
      "Training:  72%|███████▏  | 703/981 [00:57<01:22,  3.37it/s, epoch=1, grad_norm=0.1141, loss=1.3126, lr=0.001126]\u001B[A\n",
      "Training:  72%|███████▏  | 703/981 [00:58<01:22,  3.37it/s, epoch=1, grad_norm=0.1218, loss=1.3188, lr=0.001119]\u001B[A\n",
      "Training:  72%|███████▏  | 704/981 [00:58<01:22,  3.37it/s, epoch=1, grad_norm=0.1119, loss=1.3170, lr=0.001112]\u001B[A\n",
      "Training:  72%|███████▏  | 705/981 [00:58<01:02,  4.44it/s, epoch=1, grad_norm=0.1119, loss=1.3170, lr=0.001112]\u001B[A\n",
      "Training:  72%|███████▏  | 705/981 [00:58<01:02,  4.44it/s, epoch=1, grad_norm=0.1149, loss=1.3223, lr=0.001104]\u001B[A\n",
      "Training:  72%|███████▏  | 706/981 [00:58<01:01,  4.44it/s, epoch=1, grad_norm=0.1192, loss=1.3189, lr=0.001097]\u001B[A\n",
      "Training:  72%|███████▏  | 707/981 [00:58<00:47,  5.72it/s, epoch=1, grad_norm=0.1192, loss=1.3189, lr=0.001097]\u001B[A\n",
      "Training:  72%|███████▏  | 707/981 [00:58<00:47,  5.72it/s, epoch=1, grad_norm=0.1111, loss=1.3149, lr=0.001089]\u001B[A\n",
      "Training:  72%|███████▏  | 708/981 [00:58<00:47,  5.72it/s, epoch=1, grad_norm=0.1093, loss=1.3243, lr=0.001082]\u001B[A\n",
      "Training:  72%|███████▏  | 709/981 [00:58<00:37,  7.16it/s, epoch=1, grad_norm=0.1093, loss=1.3243, lr=0.001082]\u001B[A\n",
      "Training:  72%|███████▏  | 709/981 [00:58<00:37,  7.16it/s, epoch=1, grad_norm=0.1148, loss=1.3201, lr=0.001075]\u001B[A\n",
      "Training:  72%|███████▏  | 710/981 [00:58<00:37,  7.16it/s, epoch=1, grad_norm=0.1169, loss=1.3049, lr=0.001067]\u001B[A\n",
      "Training:  72%|███████▏  | 711/981 [00:58<00:31,  8.69it/s, epoch=1, grad_norm=0.1169, loss=1.3049, lr=0.001067]\u001B[A\n",
      "Training:  72%|███████▏  | 711/981 [00:58<00:31,  8.69it/s, epoch=1, grad_norm=0.1141, loss=1.3262, lr=0.001060]\u001B[A\n",
      "Training:  73%|███████▎  | 712/981 [00:58<00:30,  8.69it/s, epoch=1, grad_norm=0.1138, loss=1.3199, lr=0.001053]\u001B[A\n",
      "Training:  73%|███████▎  | 713/981 [00:58<00:26, 10.23it/s, epoch=1, grad_norm=0.1138, loss=1.3199, lr=0.001053]\u001B[A\n",
      "Training:  73%|███████▎  | 713/981 [00:58<00:26, 10.23it/s, epoch=1, grad_norm=0.1087, loss=1.3202, lr=0.001046]\u001B[A\n",
      "Training:  73%|███████▎  | 714/981 [00:58<00:26, 10.23it/s, epoch=1, grad_norm=0.1171, loss=1.3113, lr=0.001038]\u001B[A\n",
      "Training:  73%|███████▎  | 715/981 [00:58<00:22, 11.67it/s, epoch=1, grad_norm=0.1171, loss=1.3113, lr=0.001038]\u001B[A\n",
      "Training:  73%|███████▎  | 715/981 [00:58<00:22, 11.67it/s, epoch=1, grad_norm=0.1080, loss=1.3190, lr=0.001031]\u001B[A\n",
      "Training:  73%|███████▎  | 716/981 [00:58<00:22, 11.67it/s, epoch=1, grad_norm=0.1075, loss=1.3154, lr=0.001024]\u001B[A\n",
      "Training:  73%|███████▎  | 717/981 [00:58<00:20, 12.95it/s, epoch=1, grad_norm=0.1075, loss=1.3154, lr=0.001024]\u001B[A\n",
      "Training:  73%|███████▎  | 717/981 [00:58<00:20, 12.95it/s, epoch=1, grad_norm=0.1110, loss=1.3159, lr=0.001017]\u001B[A\n",
      "Training:  73%|███████▎  | 718/981 [00:58<00:20, 12.95it/s, epoch=1, grad_norm=0.1143, loss=1.3125, lr=0.001010]\u001B[A\n",
      "Training:  73%|███████▎  | 719/981 [00:58<00:18, 14.04it/s, epoch=1, grad_norm=0.1143, loss=1.3125, lr=0.001010]\u001B[A\n",
      "Training:  73%|███████▎  | 719/981 [00:58<00:18, 14.04it/s, epoch=1, grad_norm=0.1131, loss=1.3065, lr=0.001003]\u001B[A\n",
      "Training:  73%|███████▎  | 720/981 [00:59<00:18, 14.04it/s, epoch=1, grad_norm=0.1126, loss=1.3098, lr=0.000996]\u001B[A\n",
      "Training:  73%|███████▎  | 721/981 [00:59<00:17, 14.90it/s, epoch=1, grad_norm=0.1126, loss=1.3098, lr=0.000996]\u001B[A\n",
      "Training:  73%|███████▎  | 721/981 [00:59<00:17, 14.90it/s, epoch=1, grad_norm=0.1118, loss=1.3173, lr=0.000988]\u001B[A\n",
      "Training:  74%|███████▎  | 722/981 [00:59<00:17, 14.90it/s, epoch=1, grad_norm=0.1072, loss=1.3084, lr=0.000981]\u001B[A\n",
      "Training:  74%|███████▎  | 723/981 [00:59<00:16, 15.59it/s, epoch=1, grad_norm=0.1072, loss=1.3084, lr=0.000981]\u001B[A\n",
      "Training:  74%|███████▎  | 723/981 [00:59<00:16, 15.59it/s, epoch=1, grad_norm=0.1138, loss=1.3200, lr=0.000974]\u001B[A\n",
      "Training:  74%|███████▍  | 724/981 [00:59<00:16, 15.59it/s, epoch=1, grad_norm=0.1168, loss=1.3169, lr=0.000967]\u001B[A\n",
      "Training:  74%|███████▍  | 725/981 [00:59<00:15, 16.05it/s, epoch=1, grad_norm=0.1168, loss=1.3169, lr=0.000967]\u001B[A\n",
      "Training:  74%|███████▍  | 725/981 [00:59<00:15, 16.05it/s, epoch=1, grad_norm=0.1180, loss=1.3134, lr=0.000960]\u001B[A\n",
      "Training:  74%|███████▍  | 726/981 [00:59<00:15, 16.05it/s, epoch=1, grad_norm=0.1304, loss=1.3203, lr=0.000953]\u001B[A\n",
      "Training:  74%|███████▍  | 727/981 [00:59<00:15, 16.44it/s, epoch=1, grad_norm=0.1304, loss=1.3203, lr=0.000953]\u001B[A\n",
      "Training:  74%|███████▍  | 727/981 [00:59<00:15, 16.44it/s, epoch=1, grad_norm=0.1060, loss=1.3146, lr=0.000946]\u001B[A\n",
      "Training:  74%|███████▍  | 728/981 [00:59<00:15, 16.44it/s, epoch=1, grad_norm=0.1166, loss=1.3073, lr=0.000939]\u001B[A\n",
      "Training:  74%|███████▍  | 729/981 [00:59<00:15, 16.71it/s, epoch=1, grad_norm=0.1166, loss=1.3073, lr=0.000939]\u001B[A\n",
      "Training:  74%|███████▍  | 729/981 [00:59<00:15, 16.71it/s, epoch=1, grad_norm=0.1054, loss=1.3116, lr=0.000932]\u001B[A\n",
      "Training:  74%|███████▍  | 730/981 [00:59<00:15, 16.71it/s, epoch=1, grad_norm=0.1313, loss=1.3177, lr=0.000925]\u001B[A\n",
      "Training:  75%|███████▍  | 731/981 [00:59<00:14, 16.89it/s, epoch=1, grad_norm=0.1313, loss=1.3177, lr=0.000925]\u001B[A\n",
      "Training:  75%|███████▍  | 731/981 [00:59<00:14, 16.89it/s, epoch=1, grad_norm=0.1147, loss=1.3115, lr=0.000919]\u001B[A\n",
      "Training:  75%|███████▍  | 732/981 [00:59<00:14, 16.89it/s, epoch=1, grad_norm=0.1284, loss=1.3107, lr=0.000912]\u001B[A\n",
      "Training:  75%|███████▍  | 733/981 [00:59<00:14, 17.10it/s, epoch=1, grad_norm=0.1284, loss=1.3107, lr=0.000912]\u001B[A\n",
      "Training:  75%|███████▍  | 733/981 [00:59<00:14, 17.10it/s, epoch=1, grad_norm=0.1032, loss=1.3074, lr=0.000905]\u001B[A\n",
      "Training:  75%|███████▍  | 734/981 [00:59<00:14, 17.10it/s, epoch=1, grad_norm=0.1111, loss=1.3141, lr=0.000898]\u001B[A\n",
      "Training:  75%|███████▍  | 735/981 [00:59<00:14, 17.22it/s, epoch=1, grad_norm=0.1111, loss=1.3141, lr=0.000898]\u001B[A\n",
      "Training:  75%|███████▍  | 735/981 [00:59<00:14, 17.22it/s, epoch=1, grad_norm=0.1156, loss=1.3096, lr=0.000891]\u001B[A\n",
      "Training:  75%|███████▌  | 736/981 [00:59<00:14, 17.22it/s, epoch=1, grad_norm=0.1218, loss=1.3153, lr=0.000884]\u001B[A\n",
      "Training:  75%|███████▌  | 737/981 [00:59<00:14, 17.28it/s, epoch=1, grad_norm=0.1218, loss=1.3153, lr=0.000884]\u001B[A\n",
      "Training:  75%|███████▌  | 737/981 [00:59<00:14, 17.28it/s, epoch=1, grad_norm=0.1079, loss=1.3176, lr=0.000878]\u001B[A\n",
      "Training:  75%|███████▌  | 738/981 [01:00<00:14, 17.28it/s, epoch=1, grad_norm=0.1151, loss=1.3033, lr=0.000871]\u001B[A\n",
      "Training:  75%|███████▌  | 739/981 [01:00<00:13, 17.33it/s, epoch=1, grad_norm=0.1151, loss=1.3033, lr=0.000871]\u001B[A\n",
      "Training:  75%|███████▌  | 739/981 [01:00<00:13, 17.33it/s, epoch=1, grad_norm=0.1095, loss=1.3156, lr=0.000864]\u001B[A\n",
      "Training:  75%|███████▌  | 740/981 [01:00<00:13, 17.33it/s, epoch=1, grad_norm=0.1127, loss=1.3100, lr=0.000857]\u001B[A\n",
      "Training:  76%|███████▌  | 741/981 [01:00<00:13, 17.39it/s, epoch=1, grad_norm=0.1127, loss=1.3100, lr=0.000857]\u001B[A\n",
      "Training:  76%|███████▌  | 741/981 [01:00<00:13, 17.39it/s, epoch=1, grad_norm=0.1077, loss=1.3066, lr=0.000851]\u001B[A\n",
      "Training:  76%|███████▌  | 742/981 [01:00<00:13, 17.39it/s, epoch=1, grad_norm=0.1097, loss=1.3195, lr=0.000844]\u001B[A\n",
      "Training:  76%|███████▌  | 743/981 [01:00<00:13, 17.37it/s, epoch=1, grad_norm=0.1097, loss=1.3195, lr=0.000844]\u001B[A\n",
      "Training:  76%|███████▌  | 743/981 [01:00<00:13, 17.37it/s, epoch=1, grad_norm=0.1091, loss=1.3154, lr=0.000837]\u001B[A\n",
      "Training:  76%|███████▌  | 744/981 [01:00<00:13, 17.37it/s, epoch=1, grad_norm=0.1015, loss=1.3064, lr=0.000831]\u001B[A\n",
      "Training:  76%|███████▌  | 745/981 [01:00<00:13, 17.32it/s, epoch=1, grad_norm=0.1015, loss=1.3064, lr=0.000831]\u001B[A\n",
      "Training:  76%|███████▌  | 745/981 [01:00<00:13, 17.32it/s, epoch=1, grad_norm=0.1189, loss=1.3137, lr=0.000824]\u001B[A\n",
      "Training:  76%|███████▌  | 746/981 [01:00<00:13, 17.32it/s, epoch=1, grad_norm=0.1102, loss=1.3133, lr=0.000818]\u001B[A\n",
      "Training:  76%|███████▌  | 747/981 [01:00<00:13, 17.32it/s, epoch=1, grad_norm=0.1102, loss=1.3133, lr=0.000818]\u001B[A\n",
      "Training:  76%|███████▌  | 747/981 [01:00<00:13, 17.32it/s, epoch=1, grad_norm=0.1081, loss=1.3092, lr=0.000811]\u001B[A\n",
      "Training:  76%|███████▌  | 748/981 [01:00<00:13, 17.32it/s, epoch=1, grad_norm=0.1143, loss=1.3135, lr=0.000804]\u001B[A\n",
      "Training:  76%|███████▋  | 749/981 [01:00<00:13, 17.37it/s, epoch=1, grad_norm=0.1143, loss=1.3135, lr=0.000804]\u001B[A\n",
      "Training:  76%|███████▋  | 749/981 [01:00<00:13, 17.37it/s, epoch=1, grad_norm=0.1092, loss=1.3137, lr=0.000798]\u001B[A\n",
      "Training:  76%|███████▋  | 750/981 [01:00<00:13, 17.37it/s, epoch=1, grad_norm=0.1022, loss=1.3098, lr=0.000791]\u001B[A\n",
      "Training:  77%|███████▋  | 751/981 [01:00<00:13, 17.39it/s, epoch=1, grad_norm=0.1022, loss=1.3098, lr=0.000791]\u001B[A\n",
      "Training:  77%|███████▋  | 751/981 [01:00<00:13, 17.39it/s, epoch=1, grad_norm=0.1196, loss=1.3097, lr=0.000785]\u001B[A\n",
      "Training:  77%|███████▋  | 752/981 [01:00<00:13, 17.39it/s, epoch=1, grad_norm=0.1186, loss=1.3156, lr=0.000778]\u001B[A\n",
      "Training:  77%|███████▋  | 753/981 [01:00<00:13, 17.40it/s, epoch=1, grad_norm=0.1186, loss=1.3156, lr=0.000778]\u001B[A\n",
      "Training:  77%|███████▋  | 753/981 [01:00<00:13, 17.40it/s, epoch=1, grad_norm=0.1093, loss=1.3086, lr=0.000772]\u001B[A\n",
      "Training:  77%|███████▋  | 754/981 [01:00<00:13, 17.40it/s, epoch=1, grad_norm=0.1149, loss=1.3099, lr=0.000766]\u001B[A\n",
      "Training:  77%|███████▋  | 755/981 [01:00<00:13, 17.38it/s, epoch=1, grad_norm=0.1149, loss=1.3099, lr=0.000766]\u001B[A\n",
      "Training:  77%|███████▋  | 755/981 [01:01<00:13, 17.38it/s, epoch=1, grad_norm=0.1105, loss=1.3072, lr=0.000759]\u001B[A\n",
      "Training:  77%|███████▋  | 756/981 [01:01<00:12, 17.38it/s, epoch=1, grad_norm=0.1139, loss=1.3086, lr=0.000753]\u001B[A\n",
      "Training:  77%|███████▋  | 757/981 [01:01<00:12, 17.42it/s, epoch=1, grad_norm=0.1139, loss=1.3086, lr=0.000753]\u001B[A\n",
      "Training:  77%|███████▋  | 757/981 [01:01<00:12, 17.42it/s, epoch=1, grad_norm=0.1140, loss=1.3140, lr=0.000746]\u001B[A\n",
      "Training:  77%|███████▋  | 758/981 [01:01<00:12, 17.42it/s, epoch=1, grad_norm=0.1144, loss=1.3064, lr=0.000740]\u001B[A\n",
      "Training:  77%|███████▋  | 759/981 [01:01<00:12, 17.47it/s, epoch=1, grad_norm=0.1144, loss=1.3064, lr=0.000740]\u001B[A\n",
      "Training:  77%|███████▋  | 759/981 [01:01<00:12, 17.47it/s, epoch=1, grad_norm=0.1124, loss=1.3080, lr=0.000734]\u001B[A\n",
      "Training:  77%|███████▋  | 760/981 [01:01<00:12, 17.47it/s, epoch=1, grad_norm=0.1077, loss=1.3091, lr=0.000728]\u001B[A\n",
      "Training:  78%|███████▊  | 761/981 [01:01<00:12, 17.45it/s, epoch=1, grad_norm=0.1077, loss=1.3091, lr=0.000728]\u001B[A\n",
      "Training:  78%|███████▊  | 761/981 [01:01<00:12, 17.45it/s, epoch=1, grad_norm=0.1119, loss=1.3165, lr=0.000721]\u001B[A\n",
      "Training:  78%|███████▊  | 762/981 [01:01<00:12, 17.45it/s, epoch=1, grad_norm=0.1078, loss=1.2985, lr=0.000715]\u001B[A\n",
      "Training:  78%|███████▊  | 763/981 [01:01<00:12, 17.45it/s, epoch=1, grad_norm=0.1078, loss=1.2985, lr=0.000715]\u001B[A\n",
      "Training:  78%|███████▊  | 763/981 [01:01<00:12, 17.45it/s, epoch=1, grad_norm=0.1084, loss=1.3124, lr=0.000709]\u001B[A\n",
      "Training:  78%|███████▊  | 764/981 [01:01<00:12, 17.45it/s, epoch=1, grad_norm=0.1082, loss=1.3158, lr=0.000703]\u001B[A\n",
      "Training:  78%|███████▊  | 765/981 [01:01<00:12, 17.41it/s, epoch=1, grad_norm=0.1082, loss=1.3158, lr=0.000703]\u001B[A\n",
      "Training:  78%|███████▊  | 765/981 [01:01<00:12, 17.41it/s, epoch=1, grad_norm=0.1189, loss=1.3048, lr=0.000696]\u001B[A\n",
      "Training:  78%|███████▊  | 766/981 [01:01<00:12, 17.41it/s, epoch=1, grad_norm=0.1115, loss=1.3103, lr=0.000690]\u001B[A\n",
      "Training:  78%|███████▊  | 767/981 [01:01<00:12, 17.42it/s, epoch=1, grad_norm=0.1115, loss=1.3103, lr=0.000690]\u001B[A\n",
      "Training:  78%|███████▊  | 767/981 [01:01<00:12, 17.42it/s, epoch=1, grad_norm=0.1086, loss=1.3068, lr=0.000684]\u001B[A\n",
      "Training:  78%|███████▊  | 768/981 [01:01<00:12, 17.42it/s, epoch=1, grad_norm=0.1187, loss=1.3045, lr=0.000678]\u001B[A\n",
      "Training:  78%|███████▊  | 769/981 [01:01<00:12, 17.44it/s, epoch=1, grad_norm=0.1187, loss=1.3045, lr=0.000678]\u001B[A\n",
      "Training:  78%|███████▊  | 769/981 [01:01<00:12, 17.44it/s, epoch=1, grad_norm=0.1098, loss=1.2951, lr=0.000672]\u001B[A\n",
      "Training:  78%|███████▊  | 770/981 [01:01<00:12, 17.44it/s, epoch=1, grad_norm=0.1101, loss=1.3024, lr=0.000666]\u001B[A\n",
      "Training:  79%|███████▊  | 771/981 [01:01<00:12, 17.47it/s, epoch=1, grad_norm=0.1101, loss=1.3024, lr=0.000666]\u001B[A\n",
      "Training:  79%|███████▊  | 771/981 [01:01<00:12, 17.47it/s, epoch=1, grad_norm=0.1102, loss=1.3090, lr=0.000660]\u001B[A\n",
      "Training:  79%|███████▊  | 772/981 [01:01<00:11, 17.47it/s, epoch=1, grad_norm=0.1124, loss=1.3185, lr=0.000654]\u001B[A\n",
      "Training:  79%|███████▉  | 773/981 [01:01<00:11, 17.47it/s, epoch=1, grad_norm=0.1124, loss=1.3185, lr=0.000654]\u001B[A\n",
      "Training:  79%|███████▉  | 773/981 [01:02<00:11, 17.47it/s, epoch=1, grad_norm=0.1084, loss=1.3156, lr=0.000648]\u001B[A\n",
      "Training:  79%|███████▉  | 774/981 [01:02<00:11, 17.47it/s, epoch=1, grad_norm=0.1151, loss=1.3071, lr=0.000642]\u001B[A\n",
      "Training:  79%|███████▉  | 775/981 [01:02<00:11, 17.48it/s, epoch=1, grad_norm=0.1151, loss=1.3071, lr=0.000642]\u001B[A\n",
      "Training:  79%|███████▉  | 775/981 [01:02<00:11, 17.48it/s, epoch=1, grad_norm=0.1064, loss=1.3026, lr=0.000636]\u001B[A\n",
      "Training:  79%|███████▉  | 776/981 [01:02<00:11, 17.48it/s, epoch=1, grad_norm=0.1160, loss=1.3054, lr=0.000630]\u001B[A\n",
      "Training:  79%|███████▉  | 777/981 [01:02<00:11, 17.44it/s, epoch=1, grad_norm=0.1160, loss=1.3054, lr=0.000630]\u001B[A\n",
      "Training:  79%|███████▉  | 777/981 [01:02<00:11, 17.44it/s, epoch=1, grad_norm=0.1042, loss=1.3005, lr=0.000624]\u001B[A\n",
      "Training:  79%|███████▉  | 778/981 [01:02<00:11, 17.44it/s, epoch=1, grad_norm=0.1118, loss=1.3097, lr=0.000618]\u001B[A\n",
      "Training:  79%|███████▉  | 779/981 [01:02<00:11, 17.43it/s, epoch=1, grad_norm=0.1118, loss=1.3097, lr=0.000618]\u001B[A\n",
      "Training:  79%|███████▉  | 779/981 [01:02<00:11, 17.43it/s, epoch=1, grad_norm=0.1146, loss=1.3123, lr=0.000612]\u001B[A\n",
      "Training:  80%|███████▉  | 780/981 [01:02<00:11, 17.43it/s, epoch=1, grad_norm=0.1101, loss=1.3037, lr=0.000607]\u001B[A\n",
      "Training:  80%|███████▉  | 781/981 [01:02<00:11, 17.46it/s, epoch=1, grad_norm=0.1101, loss=1.3037, lr=0.000607]\u001B[A\n",
      "Training:  80%|███████▉  | 781/981 [01:02<00:11, 17.46it/s, epoch=1, grad_norm=0.1033, loss=1.3155, lr=0.000601]\u001B[A\n",
      "Training:  80%|███████▉  | 782/981 [01:02<00:11, 17.46it/s, epoch=1, grad_norm=0.1039, loss=1.3037, lr=0.000595]\u001B[A\n",
      "Training:  80%|███████▉  | 783/981 [01:02<00:11, 17.47it/s, epoch=1, grad_norm=0.1039, loss=1.3037, lr=0.000595]\u001B[A\n",
      "Training:  80%|███████▉  | 783/981 [01:02<00:11, 17.47it/s, epoch=1, grad_norm=0.1108, loss=1.3061, lr=0.000589]\u001B[A\n",
      "Training:  80%|███████▉  | 784/981 [01:02<00:11, 17.47it/s, epoch=1, grad_norm=0.1056, loss=1.3086, lr=0.000584]\u001B[A\n",
      "Training:  80%|████████  | 785/981 [01:02<00:11, 17.41it/s, epoch=1, grad_norm=0.1056, loss=1.3086, lr=0.000584]\u001B[A\n",
      "Training:  80%|████████  | 785/981 [01:02<00:11, 17.41it/s, epoch=1, grad_norm=0.1131, loss=1.3031, lr=0.000578]\u001B[A\n",
      "Training:  80%|████████  | 786/981 [01:02<00:11, 17.41it/s, epoch=1, grad_norm=0.1119, loss=1.3021, lr=0.000572]\u001B[A\n",
      "Training:  80%|████████  | 787/981 [01:02<00:11, 17.42it/s, epoch=1, grad_norm=0.1119, loss=1.3021, lr=0.000572]\u001B[A\n",
      "Training:  80%|████████  | 787/981 [01:02<00:11, 17.42it/s, epoch=1, grad_norm=0.1157, loss=1.3066, lr=0.000567]\u001B[A\n",
      "Training:  80%|████████  | 788/981 [01:02<00:11, 17.42it/s, epoch=1, grad_norm=0.1159, loss=1.3083, lr=0.000561]\u001B[A\n",
      "Training:  80%|████████  | 789/981 [01:02<00:11, 17.42it/s, epoch=1, grad_norm=0.1159, loss=1.3083, lr=0.000561]\u001B[A\n",
      "Training:  80%|████████  | 789/981 [01:02<00:11, 17.42it/s, epoch=1, grad_norm=0.0981, loss=1.3068, lr=0.000555]\u001B[A\n",
      "Training:  81%|████████  | 790/981 [01:03<00:10, 17.42it/s, epoch=1, grad_norm=0.1255, loss=1.3040, lr=0.000550]\u001B[A\n",
      "Training:  81%|████████  | 791/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1255, loss=1.3040, lr=0.000550]\u001B[A\n",
      "Training:  81%|████████  | 791/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1024, loss=1.3058, lr=0.000544]\u001B[A\n",
      "Training:  81%|████████  | 792/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1191, loss=1.3006, lr=0.000539]\u001B[A\n",
      "Training:  81%|████████  | 793/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1191, loss=1.3006, lr=0.000539]\u001B[A\n",
      "Training:  81%|████████  | 793/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1045, loss=1.3012, lr=0.000533]\u001B[A\n",
      "Training:  81%|████████  | 794/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1033, loss=1.3010, lr=0.000528]\u001B[A\n",
      "Training:  81%|████████  | 795/981 [01:03<00:10, 17.39it/s, epoch=1, grad_norm=0.1033, loss=1.3010, lr=0.000528]\u001B[A\n",
      "Training:  81%|████████  | 795/981 [01:03<00:10, 17.39it/s, epoch=1, grad_norm=0.1163, loss=1.3109, lr=0.000522]\u001B[A\n",
      "Training:  81%|████████  | 796/981 [01:03<00:10, 17.39it/s, epoch=1, grad_norm=0.1046, loss=1.3117, lr=0.000517]\u001B[A\n",
      "Training:  81%|████████  | 797/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1046, loss=1.3117, lr=0.000517]\u001B[A\n",
      "Training:  81%|████████  | 797/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1139, loss=1.2929, lr=0.000511]\u001B[A\n",
      "Training:  81%|████████▏ | 798/981 [01:03<00:10, 17.38it/s, epoch=1, grad_norm=0.1058, loss=1.3005, lr=0.000506]\u001B[A\n",
      "Training:  81%|████████▏ | 799/981 [01:03<00:10, 17.40it/s, epoch=1, grad_norm=0.1058, loss=1.3005, lr=0.000506]\u001B[A\n",
      "Training:  81%|████████▏ | 799/981 [01:03<00:10, 17.40it/s, epoch=1, grad_norm=0.1021, loss=1.2994, lr=0.000501]\u001B[A\n",
      "Training:  82%|████████▏ | 800/981 [01:05<00:10, 17.40it/s, epoch=1, grad_norm=0.1026, loss=1.3035, lr=0.000495]\u001B[A\n",
      "Training:  82%|████████▏ | 801/981 [01:05<01:11,  2.52it/s, epoch=1, grad_norm=0.1026, loss=1.3035, lr=0.000495]\u001B[A\n",
      "Training:  82%|████████▏ | 801/981 [01:05<01:11,  2.52it/s, epoch=1, grad_norm=0.1046, loss=1.2958, lr=0.000490]\u001B[A\n",
      "Training:  82%|████████▏ | 802/981 [01:05<01:11,  2.52it/s, epoch=1, grad_norm=0.0993, loss=1.2993, lr=0.000485]\u001B[A\n",
      "Training:  82%|████████▏ | 803/981 [01:05<00:52,  3.39it/s, epoch=1, grad_norm=0.0993, loss=1.2993, lr=0.000485]\u001B[A\n",
      "Training:  82%|████████▏ | 803/981 [01:06<00:52,  3.39it/s, epoch=1, grad_norm=0.1008, loss=1.2972, lr=0.000480]\u001B[A\n",
      "Training:  82%|████████▏ | 804/981 [01:06<00:52,  3.39it/s, epoch=1, grad_norm=0.0960, loss=1.2925, lr=0.000474]\u001B[A\n",
      "Training:  82%|████████▏ | 805/981 [01:06<00:39,  4.47it/s, epoch=1, grad_norm=0.0960, loss=1.2925, lr=0.000474]\u001B[A\n",
      "Training:  82%|████████▏ | 805/981 [01:06<00:39,  4.47it/s, epoch=1, grad_norm=0.0996, loss=1.3019, lr=0.000469]\u001B[A\n",
      "Training:  82%|████████▏ | 806/981 [01:06<00:39,  4.47it/s, epoch=1, grad_norm=0.1013, loss=1.2944, lr=0.000464]\u001B[A\n",
      "Training:  82%|████████▏ | 807/981 [01:06<00:30,  5.75it/s, epoch=1, grad_norm=0.1013, loss=1.2944, lr=0.000464]\u001B[A\n",
      "Training:  82%|████████▏ | 807/981 [01:06<00:30,  5.75it/s, epoch=1, grad_norm=0.0963, loss=1.2978, lr=0.000459]\u001B[A\n",
      "Training:  82%|████████▏ | 808/981 [01:06<00:30,  5.75it/s, epoch=1, grad_norm=0.0983, loss=1.3027, lr=0.000454]\u001B[A\n",
      "Training:  82%|████████▏ | 809/981 [01:06<00:23,  7.20it/s, epoch=1, grad_norm=0.0983, loss=1.3027, lr=0.000454]\u001B[A\n",
      "Training:  82%|████████▏ | 809/981 [01:06<00:23,  7.20it/s, epoch=1, grad_norm=0.1029, loss=1.3059, lr=0.000449]\u001B[A\n",
      "Training:  83%|████████▎ | 810/981 [01:06<00:23,  7.20it/s, epoch=1, grad_norm=0.1023, loss=1.2997, lr=0.000444]\u001B[A\n",
      "Training:  83%|████████▎ | 811/981 [01:06<00:19,  8.74it/s, epoch=1, grad_norm=0.1023, loss=1.2997, lr=0.000444]\u001B[A\n",
      "Training:  83%|████████▎ | 811/981 [01:06<00:19,  8.74it/s, epoch=1, grad_norm=0.1028, loss=1.3050, lr=0.000438]\u001B[A\n",
      "Training:  83%|████████▎ | 812/981 [01:06<00:19,  8.74it/s, epoch=1, grad_norm=0.0991, loss=1.2968, lr=0.000433]\u001B[A\n",
      "Training:  83%|████████▎ | 813/981 [01:06<00:16, 10.27it/s, epoch=1, grad_norm=0.0991, loss=1.2968, lr=0.000433]\u001B[A\n",
      "Training:  83%|████████▎ | 813/981 [01:06<00:16, 10.27it/s, epoch=1, grad_norm=0.1103, loss=1.3052, lr=0.000428]\u001B[A\n",
      "Training:  83%|████████▎ | 814/981 [01:06<00:16, 10.27it/s, epoch=1, grad_norm=0.1033, loss=1.3010, lr=0.000423]\u001B[A\n",
      "Training:  83%|████████▎ | 815/981 [01:06<00:14, 11.74it/s, epoch=1, grad_norm=0.1033, loss=1.3010, lr=0.000423]\u001B[A\n",
      "Training:  83%|████████▎ | 815/981 [01:06<00:14, 11.74it/s, epoch=1, grad_norm=0.0956, loss=1.3086, lr=0.000419]\u001B[A\n",
      "Training:  83%|████████▎ | 816/981 [01:06<00:14, 11.74it/s, epoch=1, grad_norm=0.0981, loss=1.3127, lr=0.000414]\u001B[A\n",
      "Training:  83%|████████▎ | 817/981 [01:06<00:12, 13.01it/s, epoch=1, grad_norm=0.0981, loss=1.3127, lr=0.000414]\u001B[A\n",
      "Training:  83%|████████▎ | 817/981 [01:06<00:12, 13.01it/s, epoch=1, grad_norm=0.1033, loss=1.3000, lr=0.000409]\u001B[A\n",
      "Training:  83%|████████▎ | 818/981 [01:06<00:12, 13.01it/s, epoch=1, grad_norm=0.1014, loss=1.3024, lr=0.000404]\u001B[A\n",
      "Training:  83%|████████▎ | 819/981 [01:06<00:11, 14.11it/s, epoch=1, grad_norm=0.1014, loss=1.3024, lr=0.000404]\u001B[A\n",
      "Training:  83%|████████▎ | 819/981 [01:06<00:11, 14.11it/s, epoch=1, grad_norm=0.0954, loss=1.3144, lr=0.000399]\u001B[A\n",
      "Training:  84%|████████▎ | 820/981 [01:07<00:11, 14.11it/s, epoch=1, grad_norm=0.0947, loss=1.2976, lr=0.000394]\u001B[A\n",
      "Training:  84%|████████▎ | 821/981 [01:07<00:10, 14.97it/s, epoch=1, grad_norm=0.0947, loss=1.2976, lr=0.000394]\u001B[A\n",
      "Training:  84%|████████▎ | 821/981 [01:07<00:10, 14.97it/s, epoch=1, grad_norm=0.1025, loss=1.3127, lr=0.000389]\u001B[A\n",
      "Training:  84%|████████▍ | 822/981 [01:07<00:10, 14.97it/s, epoch=1, grad_norm=0.0990, loss=1.3063, lr=0.000385]\u001B[A\n",
      "Training:  84%|████████▍ | 823/981 [01:07<00:10, 15.63it/s, epoch=1, grad_norm=0.0990, loss=1.3063, lr=0.000385]\u001B[A\n",
      "Training:  84%|████████▍ | 823/981 [01:07<00:10, 15.63it/s, epoch=1, grad_norm=0.0968, loss=1.2967, lr=0.000380]\u001B[A\n",
      "Training:  84%|████████▍ | 824/981 [01:07<00:10, 15.63it/s, epoch=1, grad_norm=0.0965, loss=1.3015, lr=0.000375]\u001B[A\n",
      "Training:  84%|████████▍ | 825/981 [01:07<00:09, 16.18it/s, epoch=1, grad_norm=0.0965, loss=1.3015, lr=0.000375]\u001B[A\n",
      "Training:  84%|████████▍ | 825/981 [01:07<00:09, 16.18it/s, epoch=1, grad_norm=0.0980, loss=1.2885, lr=0.000371]\u001B[A\n",
      "Training:  84%|████████▍ | 826/981 [01:07<00:09, 16.18it/s, epoch=1, grad_norm=0.1021, loss=1.3059, lr=0.000366]\u001B[A\n",
      "Training:  84%|████████▍ | 827/981 [01:07<00:09, 16.53it/s, epoch=1, grad_norm=0.1021, loss=1.3059, lr=0.000366]\u001B[A\n",
      "Training:  84%|████████▍ | 827/981 [01:07<00:09, 16.53it/s, epoch=1, grad_norm=0.0962, loss=1.2944, lr=0.000361]\u001B[A\n",
      "Training:  84%|████████▍ | 828/981 [01:07<00:09, 16.53it/s, epoch=1, grad_norm=0.0943, loss=1.2969, lr=0.000357]\u001B[A\n",
      "Training:  85%|████████▍ | 829/981 [01:07<00:09, 16.77it/s, epoch=1, grad_norm=0.0943, loss=1.2969, lr=0.000357]\u001B[A\n",
      "Training:  85%|████████▍ | 829/981 [01:07<00:09, 16.77it/s, epoch=1, grad_norm=0.0966, loss=1.2937, lr=0.000352]\u001B[A\n",
      "Training:  85%|████████▍ | 830/981 [01:07<00:09, 16.77it/s, epoch=1, grad_norm=0.1009, loss=1.2950, lr=0.000348]\u001B[A\n",
      "Training:  85%|████████▍ | 831/981 [01:07<00:08, 16.93it/s, epoch=1, grad_norm=0.1009, loss=1.2950, lr=0.000348]\u001B[A\n",
      "Training:  85%|████████▍ | 831/981 [01:07<00:08, 16.93it/s, epoch=1, grad_norm=0.0966, loss=1.3071, lr=0.000343]\u001B[A\n",
      "Training:  85%|████████▍ | 832/981 [01:07<00:08, 16.93it/s, epoch=1, grad_norm=0.0933, loss=1.3125, lr=0.000339]\u001B[A\n",
      "Training:  85%|████████▍ | 833/981 [01:07<00:08, 17.05it/s, epoch=1, grad_norm=0.0933, loss=1.3125, lr=0.000339]\u001B[A\n",
      "Training:  85%|████████▍ | 833/981 [01:07<00:08, 17.05it/s, epoch=1, grad_norm=0.1027, loss=1.3016, lr=0.000334]\u001B[A\n",
      "Training:  85%|████████▌ | 834/981 [01:07<00:08, 17.05it/s, epoch=1, grad_norm=0.1006, loss=1.3060, lr=0.000330]\u001B[A\n",
      "Training:  85%|████████▌ | 835/981 [01:07<00:08, 17.17it/s, epoch=1, grad_norm=0.1006, loss=1.3060, lr=0.000330]\u001B[A\n",
      "Training:  85%|████████▌ | 835/981 [01:07<00:08, 17.17it/s, epoch=1, grad_norm=0.0926, loss=1.2995, lr=0.000325]\u001B[A\n",
      "Training:  85%|████████▌ | 836/981 [01:07<00:08, 17.17it/s, epoch=1, grad_norm=0.0911, loss=1.2975, lr=0.000321]\u001B[A\n",
      "Training:  85%|████████▌ | 837/981 [01:07<00:08, 17.18it/s, epoch=1, grad_norm=0.0911, loss=1.2975, lr=0.000321]\u001B[A\n",
      "Training:  85%|████████▌ | 837/981 [01:07<00:08, 17.18it/s, epoch=1, grad_norm=0.0947, loss=1.2885, lr=0.000317]\u001B[A\n",
      "Training:  85%|████████▌ | 838/981 [01:08<00:08, 17.18it/s, epoch=1, grad_norm=0.0979, loss=1.3075, lr=0.000312]\u001B[A\n",
      "Training:  86%|████████▌ | 839/981 [01:08<00:08, 17.21it/s, epoch=1, grad_norm=0.0979, loss=1.3075, lr=0.000312]\u001B[A\n",
      "Training:  86%|████████▌ | 839/981 [01:08<00:08, 17.21it/s, epoch=1, grad_norm=0.0971, loss=1.3065, lr=0.000308]\u001B[A\n",
      "Training:  86%|████████▌ | 840/981 [01:08<00:08, 17.21it/s, epoch=1, grad_norm=0.0910, loss=1.2939, lr=0.000304]\u001B[A\n",
      "Training:  86%|████████▌ | 841/981 [01:08<00:08, 17.29it/s, epoch=1, grad_norm=0.0910, loss=1.2939, lr=0.000304]\u001B[A\n",
      "Training:  86%|████████▌ | 841/981 [01:08<00:08, 17.29it/s, epoch=1, grad_norm=0.1024, loss=1.2905, lr=0.000300]\u001B[A\n",
      "Training:  86%|████████▌ | 842/981 [01:08<00:08, 17.29it/s, epoch=1, grad_norm=0.0963, loss=1.2957, lr=0.000295]\u001B[A\n",
      "Training:  86%|████████▌ | 843/981 [01:08<00:07, 17.32it/s, epoch=1, grad_norm=0.0963, loss=1.2957, lr=0.000295]\u001B[A\n",
      "Training:  86%|████████▌ | 843/981 [01:08<00:07, 17.32it/s, epoch=1, grad_norm=0.0951, loss=1.2934, lr=0.000291]\u001B[A\n",
      "Training:  86%|████████▌ | 844/981 [01:08<00:07, 17.32it/s, epoch=1, grad_norm=0.0970, loss=1.3087, lr=0.000287]\u001B[A\n",
      "Training:  86%|████████▌ | 845/981 [01:08<00:07, 17.35it/s, epoch=1, grad_norm=0.0970, loss=1.3087, lr=0.000287]\u001B[A\n",
      "Training:  86%|████████▌ | 845/981 [01:08<00:07, 17.35it/s, epoch=1, grad_norm=0.0963, loss=1.3070, lr=0.000283]\u001B[A\n",
      "Training:  86%|████████▌ | 846/981 [01:08<00:07, 17.35it/s, epoch=1, grad_norm=0.0924, loss=1.2953, lr=0.000279]\u001B[A\n",
      "Training:  86%|████████▋ | 847/981 [01:08<00:07, 17.37it/s, epoch=1, grad_norm=0.0924, loss=1.2953, lr=0.000279]\u001B[A\n",
      "Training:  86%|████████▋ | 847/981 [01:08<00:07, 17.37it/s, epoch=1, grad_norm=0.0941, loss=1.2894, lr=0.000275]\u001B[A\n",
      "Training:  86%|████████▋ | 848/981 [01:08<00:07, 17.37it/s, epoch=1, grad_norm=0.0998, loss=1.3037, lr=0.000271]\u001B[A\n",
      "Training:  87%|████████▋ | 849/981 [01:08<00:07, 17.40it/s, epoch=1, grad_norm=0.0998, loss=1.3037, lr=0.000271]\u001B[A\n",
      "Training:  87%|████████▋ | 849/981 [01:08<00:07, 17.40it/s, epoch=1, grad_norm=0.0917, loss=1.2907, lr=0.000267]\u001B[A\n",
      "Training:  87%|████████▋ | 850/981 [01:08<00:07, 17.40it/s, epoch=1, grad_norm=0.0978, loss=1.2930, lr=0.000263]\u001B[A\n",
      "Training:  87%|████████▋ | 851/981 [01:08<00:07, 17.43it/s, epoch=1, grad_norm=0.0978, loss=1.2930, lr=0.000263]\u001B[A\n",
      "Training:  87%|████████▋ | 851/981 [01:08<00:07, 17.43it/s, epoch=1, grad_norm=0.0987, loss=1.3016, lr=0.000259]\u001B[A\n",
      "Training:  87%|████████▋ | 852/981 [01:08<00:07, 17.43it/s, epoch=1, grad_norm=0.0940, loss=1.2950, lr=0.000255]\u001B[A\n",
      "Training:  87%|████████▋ | 853/981 [01:08<00:07, 17.36it/s, epoch=1, grad_norm=0.0940, loss=1.2950, lr=0.000255]\u001B[A\n",
      "Training:  87%|████████▋ | 853/981 [01:08<00:07, 17.36it/s, epoch=1, grad_norm=0.0895, loss=1.2880, lr=0.000251]\u001B[A\n",
      "Training:  87%|████████▋ | 854/981 [01:08<00:07, 17.36it/s, epoch=1, grad_norm=0.0951, loss=1.3045, lr=0.000247]\u001B[A\n",
      "Training:  87%|████████▋ | 855/981 [01:08<00:07, 17.38it/s, epoch=1, grad_norm=0.0951, loss=1.3045, lr=0.000247]\u001B[A\n",
      "Training:  87%|████████▋ | 855/981 [01:09<00:07, 17.38it/s, epoch=1, grad_norm=0.0962, loss=1.3004, lr=0.000243]\u001B[A\n",
      "Training:  87%|████████▋ | 856/981 [01:09<00:07, 17.38it/s, epoch=1, grad_norm=0.0957, loss=1.3084, lr=0.000239]\u001B[A\n",
      "Training:  87%|████████▋ | 857/981 [01:09<00:07, 17.37it/s, epoch=1, grad_norm=0.0957, loss=1.3084, lr=0.000239]\u001B[A\n",
      "Training:  87%|████████▋ | 857/981 [01:09<00:07, 17.37it/s, epoch=1, grad_norm=0.0964, loss=1.2920, lr=0.000236]\u001B[A\n",
      "Training:  87%|████████▋ | 858/981 [01:09<00:07, 17.37it/s, epoch=1, grad_norm=0.0903, loss=1.2859, lr=0.000232]\u001B[A\n",
      "Training:  88%|████████▊ | 859/981 [01:09<00:07, 17.40it/s, epoch=1, grad_norm=0.0903, loss=1.2859, lr=0.000232]\u001B[A\n",
      "Training:  88%|████████▊ | 859/981 [01:09<00:07, 17.40it/s, epoch=1, grad_norm=0.0906, loss=1.2943, lr=0.000228]\u001B[A\n",
      "Training:  88%|████████▊ | 860/981 [01:09<00:06, 17.40it/s, epoch=1, grad_norm=0.0956, loss=1.3040, lr=0.000224]\u001B[A\n",
      "Training:  88%|████████▊ | 861/981 [01:09<00:06, 17.39it/s, epoch=1, grad_norm=0.0956, loss=1.3040, lr=0.000224]\u001B[A\n",
      "Training:  88%|████████▊ | 861/981 [01:09<00:06, 17.39it/s, epoch=1, grad_norm=0.0944, loss=1.2947, lr=0.000221]\u001B[A\n",
      "Training:  88%|████████▊ | 862/981 [01:09<00:06, 17.39it/s, epoch=1, grad_norm=0.0901, loss=1.3010, lr=0.000217]\u001B[A\n",
      "Training:  88%|████████▊ | 863/981 [01:09<00:06, 17.40it/s, epoch=1, grad_norm=0.0901, loss=1.3010, lr=0.000217]\u001B[A\n",
      "Training:  88%|████████▊ | 863/981 [01:09<00:06, 17.40it/s, epoch=1, grad_norm=0.0956, loss=1.2960, lr=0.000213]\u001B[A\n",
      "Training:  88%|████████▊ | 864/981 [01:09<00:06, 17.40it/s, epoch=1, grad_norm=0.0929, loss=1.2942, lr=0.000210]\u001B[A\n",
      "Training:  88%|████████▊ | 865/981 [01:09<00:06, 17.42it/s, epoch=1, grad_norm=0.0929, loss=1.2942, lr=0.000210]\u001B[A\n",
      "Training:  88%|████████▊ | 865/981 [01:09<00:06, 17.42it/s, epoch=1, grad_norm=0.0929, loss=1.3056, lr=0.000206]\u001B[A\n",
      "Training:  88%|████████▊ | 866/981 [01:09<00:06, 17.42it/s, epoch=1, grad_norm=0.0977, loss=1.2936, lr=0.000203]\u001B[A\n",
      "Training:  88%|████████▊ | 867/981 [01:09<00:06, 17.44it/s, epoch=1, grad_norm=0.0977, loss=1.2936, lr=0.000203]\u001B[A\n",
      "Training:  88%|████████▊ | 867/981 [01:09<00:06, 17.44it/s, epoch=1, grad_norm=0.0939, loss=1.2982, lr=0.000199]\u001B[A\n",
      "Training:  88%|████████▊ | 868/981 [01:09<00:06, 17.44it/s, epoch=1, grad_norm=0.0945, loss=1.2850, lr=0.000196]\u001B[A\n",
      "Training:  89%|████████▊ | 869/981 [01:09<00:06, 17.38it/s, epoch=1, grad_norm=0.0945, loss=1.2850, lr=0.000196]\u001B[A\n",
      "Training:  89%|████████▊ | 869/981 [01:09<00:06, 17.38it/s, epoch=1, grad_norm=0.0923, loss=1.3000, lr=0.000192]\u001B[A\n",
      "Training:  89%|████████▊ | 870/981 [01:09<00:06, 17.38it/s, epoch=1, grad_norm=0.0932, loss=1.2990, lr=0.000189]\u001B[A\n",
      "Training:  89%|████████▉ | 871/981 [01:09<00:06, 17.41it/s, epoch=1, grad_norm=0.0932, loss=1.2990, lr=0.000189]\u001B[A\n",
      "Training:  89%|████████▉ | 871/981 [01:09<00:06, 17.41it/s, epoch=1, grad_norm=0.0928, loss=1.2960, lr=0.000186]\u001B[A\n",
      "Training:  89%|████████▉ | 872/981 [01:09<00:06, 17.41it/s, epoch=1, grad_norm=0.0958, loss=1.2911, lr=0.000182]\u001B[A\n",
      "Training:  89%|████████▉ | 873/981 [01:09<00:06, 17.35it/s, epoch=1, grad_norm=0.0958, loss=1.2911, lr=0.000182]\u001B[A\n",
      "Training:  89%|████████▉ | 873/981 [01:10<00:06, 17.35it/s, epoch=1, grad_norm=0.0912, loss=1.2930, lr=0.000179]\u001B[A\n",
      "Training:  89%|████████▉ | 874/981 [01:10<00:06, 17.35it/s, epoch=1, grad_norm=0.0917, loss=1.2978, lr=0.000176]\u001B[A\n",
      "Training:  89%|████████▉ | 875/981 [01:10<00:06, 17.36it/s, epoch=1, grad_norm=0.0917, loss=1.2978, lr=0.000176]\u001B[A\n",
      "Training:  89%|████████▉ | 875/981 [01:10<00:06, 17.36it/s, epoch=1, grad_norm=0.0928, loss=1.2931, lr=0.000172]\u001B[A\n",
      "Training:  89%|████████▉ | 876/981 [01:10<00:06, 17.36it/s, epoch=1, grad_norm=0.0945, loss=1.2904, lr=0.000169]\u001B[A\n",
      "Training:  89%|████████▉ | 877/981 [01:10<00:05, 17.40it/s, epoch=1, grad_norm=0.0945, loss=1.2904, lr=0.000169]\u001B[A\n",
      "Training:  89%|████████▉ | 877/981 [01:10<00:05, 17.40it/s, epoch=1, grad_norm=0.0922, loss=1.2993, lr=0.000166]\u001B[A\n",
      "Training:  90%|████████▉ | 878/981 [01:10<00:05, 17.40it/s, epoch=1, grad_norm=0.0892, loss=1.3014, lr=0.000163]\u001B[A\n",
      "Training:  90%|████████▉ | 879/981 [01:10<00:05, 17.40it/s, epoch=1, grad_norm=0.0892, loss=1.3014, lr=0.000163]\u001B[A\n",
      "Training:  90%|████████▉ | 879/981 [01:10<00:05, 17.40it/s, epoch=1, grad_norm=0.0947, loss=1.2942, lr=0.000160]\u001B[A\n",
      "Training:  90%|████████▉ | 880/981 [01:10<00:05, 17.40it/s, epoch=1, grad_norm=0.0934, loss=1.3078, lr=0.000157]\u001B[A\n",
      "Training:  90%|████████▉ | 881/981 [01:10<00:05, 17.41it/s, epoch=1, grad_norm=0.0934, loss=1.3078, lr=0.000157]\u001B[A\n",
      "Training:  90%|████████▉ | 881/981 [01:10<00:05, 17.41it/s, epoch=1, grad_norm=0.0953, loss=1.2948, lr=0.000153]\u001B[A\n",
      "Training:  90%|████████▉ | 882/981 [01:10<00:05, 17.41it/s, epoch=1, grad_norm=0.0940, loss=1.2999, lr=0.000150]\u001B[A\n",
      "Training:  90%|█████████ | 883/981 [01:10<00:05, 17.37it/s, epoch=1, grad_norm=0.0940, loss=1.2999, lr=0.000150]\u001B[A\n",
      "Training:  90%|█████████ | 883/981 [01:10<00:05, 17.37it/s, epoch=1, grad_norm=0.0959, loss=1.3015, lr=0.000147]\u001B[A\n",
      "Training:  90%|█████████ | 884/981 [01:10<00:05, 17.37it/s, epoch=1, grad_norm=0.0959, loss=1.2911, lr=0.000144]\u001B[A\n",
      "Training:  90%|█████████ | 885/981 [01:10<00:05, 17.39it/s, epoch=1, grad_norm=0.0959, loss=1.2911, lr=0.000144]\u001B[A\n",
      "Training:  90%|█████████ | 885/981 [01:10<00:05, 17.39it/s, epoch=1, grad_norm=0.0888, loss=1.3051, lr=0.000141]\u001B[A\n",
      "Training:  90%|█████████ | 886/981 [01:10<00:05, 17.39it/s, epoch=1, grad_norm=0.0964, loss=1.3054, lr=0.000139]\u001B[A\n",
      "Training:  90%|█████████ | 887/981 [01:10<00:05, 17.42it/s, epoch=1, grad_norm=0.0964, loss=1.3054, lr=0.000139]\u001B[A\n",
      "Training:  90%|█████████ | 887/981 [01:10<00:05, 17.42it/s, epoch=1, grad_norm=0.0906, loss=1.2927, lr=0.000136]\u001B[A\n",
      "Training:  91%|█████████ | 888/981 [01:10<00:05, 17.42it/s, epoch=1, grad_norm=0.0887, loss=1.2929, lr=0.000133]\u001B[A\n",
      "Training:  91%|█████████ | 889/981 [01:10<00:05, 17.44it/s, epoch=1, grad_norm=0.0887, loss=1.2929, lr=0.000133]\u001B[A\n",
      "Training:  91%|█████████ | 889/981 [01:10<00:05, 17.44it/s, epoch=1, grad_norm=0.0910, loss=1.2945, lr=0.000130]\u001B[A\n",
      "Training:  91%|█████████ | 890/981 [01:11<00:05, 17.44it/s, epoch=1, grad_norm=0.0924, loss=1.2941, lr=0.000127]\u001B[A\n",
      "Training:  91%|█████████ | 891/981 [01:11<00:05, 17.44it/s, epoch=1, grad_norm=0.0924, loss=1.2941, lr=0.000127]\u001B[A\n",
      "Training:  91%|█████████ | 891/981 [01:11<00:05, 17.44it/s, epoch=1, grad_norm=0.0899, loss=1.2926, lr=0.000124]\u001B[A\n",
      "Training:  91%|█████████ | 892/981 [01:11<00:05, 17.44it/s, epoch=1, grad_norm=0.0947, loss=1.2987, lr=0.000122]\u001B[A\n",
      "Training:  91%|█████████ | 893/981 [01:11<00:05, 17.46it/s, epoch=1, grad_norm=0.0947, loss=1.2987, lr=0.000122]\u001B[A\n",
      "Training:  91%|█████████ | 893/981 [01:11<00:05, 17.46it/s, epoch=1, grad_norm=0.0916, loss=1.2948, lr=0.000119]\u001B[A\n",
      "Training:  91%|█████████ | 894/981 [01:11<00:04, 17.46it/s, epoch=1, grad_norm=0.0992, loss=1.3094, lr=0.000116]\u001B[A\n",
      "Training:  91%|█████████ | 895/981 [01:11<00:04, 17.45it/s, epoch=1, grad_norm=0.0992, loss=1.3094, lr=0.000116]\u001B[A\n",
      "Training:  91%|█████████ | 895/981 [01:11<00:04, 17.45it/s, epoch=1, grad_norm=0.0985, loss=1.2934, lr=0.000113]\u001B[A\n",
      "Training:  91%|█████████▏| 896/981 [01:11<00:04, 17.45it/s, epoch=1, grad_norm=0.0890, loss=1.2996, lr=0.000111]\u001B[A\n",
      "Training:  91%|█████████▏| 897/981 [01:11<00:04, 17.47it/s, epoch=1, grad_norm=0.0890, loss=1.2996, lr=0.000111]\u001B[A\n",
      "Training:  91%|█████████▏| 897/981 [01:11<00:04, 17.47it/s, epoch=1, grad_norm=0.0949, loss=1.2882, lr=0.000108]\u001B[A\n",
      "Training:  92%|█████████▏| 898/981 [01:11<00:04, 17.47it/s, epoch=1, grad_norm=0.0993, loss=1.3025, lr=0.000106]\u001B[A\n",
      "Training:  92%|█████████▏| 899/981 [01:11<00:04, 17.47it/s, epoch=1, grad_norm=0.0993, loss=1.3025, lr=0.000106]\u001B[A\n",
      "Training:  92%|█████████▏| 899/981 [01:11<00:04, 17.47it/s, epoch=1, grad_norm=0.0935, loss=1.3064, lr=0.000103]\u001B[A\n",
      "Training:  92%|█████████▏| 900/981 [01:13<00:04, 17.47it/s, epoch=1, grad_norm=0.0893, loss=1.2877, lr=0.000101]\u001B[A\n",
      "Training:  92%|█████████▏| 901/981 [01:13<00:32,  2.49it/s, epoch=1, grad_norm=0.0893, loss=1.2877, lr=0.000101]\u001B[A\n",
      "Training:  92%|█████████▏| 901/981 [01:13<00:32,  2.49it/s, epoch=1, grad_norm=0.0916, loss=1.2989, lr=0.000098]\u001B[A\n",
      "Training:  92%|█████████▏| 902/981 [01:14<00:31,  2.49it/s, epoch=1, grad_norm=0.0884, loss=1.2915, lr=0.000096]\u001B[A\n",
      "Training:  92%|█████████▏| 903/981 [01:14<00:23,  3.35it/s, epoch=1, grad_norm=0.0884, loss=1.2915, lr=0.000096]\u001B[A\n",
      "Training:  92%|█████████▏| 903/981 [01:14<00:23,  3.35it/s, epoch=1, grad_norm=0.0950, loss=1.2853, lr=0.000093]\u001B[A\n",
      "Training:  92%|█████████▏| 904/981 [01:14<00:22,  3.35it/s, epoch=1, grad_norm=0.0874, loss=1.2980, lr=0.000091]\u001B[A\n",
      "Training:  92%|█████████▏| 905/981 [01:14<00:17,  4.43it/s, epoch=1, grad_norm=0.0874, loss=1.2980, lr=0.000091]\u001B[A\n",
      "Training:  92%|█████████▏| 905/981 [01:14<00:17,  4.43it/s, epoch=1, grad_norm=0.0963, loss=1.3005, lr=0.000088]\u001B[A\n",
      "Training:  92%|█████████▏| 906/981 [01:14<00:16,  4.43it/s, epoch=1, grad_norm=0.0863, loss=1.2911, lr=0.000086]\u001B[A\n",
      "Training:  92%|█████████▏| 907/981 [01:14<00:12,  5.70it/s, epoch=1, grad_norm=0.0863, loss=1.2911, lr=0.000086]\u001B[A\n",
      "Training:  92%|█████████▏| 907/981 [01:14<00:12,  5.70it/s, epoch=1, grad_norm=0.0883, loss=1.2935, lr=0.000084]\u001B[A\n",
      "Training:  93%|█████████▎| 908/981 [01:14<00:12,  5.70it/s, epoch=1, grad_norm=0.0909, loss=1.2974, lr=0.000082]\u001B[A\n",
      "Training:  93%|█████████▎| 909/981 [01:14<00:10,  7.14it/s, epoch=1, grad_norm=0.0909, loss=1.2974, lr=0.000082]\u001B[A\n",
      "Training:  93%|█████████▎| 909/981 [01:14<00:10,  7.14it/s, epoch=1, grad_norm=0.0936, loss=1.2909, lr=0.000079]\u001B[A\n",
      "Training:  93%|█████████▎| 910/981 [01:14<00:09,  7.14it/s, epoch=1, grad_norm=0.0893, loss=1.2953, lr=0.000077]\u001B[A\n",
      "Training:  93%|█████████▎| 911/981 [01:14<00:08,  8.68it/s, epoch=1, grad_norm=0.0893, loss=1.2953, lr=0.000077]\u001B[A\n",
      "Training:  93%|█████████▎| 911/981 [01:14<00:08,  8.68it/s, epoch=1, grad_norm=0.0872, loss=1.3002, lr=0.000075]\u001B[A\n",
      "Training:  93%|█████████▎| 912/981 [01:14<00:07,  8.68it/s, epoch=1, grad_norm=0.0863, loss=1.2973, lr=0.000073]\u001B[A\n",
      "Training:  93%|█████████▎| 913/981 [01:14<00:06, 10.22it/s, epoch=1, grad_norm=0.0863, loss=1.2973, lr=0.000073]\u001B[A\n",
      "Training:  93%|█████████▎| 913/981 [01:14<00:06, 10.22it/s, epoch=1, grad_norm=0.0886, loss=1.2962, lr=0.000071]\u001B[A\n",
      "Training:  93%|█████████▎| 914/981 [01:14<00:06, 10.22it/s, epoch=1, grad_norm=0.0909, loss=1.2958, lr=0.000069]\u001B[A\n",
      "Training:  93%|█████████▎| 915/981 [01:14<00:05, 11.67it/s, epoch=1, grad_norm=0.0909, loss=1.2958, lr=0.000069]\u001B[A\n",
      "Training:  93%|█████████▎| 915/981 [01:14<00:05, 11.67it/s, epoch=1, grad_norm=0.0899, loss=1.2919, lr=0.000067]\u001B[A\n",
      "Training:  93%|█████████▎| 916/981 [01:14<00:05, 11.67it/s, epoch=1, grad_norm=0.0861, loss=1.2980, lr=0.000065]\u001B[A\n",
      "Training:  93%|█████████▎| 917/981 [01:14<00:04, 12.97it/s, epoch=1, grad_norm=0.0861, loss=1.2980, lr=0.000065]\u001B[A\n",
      "Training:  93%|█████████▎| 917/981 [01:14<00:04, 12.97it/s, epoch=1, grad_norm=0.0914, loss=1.2992, lr=0.000063]\u001B[A\n",
      "Training:  94%|█████████▎| 918/981 [01:14<00:04, 12.97it/s, epoch=1, grad_norm=0.0873, loss=1.2910, lr=0.000061]\u001B[A\n",
      "Training:  94%|█████████▎| 919/981 [01:14<00:04, 14.07it/s, epoch=1, grad_norm=0.0873, loss=1.2910, lr=0.000061]\u001B[A\n",
      "Training:  94%|█████████▎| 919/981 [01:14<00:04, 14.07it/s, epoch=1, grad_norm=0.0887, loss=1.2923, lr=0.000059]\u001B[A\n",
      "Training:  94%|█████████▍| 920/981 [01:15<00:04, 14.07it/s, epoch=1, grad_norm=0.0858, loss=1.2884, lr=0.000057]\u001B[A\n",
      "Training:  94%|█████████▍| 921/981 [01:15<00:04, 14.95it/s, epoch=1, grad_norm=0.0858, loss=1.2884, lr=0.000057]\u001B[A\n",
      "Training:  94%|█████████▍| 921/981 [01:15<00:04, 14.95it/s, epoch=1, grad_norm=0.0869, loss=1.2870, lr=0.000055]\u001B[A\n",
      "Training:  94%|█████████▍| 922/981 [01:15<00:03, 14.95it/s, epoch=1, grad_norm=0.0883, loss=1.2924, lr=0.000053]\u001B[A\n",
      "Training:  94%|█████████▍| 923/981 [01:15<00:03, 15.64it/s, epoch=1, grad_norm=0.0883, loss=1.2924, lr=0.000053]\u001B[A\n",
      "Training:  94%|█████████▍| 923/981 [01:15<00:03, 15.64it/s, epoch=1, grad_norm=0.0868, loss=1.2937, lr=0.000051]\u001B[A\n",
      "Training:  94%|█████████▍| 924/981 [01:15<00:03, 15.64it/s, epoch=1, grad_norm=0.0879, loss=1.2947, lr=0.000049]\u001B[A\n",
      "Training:  94%|█████████▍| 925/981 [01:15<00:03, 16.12it/s, epoch=1, grad_norm=0.0879, loss=1.2947, lr=0.000049]\u001B[A\n",
      "Training:  94%|█████████▍| 925/981 [01:15<00:03, 16.12it/s, epoch=1, grad_norm=0.0871, loss=1.2980, lr=0.000048]\u001B[A\n",
      "Training:  94%|█████████▍| 926/981 [01:15<00:03, 16.12it/s, epoch=1, grad_norm=0.0897, loss=1.2983, lr=0.000046]\u001B[A\n",
      "Training:  94%|█████████▍| 927/981 [01:15<00:03, 16.46it/s, epoch=1, grad_norm=0.0897, loss=1.2983, lr=0.000046]\u001B[A\n",
      "Training:  94%|█████████▍| 927/981 [01:15<00:03, 16.46it/s, epoch=1, grad_norm=0.0859, loss=1.2985, lr=0.000044]\u001B[A\n",
      "Training:  95%|█████████▍| 928/981 [01:15<00:03, 16.46it/s, epoch=1, grad_norm=0.0882, loss=1.3017, lr=0.000043]\u001B[A\n",
      "Training:  95%|█████████▍| 929/981 [01:15<00:03, 16.75it/s, epoch=1, grad_norm=0.0882, loss=1.3017, lr=0.000043]\u001B[A\n",
      "Training:  95%|█████████▍| 929/981 [01:15<00:03, 16.75it/s, epoch=1, grad_norm=0.0885, loss=1.3074, lr=0.000041]\u001B[A\n",
      "Training:  95%|█████████▍| 930/981 [01:15<00:03, 16.75it/s, epoch=1, grad_norm=0.0872, loss=1.2984, lr=0.000039]\u001B[A\n",
      "Training:  95%|█████████▍| 931/981 [01:15<00:02, 16.92it/s, epoch=1, grad_norm=0.0872, loss=1.2984, lr=0.000039]\u001B[A\n",
      "Training:  95%|█████████▍| 931/981 [01:15<00:02, 16.92it/s, epoch=1, grad_norm=0.0909, loss=1.2856, lr=0.000038]\u001B[A\n",
      "Training:  95%|█████████▌| 932/981 [01:15<00:02, 16.92it/s, epoch=1, grad_norm=0.0888, loss=1.2958, lr=0.000036]\u001B[A\n",
      "Training:  95%|█████████▌| 933/981 [01:15<00:02, 17.07it/s, epoch=1, grad_norm=0.0888, loss=1.2958, lr=0.000036]\u001B[A\n",
      "Training:  95%|█████████▌| 933/981 [01:15<00:02, 17.07it/s, epoch=1, grad_norm=0.0876, loss=1.2901, lr=0.000035]\u001B[A\n",
      "Training:  95%|█████████▌| 934/981 [01:15<00:02, 17.07it/s, epoch=1, grad_norm=0.0880, loss=1.2883, lr=0.000033]\u001B[A\n",
      "Training:  95%|█████████▌| 935/981 [01:15<00:02, 17.19it/s, epoch=1, grad_norm=0.0880, loss=1.2883, lr=0.000033]\u001B[A\n",
      "Training:  95%|█████████▌| 935/981 [01:15<00:02, 17.19it/s, epoch=1, grad_norm=0.0845, loss=1.2950, lr=0.000032]\u001B[A\n",
      "Training:  95%|█████████▌| 936/981 [01:15<00:02, 17.19it/s, epoch=1, grad_norm=0.0886, loss=1.2965, lr=0.000031]\u001B[A\n",
      "Training:  96%|█████████▌| 937/981 [01:15<00:02, 17.19it/s, epoch=1, grad_norm=0.0886, loss=1.2965, lr=0.000031]\u001B[A\n",
      "Training:  96%|█████████▌| 937/981 [01:16<00:02, 17.19it/s, epoch=1, grad_norm=0.0876, loss=1.2897, lr=0.000029]\u001B[A\n",
      "Training:  96%|█████████▌| 938/981 [01:16<00:02, 17.19it/s, epoch=1, grad_norm=0.0867, loss=1.2869, lr=0.000028]\u001B[A\n",
      "Training:  96%|█████████▌| 939/981 [01:16<00:02, 17.23it/s, epoch=1, grad_norm=0.0867, loss=1.2869, lr=0.000028]\u001B[A\n",
      "Training:  96%|█████████▌| 939/981 [01:16<00:02, 17.23it/s, epoch=1, grad_norm=0.0899, loss=1.2871, lr=0.000027]\u001B[A\n",
      "Training:  96%|█████████▌| 940/981 [01:16<00:02, 17.23it/s, epoch=1, grad_norm=0.0879, loss=1.2982, lr=0.000025]\u001B[A\n",
      "Training:  96%|█████████▌| 941/981 [01:16<00:02, 17.27it/s, epoch=1, grad_norm=0.0879, loss=1.2982, lr=0.000025]\u001B[A\n",
      "Training:  96%|█████████▌| 941/981 [01:16<00:02, 17.27it/s, epoch=1, grad_norm=0.0892, loss=1.3009, lr=0.000024]\u001B[A\n",
      "Training:  96%|█████████▌| 942/981 [01:16<00:02, 17.27it/s, epoch=1, grad_norm=0.0856, loss=1.2965, lr=0.000023]\u001B[A\n",
      "Training:  96%|█████████▌| 943/981 [01:16<00:02, 17.38it/s, epoch=1, grad_norm=0.0856, loss=1.2965, lr=0.000023]\u001B[A\n",
      "Training:  96%|█████████▌| 943/981 [01:16<00:02, 17.38it/s, epoch=1, grad_norm=0.0855, loss=1.3034, lr=0.000022]\u001B[A\n",
      "Training:  96%|█████████▌| 944/981 [01:16<00:02, 17.38it/s, epoch=1, grad_norm=0.0850, loss=1.2926, lr=0.000020]\u001B[A\n",
      "Training:  96%|█████████▋| 945/981 [01:16<00:02, 17.39it/s, epoch=1, grad_norm=0.0850, loss=1.2926, lr=0.000020]\u001B[A\n",
      "Training:  96%|█████████▋| 945/981 [01:16<00:02, 17.39it/s, epoch=1, grad_norm=0.0884, loss=1.3011, lr=0.000019]\u001B[A\n",
      "Training:  96%|█████████▋| 946/981 [01:16<00:02, 17.39it/s, epoch=1, grad_norm=0.0883, loss=1.3036, lr=0.000018]\u001B[A\n",
      "Training:  97%|█████████▋| 947/981 [01:16<00:01, 17.39it/s, epoch=1, grad_norm=0.0883, loss=1.3036, lr=0.000018]\u001B[A\n",
      "Training:  97%|█████████▋| 947/981 [01:16<00:01, 17.39it/s, epoch=1, grad_norm=0.0862, loss=1.2928, lr=0.000017]\u001B[A\n",
      "Training:  97%|█████████▋| 948/981 [01:16<00:01, 17.39it/s, epoch=1, grad_norm=0.0867, loss=1.2954, lr=0.000016]\u001B[A\n",
      "Training:  97%|█████████▋| 949/981 [01:16<00:01, 17.39it/s, epoch=1, grad_norm=0.0867, loss=1.2954, lr=0.000016]\u001B[A\n",
      "Training:  97%|█████████▋| 949/981 [01:16<00:01, 17.39it/s, epoch=1, grad_norm=0.0870, loss=1.2981, lr=0.000015]\u001B[A\n",
      "Training:  97%|█████████▋| 950/981 [01:16<00:01, 17.39it/s, epoch=1, grad_norm=0.0856, loss=1.2949, lr=0.000014]\u001B[A\n",
      "Training:  97%|█████████▋| 951/981 [01:16<00:01, 17.37it/s, epoch=1, grad_norm=0.0856, loss=1.2949, lr=0.000014]\u001B[A\n",
      "Training:  97%|█████████▋| 951/981 [01:16<00:01, 17.37it/s, epoch=1, grad_norm=0.0875, loss=1.2893, lr=0.000013]\u001B[A\n",
      "Training:  97%|█████████▋| 952/981 [01:16<00:01, 17.37it/s, epoch=1, grad_norm=0.0858, loss=1.2930, lr=0.000012]\u001B[A\n",
      "Training:  97%|█████████▋| 953/981 [01:16<00:01, 17.33it/s, epoch=1, grad_norm=0.0858, loss=1.2930, lr=0.000012]\u001B[A\n",
      "Training:  97%|█████████▋| 953/981 [01:16<00:01, 17.33it/s, epoch=1, grad_norm=0.0878, loss=1.3009, lr=0.000012]\u001B[A\n",
      "Training:  97%|█████████▋| 954/981 [01:16<00:01, 17.33it/s, epoch=1, grad_norm=0.0860, loss=1.2881, lr=0.000011]\u001B[A\n",
      "Training:  97%|█████████▋| 955/981 [01:16<00:01, 17.34it/s, epoch=1, grad_norm=0.0860, loss=1.2881, lr=0.000011]\u001B[A\n",
      "Training:  97%|█████████▋| 955/981 [01:17<00:01, 17.34it/s, epoch=1, grad_norm=0.0878, loss=1.2967, lr=0.000010]\u001B[A\n",
      "Training:  97%|█████████▋| 956/981 [01:17<00:01, 17.34it/s, epoch=1, grad_norm=0.0847, loss=1.2955, lr=0.000009]\u001B[A\n",
      "Training:  98%|█████████▊| 957/981 [01:17<00:01, 17.36it/s, epoch=1, grad_norm=0.0847, loss=1.2955, lr=0.000009]\u001B[A\n",
      "Training:  98%|█████████▊| 957/981 [01:17<00:01, 17.36it/s, epoch=1, grad_norm=0.0842, loss=1.3013, lr=0.000008]\u001B[A\n",
      "Training:  98%|█████████▊| 958/981 [01:17<00:01, 17.36it/s, epoch=1, grad_norm=0.0858, loss=1.2996, lr=0.000008]\u001B[A\n",
      "Training:  98%|█████████▊| 959/981 [01:17<00:01, 17.36it/s, epoch=1, grad_norm=0.0858, loss=1.2996, lr=0.000008]\u001B[A\n",
      "Training:  98%|█████████▊| 959/981 [01:17<00:01, 17.36it/s, epoch=1, grad_norm=0.0823, loss=1.2968, lr=0.000007]\u001B[A\n",
      "Training:  98%|█████████▊| 960/981 [01:17<00:01, 17.36it/s, epoch=1, grad_norm=0.0865, loss=1.2896, lr=0.000006]\u001B[A\n",
      "Training:  98%|█████████▊| 961/981 [01:17<00:01, 17.34it/s, epoch=1, grad_norm=0.0865, loss=1.2896, lr=0.000006]\u001B[A\n",
      "Training:  98%|█████████▊| 961/981 [01:17<00:01, 17.34it/s, epoch=1, grad_norm=0.0869, loss=1.2990, lr=0.000006]\u001B[A\n",
      "Training:  98%|█████████▊| 962/981 [01:17<00:01, 17.34it/s, epoch=1, grad_norm=0.0862, loss=1.2919, lr=0.000005]\u001B[A\n",
      "Training:  98%|█████████▊| 963/981 [01:17<00:01, 17.31it/s, epoch=1, grad_norm=0.0862, loss=1.2919, lr=0.000005]\u001B[A\n",
      "Training:  98%|█████████▊| 963/981 [01:17<00:01, 17.31it/s, epoch=1, grad_norm=0.0858, loss=1.2969, lr=0.000005]\u001B[A\n",
      "Training:  98%|█████████▊| 964/981 [01:17<00:00, 17.31it/s, epoch=1, grad_norm=0.0856, loss=1.2872, lr=0.000004]\u001B[A\n",
      "Training:  98%|█████████▊| 965/981 [01:17<00:00, 17.31it/s, epoch=1, grad_norm=0.0856, loss=1.2872, lr=0.000004]\u001B[A\n",
      "Training:  98%|█████████▊| 965/981 [01:17<00:00, 17.31it/s, epoch=1, grad_norm=0.0854, loss=1.2850, lr=0.000004]\u001B[A\n",
      "Training:  98%|█████████▊| 966/981 [01:17<00:00, 17.31it/s, epoch=1, grad_norm=0.0849, loss=1.2968, lr=0.000003]\u001B[A\n",
      "Training:  99%|█████████▊| 967/981 [01:17<00:00, 17.34it/s, epoch=1, grad_norm=0.0849, loss=1.2968, lr=0.000003]\u001B[A\n",
      "Training:  99%|█████████▊| 967/981 [01:17<00:00, 17.34it/s, epoch=1, grad_norm=0.0875, loss=1.2969, lr=0.000003]\u001B[A\n",
      "Training:  99%|█████████▊| 968/981 [01:17<00:00, 17.34it/s, epoch=1, grad_norm=0.0878, loss=1.2912, lr=0.000002]\u001B[A\n",
      "Training:  99%|█████████▉| 969/981 [01:17<00:00, 17.35it/s, epoch=1, grad_norm=0.0878, loss=1.2912, lr=0.000002]\u001B[A\n",
      "Training:  99%|█████████▉| 969/981 [01:17<00:00, 17.35it/s, epoch=1, grad_norm=0.0833, loss=1.2929, lr=0.000002]\u001B[A\n",
      "Training:  99%|█████████▉| 970/981 [01:17<00:00, 17.35it/s, epoch=1, grad_norm=0.0847, loss=1.2927, lr=0.000002]\u001B[A\n",
      "Training:  99%|█████████▉| 971/981 [01:17<00:00, 17.38it/s, epoch=1, grad_norm=0.0847, loss=1.2927, lr=0.000002]\u001B[A\n",
      "Training:  99%|█████████▉| 971/981 [01:17<00:00, 17.38it/s, epoch=1, grad_norm=0.0860, loss=1.2935, lr=0.000001]\u001B[A\n",
      "Training:  99%|█████████▉| 972/981 [01:18<00:00, 17.38it/s, epoch=1, grad_norm=0.0849, loss=1.2933, lr=0.000001]\u001B[A\n",
      "Training:  99%|█████████▉| 973/981 [01:18<00:00, 14.65it/s, epoch=1, grad_norm=0.0849, loss=1.2933, lr=0.000001]\u001B[A\n",
      "Training:  99%|█████████▉| 973/981 [01:18<00:00, 14.65it/s, epoch=1, grad_norm=0.0855, loss=1.2883, lr=0.000001]\u001B[A\n",
      "Training:  99%|█████████▉| 974/981 [01:18<00:00, 14.65it/s, epoch=1, grad_norm=0.0859, loss=1.3015, lr=0.000001]\u001B[A\n",
      "Training:  99%|█████████▉| 975/981 [01:18<00:00, 15.52it/s, epoch=1, grad_norm=0.0859, loss=1.3015, lr=0.000001]\u001B[A\n",
      "Training:  99%|█████████▉| 975/981 [01:18<00:00, 15.52it/s, epoch=1, grad_norm=0.0853, loss=1.3019, lr=0.000000]\u001B[A\n",
      "Training:  99%|█████████▉| 976/981 [01:18<00:00, 15.52it/s, epoch=1, grad_norm=0.0850, loss=1.3019, lr=0.000000]\u001B[A\n",
      "Training: 100%|█████████▉| 977/981 [01:18<00:00, 16.14it/s, epoch=1, grad_norm=0.0850, loss=1.3019, lr=0.000000]\u001B[A\n",
      "Training: 100%|█████████▉| 977/981 [01:18<00:00, 16.14it/s, epoch=1, grad_norm=0.0856, loss=1.2950, lr=0.000000]\u001B[A\n",
      "Training: 100%|█████████▉| 978/981 [01:18<00:00, 16.14it/s, epoch=1, grad_norm=0.0857, loss=1.2848, lr=0.000000]\u001B[A\n",
      "Training: 100%|█████████▉| 979/981 [01:18<00:00, 16.50it/s, epoch=1, grad_norm=0.0857, loss=1.2848, lr=0.000000]\u001B[A\n",
      "Training: 100%|█████████▉| 979/981 [01:18<00:00, 16.50it/s, epoch=1, grad_norm=0.0851, loss=1.2982, lr=0.000000]\u001B[A\n",
      "Training: 100%|██████████| 981/981 [01:20<00:00, 12.14it/s, epoch=1, grad_norm=0.1947, loss=1.2895, lr=0.000000]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>▆▇▅▂▂█▃▅▄▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning Rate</td><td>▂▄▄▇█████████▇▇▇▇▇▇▇▆▆▆▆▅▅▄▄▄▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>█▆▅▅▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>Val Perplexity</td><td>█▃▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>0.19474</td></tr><tr><td>Learning Rate</td><td>0</td></tr><tr><td>Train Loss</td><td>1.28946</td></tr><tr><td>Val Loss</td><td>1.49244</td></tr><tr><td>Val Perplexity</td><td>4.44792</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PathFinder-nano-2025-06-13_11-00-29</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/p07avk57' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/p07avk57</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>/workspace/PathFinder/wandb/run-20250613_110131-p07avk57/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "df43a98cae309849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:16.368069Z",
     "start_time": "2025-06-13T11:04:16.324630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not train_config.debug:\n",
    "    output_dir = os.path.join(PROJECT_ROOT, \"checkpoints\", train_config.model_name, train_config.run_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        model.save_pretrained(\n",
    "            output_dir,\n",
    "            safe_serialization=True\n",
    "        )\n",
    "        print(\"Model saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    # Push to Hugging Face Hub\n",
    "    #model.push_to_hub(\n",
    "    #    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    #    private=True,\n",
    "    #    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    #)\n",
    "    #print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{train_config.model_name}-{train_config.run_name}\")"
   ],
   "id": "a587e8a215674e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:19.266236Z",
     "start_time": "2025-06-13T11:04:19.260405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT(model_config)\n",
    "# model = model.from_pretrained(output_dir).to(device)"
   ],
   "id": "f3cad31e5a514fc0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "1ed82e9a71b4c9bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:20.014677Z",
     "start_time": "2025-06-13T11:04:19.343160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k,\n",
    "    tokenizer=char_tokenizer\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "847724c2f13f7a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flash_attn_decode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m user_prompt = \u001B[33m\"\u001B[39m\u001B[33mTo be, or not to be, that is the question\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m input_ids = char_tokenizer.encode(user_prompt).unsqueeze(\u001B[32m0\u001B[39m).to(device)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m output = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchar_tokenizer\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m response = char_tokenizer.decode(output[\u001B[32m0\u001B[39m].squeeze().cpu().numpy())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:336\u001B[39m, in \u001B[36mGPT.generate\u001B[39m\u001B[34m(self, idx, use_cache, max_new_tokens, temperature, top_k, tokenizer)\u001B[39m\n\u001B[32m    333\u001B[39m idx_input = idx \u001B[38;5;28;01mif\u001B[39;00m idx.size(\u001B[32m1\u001B[39m) <= \u001B[38;5;28mself\u001B[39m.config.max_seq_len \u001B[38;5;28;01melse\u001B[39;00m idx[:, -\u001B[38;5;28mself\u001B[39m.config.max_seq_len:]\n\u001B[32m    335\u001B[39m \u001B[38;5;66;03m# ---------- Forward pass ----------------------------------------------------------------------------------\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m logits, _, kv_cache = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    337\u001B[39m \u001B[43m    \u001B[49m\u001B[43midx_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    338\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m    339\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m                                                                              \u001B[38;5;66;03m# [batch_size, 1, vocab_size]\u001B[39;00m\n\u001B[32m    340\u001B[39m logits = logits[:, -\u001B[32m1\u001B[39m, :]                                                         \u001B[38;5;66;03m# [batch_size, vocab_size]\u001B[39;00m\n\u001B[32m    342\u001B[39m \u001B[38;5;66;03m# ---------- Temperature -----------------------------------------------------------------------------------\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1843\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1844\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1845\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1846\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1847\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1848\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n\u001B[32m   1849\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m _global_forward_hooks.items():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1787\u001B[39m     bw_hook = BackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[32m   1788\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1790\u001B[39m result = \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1791\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n\u001B[32m   1792\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[32m   1793\u001B[39m         *_global_forward_hooks.items(),\n\u001B[32m   1794\u001B[39m         *\u001B[38;5;28mself\u001B[39m._forward_hooks.items(),\n\u001B[32m   1795\u001B[39m     ):\n\u001B[32m   1796\u001B[39m         \u001B[38;5;66;03m# mark that always called hook is run\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:297\u001B[39m, in \u001B[36mGPT.forward\u001B[39m\u001B[34m(self, input_ids, target_ids, kv_cache)\u001B[39m\n\u001B[32m    295\u001B[39m new_kv_cache = []\n\u001B[32m    296\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer_idx, block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.blocks):\n\u001B[32m--> \u001B[39m\u001B[32m297\u001B[39m     x, kv_cache_layer = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m                  \u001B[38;5;66;03m# [batch_size, seq_len, d_embed]\u001B[39;00m\n\u001B[32m    298\u001B[39m     new_kv_cache.append(kv_cache_layer)\n\u001B[32m    300\u001B[39m \u001B[38;5;66;03m# ---------- Final linear layer --------------------------------------------------------------------------------\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:217\u001B[39m, in \u001B[36mBlock.forward\u001B[39m\u001B[34m(self, x, kv_cache)\u001B[39m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, kv_cache):\n\u001B[32m    216\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm1(x)\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     y, new_kv_cache = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    218\u001B[39m     x = x + y\n\u001B[32m    220\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm2(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:78\u001B[39m, in \u001B[36mMultiHeadAttention.forward\u001B[39m\u001B[34m(self, x, kv_cache)\u001B[39m\n\u001B[32m     71\u001B[39m         attn_out = F.scaled_dot_product_attention(\n\u001B[32m     72\u001B[39m             q, k, v,\n\u001B[32m     73\u001B[39m             scale=\u001B[38;5;28mself\u001B[39m.scale,\n\u001B[32m     74\u001B[39m             dropout_p=\u001B[38;5;28mself\u001B[39m.config.dropout,\n\u001B[32m     75\u001B[39m             is_causal=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     76\u001B[39m         )                                                           \u001B[38;5;66;03m# [batch_size, n_heads, seq_len, d_head]\u001B[39;00m\n\u001B[32m     77\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:               \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m         attn_out = \u001B[43mflash_attn_decode\u001B[49m(\n\u001B[32m     79\u001B[39m             q,                                                            \u001B[38;5;66;03m# [batch_size, n_heads, 1, d_head]\u001B[39;00m\n\u001B[32m     80\u001B[39m             k, v,                                                   \u001B[38;5;66;03m# [batch_size, n_heads, seq_len, d_head]\u001B[39;00m\n\u001B[32m     81\u001B[39m             scale=\u001B[38;5;28mself\u001B[39m.scale\n\u001B[32m     82\u001B[39m         )                                                                 \u001B[38;5;66;03m# [batch_size, n_heads, 1, d_head]\u001B[39;00m\n\u001B[32m     84\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     85\u001B[39m     attn_scores = (q @ k.transpose(-\u001B[32m2\u001B[39m, -\u001B[32m1\u001B[39m)) * \u001B[38;5;28mself\u001B[39m.scale           \u001B[38;5;66;03m# [batch_size, n_heads, seq_len, seq_len]\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'flash_attn_decode' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:20.043049Z",
     "start_time": "2025-06-13T07:15:34.479850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"🤖 Model Response:\")\n",
    "print(response)"
   ],
   "id": "1ea28687151601ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User prompt: \n",
      "To be, or not to be, that is the question\n",
      "--------------------------------------------------\n",
      "🤖 Model Response:\n",
      "To be, or not to be, that is the question\n",
      "A Conspy, good preserve yourself\n",
      "All a worldred to-night. When the bloody might\n",
      "Makes; lock selfloughtill, brence as if it reason\n",
      "When I come, I let not not depended.\n",
      "\n",
      "YORK:\n",
      "No more; for I am mine own wrongs, good d ser then wit.\n",
      "\n",
      "CLARENCE:\n",
      "My hardeous is said to my light safety, be a man,\n",
      "That nature's joy, 'tless spurplish'd,\n",
      "Shall I. Who now hearing us hold thing Rome, thy teether,\n",
      "My grace to the last times pray.\n",
      "Join Margaret: Ah, we'll remember them or Rutland, bawn offick and Emile are, they all Love of York;\n",
      "Raised heart and grave to you will command you.\n",
      "What dalice that you you stan forced awards, of Henry:\n",
      "No welcome, sir, welcome, some hold true to thee!\n",
      "O the moon foolish and name with consented Isabel.\n",
      "\n",
      "cast hers.\n",
      "\n",
      "Thereing here be year the merity with his name, to my duty\n",
      "Would weep her lie and with the king's piece.\n",
      "\n",
      "KING RICHAr up havest,\n",
      "Death; what setting exile.\n",
      "\n",
      "PETRUCHIO:\n",
      "But upon his own ancient o' the common of people!\n",
      "Even, prince, and think whe wore that that\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speedometer",
   "id": "85c7993c75040a06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:20.043313Z",
     "start_time": "2025-06-13T07:15:16.024230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=False,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "297cc7c8a09ae162",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache Enabled: False\n",
      "Warmup Tokens: 100, Timing Tokens: 100, Runs: 5\n",
      "--------------------------------------------------\n",
      "Run  1: Latency = 0.79 ms/token, Throughput = 1269.76 tokens/sec\n",
      "Run  2: Latency = 0.75 ms/token, Throughput = 1324.76 tokens/sec\n",
      "Run  3: Latency = 0.75 ms/token, Throughput = 1333.79 tokens/sec\n",
      "Run  4: Latency = 0.77 ms/token, Throughput = 1307.03 tokens/sec\n",
      "Run  5: Latency = 0.76 ms/token, Throughput = 1316.55 tokens/sec\n",
      "--------------------------------------------------\n",
      "Summary (over 5 runs):\n",
      "  Avg    Latency: 0.76 ms/token\n",
      "  Std    Latency: 0.01 ms/token\n",
      "  Min    Latency: 0.75 ms/token\n",
      "  Max    Latency: 0.79 ms/token\n",
      "  Median Latency: 0.76 ms/token\n",
      "  Avg    Throughput: 1310.00 tokens/sec\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:20.043488Z",
     "start_time": "2025-06-13T07:15:16.942076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=True,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "b2ce78011b613db2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache Enabled: True\n",
      "Warmup Tokens: 100, Timing Tokens: 100, Runs: 5\n",
      "--------------------------------------------------\n",
      "Run  1: Latency = 0.96 ms/token, Throughput = 1043.41 tokens/sec\n",
      "Run  2: Latency = 0.95 ms/token, Throughput = 1048.69 tokens/sec\n",
      "Run  3: Latency = 0.95 ms/token, Throughput = 1054.51 tokens/sec\n",
      "Run  4: Latency = 0.96 ms/token, Throughput = 1037.64 tokens/sec\n",
      "Run  5: Latency = 0.95 ms/token, Throughput = 1049.47 tokens/sec\n",
      "--------------------------------------------------\n",
      "Summary (over 5 runs):\n",
      "  Avg    Latency: 0.96 ms/token\n",
      "  Std    Latency: 0.01 ms/token\n",
      "  Min    Latency: 0.95 ms/token\n",
      "  Max    Latency: 0.96 ms/token\n",
      "  Median Latency: 0.95 ms/token\n",
      "  Avg    Throughput: 1046.71 tokens/sec\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Profiling",
   "id": "3b0d7f5f39fef372"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:27.697234Z",
     "start_time": "2025-06-13T11:04:27.621106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = torch.randint(0, model_config.vocab_size, (1, model_config.max_seq_len), device=device)\n",
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(input_ids)\n",
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cuda_time_total\", row_limit=20))"
   ],
   "id": "d5f7d9c65d660033",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us      94.592us        40.49%      94.592us      23.648us             4  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_6...         0.00%       0.000us         0.00%       0.000us       0.000us      71.517us        30.61%      71.517us       4.470us            16  \n",
      "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us      32.799us        14.04%      32.799us       3.644us             9  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.648us         4.99%      11.648us       1.294us             9  \n",
      "void splitKreduce_kernel<32, 16, int, float, float, ...         0.00%       0.000us         0.00%       0.000us       0.000us       8.478us         3.63%       8.478us       1.696us             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.143us         2.63%       6.143us       1.536us             4  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       4.960us         2.12%       4.960us       2.480us             2  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       2.240us         0.96%       2.240us       2.240us             1  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.248us         0.53%       1.248us       1.248us             1  \n",
      "                                       cudaLaunchKernel        85.75%     460.571us        85.75%     460.571us       9.031us       0.000us         0.00%       0.000us       0.000us            51  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         3.42%      18.350us         3.42%      18.350us       0.918us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "                                   cudaFuncSetAttribute         5.65%      30.341us         5.65%      30.341us       1.896us       0.000us         0.00%       0.000us       0.000us            16  \n",
      "                                  cudaStreamIsCapturing         1.76%       9.470us         1.76%       9.470us       0.789us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                  cudaDeviceSynchronize         3.42%      18.379us         3.42%      18.379us      18.379us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 537.111us\n",
      "Self CUDA time total: 233.625us\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Scores",
   "id": "bcee9213d52654ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T11:04:30.115559Z",
     "start_time": "2025-06-13T11:04:29.770501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate a few samples and track attention scores\n",
    "prompts = [\n",
    "    \"To be, or not to be\",\n",
    "    \"Now is the winter of\",\n",
    "    \"Friends, Romans, countrymen\"\n",
    "]\n",
    "\n",
    "# Get attention scores and generate samples\n",
    "attention_scores = []\n",
    "print(\"Generated Samples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = char_tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate with attention tracking enabled\n",
    "    model.track_attention = True\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        use_cache=True,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        tokenizer=char_tokenizer\n",
    "    )\n",
    "\n",
    "    # Get attention scores from model\n",
    "    scores = torch.cat([layer.last_attn_scores for layer in model.transformer.blocks])\n",
    "    attention_scores.append(scores.detach().cpu())\n",
    "\n",
    "    # Print generated text\n",
    "    response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Plot histogram of attention scores\n",
    "scores_flat = torch.cat([s.flatten() for s in attention_scores])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores_flat.numpy(), bins=50, density=True)\n",
    "plt.title(\"Distribution of Attention Scores (Before Softmax)\")\n",
    "plt.xlabel(\"Score Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "bd75e7b9ae96e45e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Samples:\n",
      "--------------------------------------------------\n",
      " "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flash_attn_decode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Generate with attention tracking enabled\u001B[39;00m\n\u001B[32m     17\u001B[39m model.track_attention = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m output = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchar_tokenizer\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[38;5;66;03m# Get attention scores from model\u001B[39;00m\n\u001B[32m     28\u001B[39m scores = torch.cat([layer.last_attn_scores \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model.transformer.blocks])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:336\u001B[39m, in \u001B[36mGPT.generate\u001B[39m\u001B[34m(self, idx, use_cache, max_new_tokens, temperature, top_k, tokenizer)\u001B[39m\n\u001B[32m    333\u001B[39m idx_input = idx \u001B[38;5;28;01mif\u001B[39;00m idx.size(\u001B[32m1\u001B[39m) <= \u001B[38;5;28mself\u001B[39m.config.max_seq_len \u001B[38;5;28;01melse\u001B[39;00m idx[:, -\u001B[38;5;28mself\u001B[39m.config.max_seq_len:]\n\u001B[32m    335\u001B[39m \u001B[38;5;66;03m# ---------- Forward pass ----------------------------------------------------------------------------------\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m logits, _, kv_cache = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    337\u001B[39m \u001B[43m    \u001B[49m\u001B[43midx_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    338\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m    339\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m                                                                              \u001B[38;5;66;03m# [batch_size, 1, vocab_size]\u001B[39;00m\n\u001B[32m    340\u001B[39m logits = logits[:, -\u001B[32m1\u001B[39m, :]                                                         \u001B[38;5;66;03m# [batch_size, vocab_size]\u001B[39;00m\n\u001B[32m    342\u001B[39m \u001B[38;5;66;03m# ---------- Temperature -----------------------------------------------------------------------------------\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1843\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1844\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1845\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1846\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1847\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1848\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n\u001B[32m   1849\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m _global_forward_hooks.items():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1787\u001B[39m     bw_hook = BackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[32m   1788\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1790\u001B[39m result = \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1791\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n\u001B[32m   1792\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[32m   1793\u001B[39m         *_global_forward_hooks.items(),\n\u001B[32m   1794\u001B[39m         *\u001B[38;5;28mself\u001B[39m._forward_hooks.items(),\n\u001B[32m   1795\u001B[39m     ):\n\u001B[32m   1796\u001B[39m         \u001B[38;5;66;03m# mark that always called hook is run\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:297\u001B[39m, in \u001B[36mGPT.forward\u001B[39m\u001B[34m(self, input_ids, target_ids, kv_cache)\u001B[39m\n\u001B[32m    295\u001B[39m new_kv_cache = []\n\u001B[32m    296\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer_idx, block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.blocks):\n\u001B[32m--> \u001B[39m\u001B[32m297\u001B[39m     x, kv_cache_layer = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m                  \u001B[38;5;66;03m# [batch_size, seq_len, d_embed]\u001B[39;00m\n\u001B[32m    298\u001B[39m     new_kv_cache.append(kv_cache_layer)\n\u001B[32m    300\u001B[39m \u001B[38;5;66;03m# ---------- Final linear layer --------------------------------------------------------------------------------\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:217\u001B[39m, in \u001B[36mBlock.forward\u001B[39m\u001B[34m(self, x, kv_cache)\u001B[39m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, kv_cache):\n\u001B[32m    216\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm1(x)\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     y, new_kv_cache = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    218\u001B[39m     x = x + y\n\u001B[32m    220\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm2(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/PathFinder/models/GPT.py:78\u001B[39m, in \u001B[36mMultiHeadAttention.forward\u001B[39m\u001B[34m(self, x, kv_cache)\u001B[39m\n\u001B[32m     71\u001B[39m         attn_out = F.scaled_dot_product_attention(\n\u001B[32m     72\u001B[39m             q, k, v,\n\u001B[32m     73\u001B[39m             scale=\u001B[38;5;28mself\u001B[39m.scale,\n\u001B[32m     74\u001B[39m             dropout_p=\u001B[38;5;28mself\u001B[39m.config.dropout,\n\u001B[32m     75\u001B[39m             is_causal=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     76\u001B[39m         )                                                           \u001B[38;5;66;03m# [batch_size, n_heads, seq_len, d_head]\u001B[39;00m\n\u001B[32m     77\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:               \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m         attn_out = \u001B[43mflash_attn_decode\u001B[49m(\n\u001B[32m     79\u001B[39m             q,                                                            \u001B[38;5;66;03m# [batch_size, n_heads, 1, d_head]\u001B[39;00m\n\u001B[32m     80\u001B[39m             k, v,                                                   \u001B[38;5;66;03m# [batch_size, n_heads, seq_len, d_head]\u001B[39;00m\n\u001B[32m     81\u001B[39m             scale=\u001B[38;5;28mself\u001B[39m.scale\n\u001B[32m     82\u001B[39m         )                                                                 \u001B[38;5;66;03m# [batch_size, n_heads, 1, d_head]\u001B[39;00m\n\u001B[32m     84\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     85\u001B[39m     attn_scores = (q @ k.transpose(-\u001B[32m2\u001B[39m, -\u001B[32m1\u001B[39m)) * \u001B[38;5;28mself\u001B[39m.scale           \u001B[38;5;66;03m# [batch_size, n_heads, seq_len, seq_len]\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'flash_attn_decode' is not defined"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "663918715b83a74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
