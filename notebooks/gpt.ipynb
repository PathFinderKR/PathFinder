{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "36b2fa8101c30390"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:55.859694Z",
     "start_time": "2025-06-13T07:13:52.040790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import wandb\n",
    "from src.utils import set_seed, load_text, split_text, speedometer\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.tokenizer import CharTokenizer\n",
    "from models.GPT import GPT\n",
    "from src.train import Trainer"
   ],
   "id": "a38a265df86e4231",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:56.039672Z",
     "start_time": "2025-06-13T07:13:56.036561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + \"/..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")"
   ],
   "id": "70df4b9194f3773f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /workspace/PathFinder\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "1b632728e9a730d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:56.113521Z",
     "start_time": "2025-06-13T07:13:56.110271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=-1,\n",
    "    max_seq_len=128,\n",
    "    d_embed=128,\n",
    "    n_layers=4,\n",
    "    flash=True,\n",
    "    n_heads=4,\n",
    "    d_head=32,\n",
    "    scale=32 ** -0.5,\n",
    "    #rank=16,\n",
    "    d_ff=512,\n",
    "    #d_ff_multiple_of=64,\n",
    "    #beta_min=1/2,\n",
    "    #beta_max=8\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    model_name=\"PathFinder-nano\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    gradient_accumulation_steps=512 // 512,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-3,\n",
    "    attn_decay=0.5,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=True,\n",
    "    matmul_precision=\"high\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    top_k=50\n",
    ")"
   ],
   "id": "7321faac2fc60c61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:56.947463Z",
     "start_time": "2025-06-13T07:13:56.172713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "78c21c905da1750e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "a5aea71474607392"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "1a5ca12f86ee8ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:57.006394Z",
     "start_time": "2025-06-13T07:13:57.001575Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "1aee2436650dc3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "4d7ae71c3abae1ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:57.225727Z",
     "start_time": "2025-06-13T07:13:57.077884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores\n",
    "print(f\"MatMul Precision: {train_config.matmul_precision}\")"
   ],
   "id": "6fd52c7cdf9b9fff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA RTX A6000\n",
      "MatMul Precision: high\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "881b3d83e0f9a02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:57.376741Z",
     "start_time": "2025-06-13T07:13:57.370618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/shakespeare.txt\")\n",
    "shakespeare_text = load_text(dataset_path)"
   ],
   "id": "3fb5b3cb4c7caae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /workspace/PathFinder/datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:57.534831Z",
     "start_time": "2025-06-13T07:13:57.532404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    subset_shakespeare_text = shakespeare_text[:10000]\n",
    "    print(subset_shakespeare_text)\n",
    "    shakespeare_text = subset_shakespeare_text"
   ],
   "id": "891a7456f7efefb8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "73942bc06caadbf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:57.695495Z",
     "start_time": "2025-06-13T07:13:57.686559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "vocab_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/vocab.json\")\n",
    "char_tokenizer.save_vocab(vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "1422132939291d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n",
      "Vocabulary saved to /workspace/PathFinder/datasets/Shakespeare/vocab.json.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:57.836746Z",
     "start_time": "2025-06-13T07:13:57.834192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "28702519b26b8516",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "3e19cdb93ce1cca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:57.862604Z",
     "start_time": "2025-06-13T07:13:57.858949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "369661fd35cc5408",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 1003854 characters\n",
      "Validation text length: 111540 characters\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:58.059888Z",
     "start_time": "2025-06-13T07:13:57.938268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "ade0a4f3f06bc47e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:58.279204Z",
     "start_time": "2025-06-13T07:13:58.274818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][0]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][0]}\")"
   ],
   "id": "bd0090591cadc29e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c7426d2c0a176973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:13:59.749429Z",
     "start_time": "2025-06-13T07:13:58.293822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_params() / 1e6:.2f}M\")"
   ],
   "id": "2d25545316ca49d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (token_embedding): Embedding(69, 128)\n",
      "    (positional_encoding): Embedding(128, 128)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (qkv_proj): Linear(in_features=128, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (lm_head): Linear(in_features=128, out_features=69, bias=False)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 0.81M\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "bbe1c76409605d58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:13.926869Z",
     "start_time": "2025-06-13T07:13:59.778585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "df06c866b6549d28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/workspace/PathFinder/wandb/run-20250613_071359-v50enmnb</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/v50enmnb' target=\"_blank\">PathFinder-nano-2025-06-13_07-13-56</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/v50enmnb' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/v50enmnb</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 981/981 [01:11<00:00, 13.66it/s, epoch=1, grad_norm=0.1856, loss=1.2961, lr=0.000000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Learning Rate</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Val Loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Val Perplexity</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>0.1856</td></tr><tr><td>Learning Rate</td><td>0</td></tr><tr><td>Train Loss</td><td>1.29614</td></tr><tr><td>Val Loss</td><td>1.48906</td></tr><tr><td>Val Perplexity</td><td>4.43292</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PathFinder-nano-2025-06-13_07-13-56</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/v50enmnb' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/v50enmnb</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>/workspace/PathFinder/wandb/run-20250613_071359-v50enmnb/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "df43a98cae309849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:14.067654Z",
     "start_time": "2025-06-13T07:15:14.048200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not train_config.debug:\n",
    "    output_dir = os.path.join(PROJECT_ROOT, \"checkpoints\", train_config.model_name, train_config.run_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        model.save_pretrained(\n",
    "            output_dir,\n",
    "            safe_serialization=True\n",
    "        )\n",
    "        print(\"Model saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    # Push to Hugging Face Hub\n",
    "    #model.push_to_hub(\n",
    "    #    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    #    private=True,\n",
    "    #    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    #)\n",
    "    #print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{train_config.model_name}-{train_config.run_name}\")"
   ],
   "id": "a587e8a215674e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:14.087239Z",
     "start_time": "2025-06-13T07:15:14.085086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT(model_config)\n",
    "# model = model.from_pretrained(output_dir).to(device)"
   ],
   "id": "f3cad31e5a514fc0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "1ed82e9a71b4c9bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:33.199374Z",
     "start_time": "2025-06-13T07:15:31.710698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k,\n",
    "    tokenizer=char_tokenizer\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "847724c2f13f7a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Conspy, good preserve yourself\n",
      "All a worldred to-night. When the bloody might\n",
      "Makes; Resetting KV cache\n",
      "lock selfloughtill, brence as if it reason\n",
      "When I come, I let not not depended.\n",
      "\n",
      "YORK:\n",
      "No more; for I am mine own wrongs, good dResetting KV cache\n",
      " ser then wit.\n",
      "\n",
      "CLARENCE:\n",
      "My hardeous is said to my light safety, be a man,\n",
      "That nature's joy, 'tless spurplish'd,\n",
      "Shall I. Who Resetting KV cache\n",
      "now hearing us hold thing Rome, thy teether,\n",
      "My grace to the last times pray.\n",
      "Join Margaret: Ah, we'll remember them or Rutland,Resetting KV cache\n",
      " bawn offick and Emile are, they all Love of York;\n",
      "Raised heart and grave to you will command you.\n",
      "What dalice that you you stanResetting KV cache\n",
      " forced awards, of Henry:\n",
      "No welcome, sir, welcome, some hold true to thee!\n",
      "O the moon foolish and name with consented Isabel.\n",
      "\n",
      "Resetting KV cache\n",
      "cast hers.\n",
      "\n",
      "Thereing here be year the merity with his name, to my duty\n",
      "Would weep her lie and with the king's piece.\n",
      "\n",
      "KING RICHAResetting KV cache\n",
      "r up havest,\n",
      "Death; what setting exile.\n",
      "\n",
      "PETRUCHIO:\n",
      "But upon his own ancient o' the common of people!\n",
      "Even, prince, and think whResetting KV cache\n",
      "e wore that that"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:34.485273Z",
     "start_time": "2025-06-13T07:15:34.479850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"ü§ñ Model Response:\")\n",
    "print(response)"
   ],
   "id": "1ea28687151601ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User prompt: \n",
      "To be, or not to be, that is the question\n",
      "--------------------------------------------------\n",
      "ü§ñ Model Response:\n",
      "To be, or not to be, that is the question\n",
      "A Conspy, good preserve yourself\n",
      "All a worldred to-night. When the bloody might\n",
      "Makes; lock selfloughtill, brence as if it reason\n",
      "When I come, I let not not depended.\n",
      "\n",
      "YORK:\n",
      "No more; for I am mine own wrongs, good d ser then wit.\n",
      "\n",
      "CLARENCE:\n",
      "My hardeous is said to my light safety, be a man,\n",
      "That nature's joy, 'tless spurplish'd,\n",
      "Shall I. Who now hearing us hold thing Rome, thy teether,\n",
      "My grace to the last times pray.\n",
      "Join Margaret: Ah, we'll remember them or Rutland, bawn offick and Emile are, they all Love of York;\n",
      "Raised heart and grave to you will command you.\n",
      "What dalice that you you stan forced awards, of Henry:\n",
      "No welcome, sir, welcome, some hold true to thee!\n",
      "O the moon foolish and name with consented Isabel.\n",
      "\n",
      "cast hers.\n",
      "\n",
      "Thereing here be year the merity with his name, to my duty\n",
      "Would weep her lie and with the king's piece.\n",
      "\n",
      "KING RICHAr up havest,\n",
      "Death; what setting exile.\n",
      "\n",
      "PETRUCHIO:\n",
      "But upon his own ancient o' the common of people!\n",
      "Even, prince, and think whe wore that that\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speedometer",
   "id": "85c7993c75040a06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:16.921881Z",
     "start_time": "2025-06-13T07:15:16.024230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=False,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "297cc7c8a09ae162",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache Enabled: False\n",
      "Warmup Tokens: 100, Timing Tokens: 100, Runs: 5\n",
      "--------------------------------------------------\n",
      "Run  1: Latency = 0.79 ms/token, Throughput = 1269.76 tokens/sec\n",
      "Run  2: Latency = 0.75 ms/token, Throughput = 1324.76 tokens/sec\n",
      "Run  3: Latency = 0.75 ms/token, Throughput = 1333.79 tokens/sec\n",
      "Run  4: Latency = 0.77 ms/token, Throughput = 1307.03 tokens/sec\n",
      "Run  5: Latency = 0.76 ms/token, Throughput = 1316.55 tokens/sec\n",
      "--------------------------------------------------\n",
      "Summary (over 5 runs):\n",
      "  Avg    Latency: 0.76 ms/token\n",
      "  Std    Latency: 0.01 ms/token\n",
      "  Min    Latency: 0.75 ms/token\n",
      "  Max    Latency: 0.79 ms/token\n",
      "  Median Latency: 0.76 ms/token\n",
      "  Avg    Throughput: 1310.00 tokens/sec\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:18.306477Z",
     "start_time": "2025-06-13T07:15:16.942076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=True,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "b2ce78011b613db2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache Enabled: True\n",
      "Warmup Tokens: 100, Timing Tokens: 100, Runs: 5\n",
      "--------------------------------------------------\n",
      "Run  1: Latency = 0.96 ms/token, Throughput = 1043.41 tokens/sec\n",
      "Run  2: Latency = 0.95 ms/token, Throughput = 1048.69 tokens/sec\n",
      "Run  3: Latency = 0.95 ms/token, Throughput = 1054.51 tokens/sec\n",
      "Run  4: Latency = 0.96 ms/token, Throughput = 1037.64 tokens/sec\n",
      "Run  5: Latency = 0.95 ms/token, Throughput = 1049.47 tokens/sec\n",
      "--------------------------------------------------\n",
      "Summary (over 5 runs):\n",
      "  Avg    Latency: 0.96 ms/token\n",
      "  Std    Latency: 0.01 ms/token\n",
      "  Min    Latency: 0.95 ms/token\n",
      "  Max    Latency: 0.96 ms/token\n",
      "  Median Latency: 0.95 ms/token\n",
      "  Avg    Throughput: 1046.71 tokens/sec\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Profiling",
   "id": "3b0d7f5f39fef372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_ids = torch.randint(0, model_config.vocab_size, (1, model_config.max_seq_len), device=device)\n",
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(input_ids)\n",
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cuda_time_total\", row_limit=20))"
   ],
   "id": "d5f7d9c65d660033"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Scores",
   "id": "bcee9213d52654ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T07:15:18.320317Z",
     "start_time": "2025-06-13T07:15:18.317857Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bd75e7b9ae96e45e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
