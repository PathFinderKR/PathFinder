{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "36b2fa8101c30390"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:53.860480Z",
     "start_time": "2025-06-08T13:54:50.592310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from src.utils import set_seed, load_text, split_text, speedometer\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.tokenizer import CharTokenizer\n",
    "from models.GPT import GPT\n",
    "from src.train import Trainer"
   ],
   "id": "a38a265df86e4231",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/miniconda3/envs/torch-env/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:53.927118Z",
     "start_time": "2025-06-08T13:54:53.923767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")"
   ],
   "id": "70df4b9194f3773f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pathfinder/projects/PathFinder\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "1b632728e9a730d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:53.986605Z",
     "start_time": "2025-06-08T13:54:53.937804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=-1,\n",
    "    max_seq_len=128,\n",
    "    d_embed=128,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    d_head=32,\n",
    "    rank=16,\n",
    "    d_ff=-1,\n",
    "    d_ff_multiple_of=64,\n",
    "    beta_min=1/2,\n",
    "    beta_max=8\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    model_name=\"PathFinder\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    gradient_accumulation_steps=512 // 512,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-3,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=True,\n",
    "    matmul_precision=\"high\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    top_k=50\n",
    ")"
   ],
   "id": "7321faac2fc60c61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:54.694329Z",
     "start_time": "2025-06-08T13:54:53.995453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "78c21c905da1750e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/pathfinder/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "a5aea71474607392"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "1a5ca12f86ee8ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:54.713864Z",
     "start_time": "2025-06-08T13:54:54.708950Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "1aee2436650dc3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "4d7ae71c3abae1ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:54.884750Z",
     "start_time": "2025-06-08T13:54:54.729404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores\n",
    "print(f\"MatMul Precision: {train_config.matmul_precision}\")"
   ],
   "id": "6fd52c7cdf9b9fff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4080 SUPER\n",
      "MatMul Precision: high\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "881b3d83e0f9a02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:54.900769Z",
     "start_time": "2025-06-08T13:54:54.896398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/shakespeare.txt\")\n",
    "shakespeare_text = load_text(dataset_path)"
   ],
   "id": "3fb5b3cb4c7caae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /home/pathfinder/projects/PathFinder/datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:54.923913Z",
     "start_time": "2025-06-08T13:54:54.920935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    subset_shakespeare_text = shakespeare_text[:10000]\n",
    "    print(subset_shakespeare_text)\n",
    "    shakespeare_text = subset_shakespeare_text"
   ],
   "id": "891a7456f7efefb8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "73942bc06caadbf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:54.946522Z",
     "start_time": "2025-06-08T13:54:54.938286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "vocab_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/vocab.json\")\n",
    "char_tokenizer.save_vocab(vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "1422132939291d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n",
      "Vocabulary saved to /home/pathfinder/projects/PathFinder/datasets/Shakespeare/vocab.json.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:55.015094Z",
     "start_time": "2025-06-08T13:54:54.964947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "28702519b26b8516",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "3e19cdb93ce1cca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:55.027715Z",
     "start_time": "2025-06-08T13:54:55.023800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "369661fd35cc5408",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 1003854 characters\n",
      "Validation text length: 111540 characters\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:55.152652Z",
     "start_time": "2025-06-08T13:54:55.043360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "ade0a4f3f06bc47e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:55.164035Z",
     "start_time": "2025-06-08T13:54:55.161293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][0]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][0]}\")"
   ],
   "id": "bd0090591cadc29e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c7426d2c0a176973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:54:57.050867Z",
     "start_time": "2025-06-08T13:54:55.172894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_params() / 1e6:.2f}M\")"
   ],
   "id": "2d25545316ca49d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (token_embedding): Embedding(69, 128)\n",
      "    (positional_encoding): Embedding(128, 128)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (Wkv_down): Linear(in_features=128, out_features=16, bias=False)\n",
      "          (Wk_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (Wv_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=128, out_features=64, bias=False)\n",
      "          (fc2): Linear(in_features=64, out_features=128, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (Wkv_down): Linear(in_features=128, out_features=16, bias=False)\n",
      "          (Wk_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (Wv_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=128, out_features=384, bias=False)\n",
      "          (fc2): Linear(in_features=384, out_features=128, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (Wkv_down): Linear(in_features=128, out_features=16, bias=False)\n",
      "          (Wk_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (Wv_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=128, out_features=704, bias=False)\n",
      "          (fc2): Linear(in_features=704, out_features=128, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (Wkv_down): Linear(in_features=128, out_features=16, bias=False)\n",
      "          (Wk_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (Wv_up): Linear(in_features=16, out_features=128, bias=False)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=128, out_features=1024, bias=False)\n",
      "          (fc2): Linear(in_features=1024, out_features=128, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (lm_head): Linear(in_features=128, out_features=69, bias=False)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 0.73M\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "bbe1c76409605d58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:56:23.868813Z",
     "start_time": "2025-06-08T13:54:57.062661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "df06c866b6549d28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/pathfinder/projects/PathFinder/wandb/run-20250608_225457-036lzl43</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/036lzl43' target=\"_blank\">PathFinder-2025-06-08_22-54-53</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/036lzl43' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/036lzl43</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 981/981 [01:24<00:00, 11.55it/s, epoch=1, grad_norm=0.2023, loss=1.3613, lr=0.000000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>‚ñà‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Learning Rate</td><td>‚ñÇ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Val Loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Val Perplexity</td><td>‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>0.20231</td></tr><tr><td>Learning Rate</td><td>0</td></tr><tr><td>Train Loss</td><td>1.36126</td></tr><tr><td>Val Loss</td><td>1.52853</td></tr><tr><td>Val Perplexity</td><td>4.61139</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PathFinder-2025-06-08_22-54-53</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/036lzl43' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/036lzl43</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250608_225457-036lzl43/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "df43a98cae309849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:56:23.907107Z",
     "start_time": "2025-06-08T13:56:23.886694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not train_config.debug:\n",
    "    output_dir = os.path.join(PROJECT_ROOT, \"checkpoints\", train_config.model_name, train_config.run_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        model.save_pretrained(\n",
    "            output_dir,\n",
    "            safe_serialization=True\n",
    "        )\n",
    "        print(\"Model saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    # Push to Hugging Face Hub\n",
    "    #model.push_to_hub(\n",
    "    #    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    #    private=True,\n",
    "    #    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    #)\n",
    "    #print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{train_config.model_name}-{train_config.run_name}\")"
   ],
   "id": "a587e8a215674e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:56:23.931835Z",
     "start_time": "2025-06-08T13:56:23.929330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT(model_config)\n",
    "# model = model.from_pretrained(output_dir).to(device)"
   ],
   "id": "f3cad31e5a514fc0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "1ed82e9a71b4c9bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:56:28.596761Z",
     "start_time": "2025-06-08T13:56:23.957890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k,\n",
    "    tokenizer=char_tokenizer\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "847724c2f13f7a7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s of thy mother,\n",
      "Angel, if you not, I'll say unto you not so light.\n",
      "\n",
      "Page:\n",
      "Ay, farewell:Resetting KV cache\n",
      "\n",
      "Return you are being bottle, give me.\n",
      "\n",
      "LUCIO:\n",
      "What, I know have been, no before me, sweet sound a\n",
      "peace what will such rememberResetting KV cache\n",
      ", my sovereign, at is, one shame\n",
      "And all gives, if you should be gone.\n",
      "\n",
      "POLIXENES:\n",
      "She dare noble pure;\n",
      "And does but you.\n",
      "\n",
      "BUCKIResetting KV cache\n",
      "ll you of all\n",
      "The mark in Rome of Prince, truth, pitch me speak,\n",
      "We will be to have answer up in good. Thou despairs\n",
      "When it is Resetting KV cache\n",
      "prince content?\n",
      "He have my fellow resign than you ask me not.\n",
      "\n",
      "RICHARD:\n",
      "Tell me you die outh.\n",
      "But all thyself see must brew whatResetting KV cache\n",
      "he crowns,\n",
      "And work thy sisters boldness to me griave\n",
      "The birth free of it, banishment lords.\n",
      "\n",
      "KING RICHARD III:\n",
      "With woman--\n",
      "\n",
      "QResetting KV cache\n",
      "UEEN ELIZABETH:\n",
      "My roter's country, do your command, seem of night.\n",
      "I saw hear thee, fept, and kill'd them all the trebling worlResetting KV cache\n",
      "f\n",
      "To be a more and blest time in the foe;\n",
      "For I must cursess thou corn'd on our parding Lewis be\n",
      "To London, but so beg; our trueResetting KV cache\n",
      "r her:\n",
      "O, that s"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:56:28.617081Z",
     "start_time": "2025-06-08T13:56:28.613807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"ü§ñ Model Response:\")\n",
    "print(response)"
   ],
   "id": "1ea28687151601ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User prompt: \n",
      "To be, or not to be, that is the question\n",
      "--------------------------------------------------\n",
      "ü§ñ Model Response:\n",
      "To be, or not to be, that is the questions of thy mother,\n",
      "Angel, if you not, I'll say unto you not so light.\n",
      "\n",
      "Page:\n",
      "Ay, farewell:\n",
      "Return you are being bottle, give me.\n",
      "\n",
      "LUCIO:\n",
      "What, I know have been, no before me, sweet sound a\n",
      "peace what will such remember, my sovereign, at is, one shame\n",
      "And all gives, if you should be gone.\n",
      "\n",
      "POLIXENES:\n",
      "She dare noble pure;\n",
      "And does but you.\n",
      "\n",
      "BUCKIll you of all\n",
      "The mark in Rome of Prince, truth, pitch me speak,\n",
      "We will be to have answer up in good. Thou despairs\n",
      "When it is prince content?\n",
      "He have my fellow resign than you ask me not.\n",
      "\n",
      "RICHARD:\n",
      "Tell me you die outh.\n",
      "But all thyself see must brew whathe crowns,\n",
      "And work thy sisters boldness to me griave\n",
      "The birth free of it, banishment lords.\n",
      "\n",
      "KING RICHARD III:\n",
      "With woman--\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "My roter's country, do your command, seem of night.\n",
      "I saw hear thee, fept, and kill'd them all the trebling worlf\n",
      "To be a more and blest time in the foe;\n",
      "For I must cursess thou corn'd on our parding Lewis be\n",
      "To London, but so beg; our truer her:\n",
      "O, that s\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speedometer",
   "id": "85c7993c75040a06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:56:30.150905Z",
     "start_time": "2025-06-08T13:56:28.633860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=False,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "297cc7c8a09ae162",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV cache enabled: False\n",
      "Latency (Time per token): 2.80 ms\n",
      "Throughput (Tokens per second): 357.06\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:56:31.629176Z",
     "start_time": "2025-06-08T13:56:30.166150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedometer(\n",
    "    model=model,\n",
    "    input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "    use_cache=True,\n",
    "    warmup_tokens=100,\n",
    "    timing_tokens=100,\n",
    "    num_runs=5\n",
    ")"
   ],
   "id": "b2ce78011b613db2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV cache enabled: True\n",
      "Latency (Time per token): 2.66 ms\n",
      "Throughput (Tokens per second): 376.01\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
