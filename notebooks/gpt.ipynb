{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "9c5839c9a3cf64fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:35.175754Z",
     "start_time": "2025-10-04T11:22:30.354287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import wandb\n",
    "from src.utils import set_seed, load_text, split_text\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.train import Trainer\n",
    "from tokenizer.tokenizer import CharTokenizer\n",
    "from models.GPT import GPT"
   ],
   "id": "46f01dcbb360429a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:35.191232Z",
     "start_time": "2025-10-04T11:22:35.187732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + \"/..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")"
   ],
   "id": "8d59d28379b9627c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pathfinder/projects/PathFinder\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "e1db9708d016dbff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:35.201037Z",
     "start_time": "2025-10-04T11:22:35.197185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=-1,\n",
    "    max_seq_len=128,\n",
    "    flash=True,\n",
    "    d_embed=256,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    rank=16,\n",
    "    d_ff=1024,\n",
    "    cla=False\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    gradient_accumulation_steps=512 // 512,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    attn_decay=0.5,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=True,\n",
    "    matmul_precision=\"high\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    top_k=50\n",
    ")"
   ],
   "id": "40c1b49185fe925c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:36.892702Z",
     "start_time": "2025-10-04T11:22:35.253891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "d4f61f1f62a34e98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/pathfinder/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "c58f70d689c96eb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "f274b536c84b9fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:36.970543Z",
     "start_time": "2025-10-04T11:22:36.963981Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "589c9877d81b1ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "41576077c1495528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.153174Z",
     "start_time": "2025-10-04T11:22:36.975802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores\n",
    "print(f\"MatMul Precision: {train_config.matmul_precision}\")"
   ],
   "id": "4480ebea1f838033",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4080 SUPER\n",
      "MatMul Precision: high\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "3ce6a33cb70f30bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.172764Z",
     "start_time": "2025-10-04T11:22:37.163809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/shakespeare.txt\")\n",
    "shakespeare_text = load_text(dataset_path)"
   ],
   "id": "c8b140fbb518dd26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /home/pathfinder/projects/PathFinder/datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.183542Z",
     "start_time": "2025-10-04T11:22:37.179358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    subset_shakespeare_text = shakespeare_text[:10000]\n",
    "    print(subset_shakespeare_text)\n",
    "    shakespeare_text = subset_shakespeare_text"
   ],
   "id": "2d80cd5188573c48",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "8d61430be1c7c5eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.199317Z",
     "start_time": "2025-10-04T11:22:37.190207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "vocab_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/vocab.json\")\n",
    "char_tokenizer.save_vocab(vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "432c5fbe09cb4f09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n",
      "Vocabulary saved to /home/pathfinder/projects/PathFinder/datasets/Shakespeare/vocab.json.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.209389Z",
     "start_time": "2025-10-04T11:22:37.206243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "11c6e1486b065782",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "ca72c158757f0704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.218789Z",
     "start_time": "2025-10-04T11:22:37.214652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "9c950bd3950a08db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 1003854 characters\n",
      "Validation text length: 111540 characters\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.391154Z",
     "start_time": "2025-10-04T11:22:37.275691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        #\"attention_mask\": attention_mask,\n",
    "        \"target_ids\": target_ids\n",
    "    }\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "620a9b685b7bca7c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:37.402181Z",
     "start_time": "2025-10-04T11:22:37.398881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][0]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][0]}\")"
   ],
   "id": "a6a326932c2034e1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d680f9ad3de3eefb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:22:39.053822Z",
     "start_time": "2025-10-04T11:22:37.408072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.get_num_params() / 1e6:.2f}M\")"
   ],
   "id": "2a82b103029ecc60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (token_embedding): Embedding(69, 256)\n",
      "    (positional_encoding): Embedding(128, 256)\n",
      "    (dropout): Dropout(p=0.01, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (Wkv_down): Linear(in_features=256, out_features=16, bias=False)\n",
      "          (Wk_up): Linear(in_features=16, out_features=256, bias=False)\n",
      "          (Wv_up): Linear(in_features=16, out_features=256, bias=False)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.01, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): FeedForward(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.01, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (lm_head): Linear(in_features=256, out_features=69, bias=False)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 2.73M\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "d016b0c6fe5f3be0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:24:56.527628Z",
     "start_time": "2025-10-04T11:22:39.113780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "57885df1d97b3537",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/pathfinder/projects/PathFinder/wandb/run-20251004_202239-21p0gaam</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/21p0gaam' target=\"_blank\">GPT2-MLA-2025-10-04_20-22-35</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/21p0gaam' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/21p0gaam</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 981/981 [02:15<00:00,  7.23it/s, epoch=1, grad_norm=0.5658, loss=2.0813, lr=0.000000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "70d13a54dbbc980bf1454032cefc6f69"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>▄▃▂▂▂▁▁▁▁▂▂▁▂▁▂▄█▂▃▁▂▂▂▃▂▂▂▃▂▁▂▂▃▂▁▁▁▁▁▂</td></tr><tr><td>Learning Rate</td><td>▅▆▆▆█████▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>██▇▇▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▆▅▄▂▂▁▁▁▁</td></tr><tr><td>Val Perplexity</td><td>█▆▅▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>0.56576</td></tr><tr><td>Learning Rate</td><td>0</td></tr><tr><td>Train Loss</td><td>2.08134</td></tr><tr><td>Val Loss</td><td>2.13713</td></tr><tr><td>Val Perplexity</td><td>8.47509</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT2-MLA-2025-10-04_20-22-35</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/21p0gaam' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/21p0gaam</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>/home/pathfinder/projects/PathFinder/wandb/run-20251004_202239-21p0gaam/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "5fb3b0ae98408329"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:24:56.597957Z",
     "start_time": "2025-10-04T11:24:56.595172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not train_config.debug:\n",
    "    pass\n",
    "    #output_dir = os.path.join(PROJECT_ROOT, \"checkpoints\", train_config.model_name, train_config.run_name)\n",
    "    #os.makedirs(output_dir, exist_ok=True)\n",
    "    #try:\n",
    "    #    model.save_pretrained(\n",
    "    #        output_dir,\n",
    "    #        safe_serialization=True\n",
    "    #    )\n",
    "    #    print(\"Model saved successfully\")\n",
    "    #xcept Exception as e:\n",
    "    #    print(f\"Error saving model: {e}\")\n",
    "    # Push to Hugging Face Hub\n",
    "    #model.push_to_hub(\n",
    "    #    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    #    private=True,\n",
    "    #    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    #)\n",
    "    #print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{train_config.model_name}-{train_config.run_name}\")"
   ],
   "id": "aa7fc811417bd46",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:24:56.604821Z",
     "start_time": "2025-10-04T11:24:56.601930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT.from_pretrained(output_dir).to(device)"
   ],
   "id": "189947e80319f23e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "754c9ac783ec7ce0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:25:00.082403Z",
     "start_time": "2025-10-04T11:24:56.608928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k,\n",
    "    tokenizer=char_tokenizer\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "51651a0e0120012d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g utrngs.\n",
      "Thasth high dare Seve's hear weis wellds,\n",
      "Andve shous mele fat wor eal thingm\n",
      "Resetting KV cache\n",
      "Mame he you thour 'te bottlle! alle bay,\n",
      "Ived Ran'sw I Happromes femartod be facambts\n",
      "whend our levesens whre wilour worre the tResetting KV cache\n",
      ", mort of aslt, at iny onges peor esm,\n",
      "Id im twither chadere oul gows oulys hour\n",
      "Whutind! jut bon pureding dvighm, theef.\n",
      "\n",
      "BOMINResetting KV cache\n",
      "lce, I'l way,\n",
      "Thiper, fit balim kidrp, tay theiced ive thas tooty\n",
      "He prauke tomey hean.\n",
      "\n",
      "Sey:\n",
      "Deer; the sonr thy alde my.\n",
      "\n",
      "BEDY:Resetting KV cache\n",
      " Rit to, ktoy, a lofe wigh'd hour reaiag\n",
      "Warsen stiser thates ime; hef heer, tity nidd ard halll dak\n",
      "Clits youses my co deor hicResetting KV cache\n",
      "ovick,\n",
      "Thou sher the 'vis, meavers tith nouimes\n",
      "I ave con tim. Whan boty you and thaks lorch.\n",
      "\n",
      "KING VIMARK:\n",
      "Mis hener'd, det balResetting KV cache\n",
      "et the oowelg nise rutld\n",
      "Lecart seamessog whas all hes budol nit,\n",
      "O, es wourd you tit ep; balllk I'suckes, al sthe?\n",
      "\n",
      "GLOUMES:\n",
      "DuResetting KV cache\n",
      "f mory.\n",
      "\n",
      "DUCHANCES:\n",
      "Hest tisth by scon whey you o! do bre stro oter?\n",
      "GLat I fordo. dis Aneeleng ut theiors, dity.\n",
      "\n",
      "RONGLTIO:\n",
      "I tResetting KV cache\n",
      "he noid agorr:\n",
      "T"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:25:00.148397Z",
     "start_time": "2025-10-04T11:25:00.145074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"🤖 Model Response:\")\n",
    "print(response)"
   ],
   "id": "6e66a6c36144f057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User prompt: \n",
      "To be, or not to be, that is the question\n",
      "--------------------------------------------------\n",
      "🤖 Model Response:\n",
      "To be, or not to be, that is the questiong utrngs.\n",
      "Thasth high dare Seve's hear weis wellds,\n",
      "Andve shous mele fat wor eal thingm\n",
      "Mame he you thour 'te bottlle! alle bay,\n",
      "Ived Ran'sw I Happromes femartod be facambts\n",
      "whend our levesens whre wilour worre the t, mort of aslt, at iny onges peor esm,\n",
      "Id im twither chadere oul gows oulys hour\n",
      "Whutind! jut bon pureding dvighm, theef.\n",
      "\n",
      "BOMINlce, I'l way,\n",
      "Thiper, fit balim kidrp, tay theiced ive thas tooty\n",
      "He prauke tomey hean.\n",
      "\n",
      "Sey:\n",
      "Deer; the sonr thy alde my.\n",
      "\n",
      "BEDY: Rit to, ktoy, a lofe wigh'd hour reaiag\n",
      "Warsen stiser thates ime; hef heer, tity nidd ard halll dak\n",
      "Clits youses my co deor hicovick,\n",
      "Thou sher the 'vis, meavers tith nouimes\n",
      "I ave con tim. Whan boty you and thaks lorch.\n",
      "\n",
      "KING VIMARK:\n",
      "Mis hener'd, det balet the oowelg nise rutld\n",
      "Lecart seamessog whas all hes budol nit,\n",
      "O, es wourd you tit ep; balllk I'suckes, al sthe?\n",
      "\n",
      "GLOUMES:\n",
      "Duf mory.\n",
      "\n",
      "DUCHANCES:\n",
      "Hest tisth by scon whey you o! do bre stro oter?\n",
      "GLat I fordo. dis Aneeleng ut theiors, dity.\n",
      "\n",
      "RONGLTIO:\n",
      "I the noid agorr:\n",
      "T\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T11:25:00.510651Z",
     "start_time": "2025-10-04T11:25:00.195721Z"
    }
   },
   "cell_type": "code",
   "source": "asdf",
   "id": "27916ddfcd922be5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m asdf\n",
      "\u001B[31mNameError\u001B[39m: name 'asdf' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speedometer",
   "id": "df022595674f1a7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if train_config.debug:\n",
    "    speedometer(\n",
    "        model=model,\n",
    "        input_ids=char_tokenizer.encode(\"a\").unsqueeze(0).to(device),\n",
    "        use_cache=False,\n",
    "        warmup_tokens=100,\n",
    "        timing_tokens=100,\n",
    "        num_runs=5\n",
    "    )"
   ],
   "id": "ec609d8746cd31d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Profiling",
   "id": "df64b918376b3b29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if train_config.debug:\n",
    "    input_ids = torch.randint(0, model_config.vocab_size, (1, model_config.max_seq_len), device=device)\n",
    "    with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            model(input_ids)\n",
    "    print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cuda_time_total\", row_limit=20))"
   ],
   "id": "acc60cff49785962"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Scores",
   "id": "f4b5be20f8b3715d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_max_distribution(self, layer_idx=0, head_idx=0, seq_len=512, num_samples=1000, plot=True):\n",
    "    \"\"\"\n",
    "    Predict and visualize the distribution of maximum attention scores\n",
    "\n",
    "    Args:\n",
    "        layer_idx: Layer to analyze\n",
    "        head_idx: Head to analyze\n",
    "        seq_len: Sequence length\n",
    "        num_samples: Number of samples for distribution\n",
    "        plot: Whether to plot the histogram\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with max values and statistics\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    device = next(self.parameters()).device\n",
    "    config = self.config\n",
    "    scale = config.scale if config.scale is not None else config.d_head ** -0.5\n",
    "\n",
    "    if config.rank is not None:\n",
    "        print(\"Multi Head Latent Attention not supported yet\")\n",
    "        return None\n",
    "\n",
    "    # Get attention weights\n",
    "    attn_layer = self.blocks[layer_idx].attn\n",
    "    qkv_weight = attn_layer.qkv_proj.weight\n",
    "    wq_full = qkv_weight[:config.d_embed, :]\n",
    "    wk_full = qkv_weight[config.d_embed:2*config.d_embed, :]\n",
    "\n",
    "    # Extract head-specific weights\n",
    "    head_start = head_idx * config.d_head\n",
    "    head_end = (head_idx + 1) * config.d_head\n",
    "    wq_head = wq_full[head_start:head_end, :]  # [d_head, d_embed]\n",
    "    wk_head = wk_full[head_start:head_end, :]  # [d_head, d_embed]\n",
    "\n",
    "    print(f\"Generating distribution for Layer {layer_idx}, Head {head_idx}\")\n",
    "    print(f\"Sequence length: {seq_len}, Samples: {num_samples}\")\n",
    "\n",
    "    max_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Progress: {i}/{num_samples}\")\n",
    "\n",
    "            # Sample random input (LayerNorm output assumption)\n",
    "            x = torch.randn(seq_len, config.d_embed, device=device)\n",
    "\n",
    "            # Compute Q, K\n",
    "            q = x @ wq_head.T  # [seq_len, d_head]\n",
    "            k = x @ wk_head.T  # [seq_len, d_head]\n",
    "\n",
    "            # Compute attention scores\n",
    "            attn_scores = q @ k.T * scale  # [seq_len, seq_len]\n",
    "\n",
    "            # Apply causal mask\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "            attn_masked = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "\n",
    "            # Get maximum value\n",
    "            max_val = attn_masked.max().item()\n",
    "            if max_val != float('-inf'):\n",
    "                max_values.append(max_val)\n",
    "\n",
    "    max_values = np.array(max_values)\n",
    "\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'max_values': max_values,\n",
    "        'mean': np.mean(max_values),\n",
    "        'std': np.std(max_values),\n",
    "        'median': np.median(max_values),\n",
    "        'min': np.min(max_values),\n",
    "        'max': np.max(max_values),\n",
    "        'percentiles': {\n",
    "            '25': np.percentile(max_values, 25),\n",
    "            '75': np.percentile(max_values, 75),\n",
    "            '90': np.percentile(max_values, 90),\n",
    "            '95': np.percentile(max_values, 95),\n",
    "            '99': np.percentile(max_values, 99)\n",
    "        },\n",
    "        'layer_idx': layer_idx,\n",
    "        'head_idx': head_idx,\n",
    "        'seq_len': seq_len,\n",
    "        'scale': scale\n",
    "    }\n",
    "\n",
    "    # Theoretical comparison\n",
    "    wqk_product = wq_head @ wk_head.T\n",
    "    qk_variance = torch.trace(wqk_product).item()\n",
    "    scaled_std = np.sqrt(qk_variance) * scale\n",
    "    theoretical_max = scaled_std * np.sqrt(2 * np.log(seq_len))\n",
    "\n",
    "    stats['theoretical'] = {\n",
    "        'qk_std': np.sqrt(qk_variance),\n",
    "        'scaled_std': scaled_std,\n",
    "        'predicted_max': theoretical_max\n",
    "    }\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\n=== DISTRIBUTION STATISTICS ===\")\n",
    "    print(f\"Sample size: {len(max_values)}\")\n",
    "    print(f\"Mean: {stats['mean']:.4f}\")\n",
    "    print(f\"Std: {stats['std']:.4f}\")\n",
    "    print(f\"Median: {stats['median']:.4f}\")\n",
    "    print(f\"Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "    print(f\"90th percentile: {stats['percentiles']['90']:.4f}\")\n",
    "    print(f\"95th percentile: {stats['percentiles']['95']:.4f}\")\n",
    "    print(f\"99th percentile: {stats['percentiles']['99']:.4f}\")\n",
    "    print(f\"Theoretical prediction: {theoretical_max:.4f}\")\n",
    "    print(f\"Empirical vs Theoretical: {stats['mean']/theoretical_max:.4f}\")\n",
    "\n",
    "    if plot:\n",
    "        plot_max_distribution(stats)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def plot_max_distribution(stats):\n",
    "    \"\"\"\n",
    "    Plot histogram of maximum attention scores\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    max_values = stats['max_values']\n",
    "    theoretical_max = stats['theoretical']['predicted_max']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Main histogram\n",
    "    plt.subplot(2, 2, 1)\n",
    "    n, bins, patches = plt.hist(max_values, bins=50, density=True, alpha=0.7,\n",
    "                               color='lightcoral', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    # Add vertical lines for key statistics\n",
    "    plt.axvline(stats['mean'], color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {stats[\"mean\"]:.3f}')\n",
    "    plt.axvline(stats['median'], color='blue', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {stats[\"median\"]:.3f}')\n",
    "    plt.axvline(theoretical_max, color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Theoretical: {theoretical_max:.3f}')\n",
    "    plt.axvline(stats['percentiles']['95'], color='orange', linestyle='--', linewidth=2,\n",
    "                label=f'95th %ile: {stats[\"percentiles\"][\"95\"]:.3f}')\n",
    "\n",
    "    plt.xlabel('Maximum Attention Score (QK^T/√D)')\n",
    "    plt.ylabel('Frequency Density')\n",
    "    plt.title(f'Distribution of Max Attention Scores\\n'\n",
    "              f'Layer {stats[\"layer_idx\"]}, Head {stats[\"head_idx\"]}, T={stats[\"seq_len\"]}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Cumulative distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sorted_values = np.sort(max_values)\n",
    "    cumulative = np.arange(1, len(sorted_values) + 1) / len(sorted_values)\n",
    "    plt.plot(sorted_values, cumulative, 'b-', linewidth=2)\n",
    "    plt.axvline(stats['mean'], color='red', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(theoretical_max, color='green', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Maximum Attention Score')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.title('Cumulative Distribution Function')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Box plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    box_data = [max_values]\n",
    "    bp = plt.boxplot(box_data, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    plt.ylabel('Maximum Attention Score')\n",
    "    plt.title('Box Plot')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Q-Q plot vs normal distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    from scipy import stats as scipy_stats\n",
    "    scipy_stats.probplot(max_values, dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot vs Normal Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Separate figure for detailed histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # More detailed histogram\n",
    "    plt.hist(max_values, bins=75, density=True, alpha=0.8,\n",
    "             color='lightcoral', edgecolor='black', linewidth=0.3)\n",
    "\n",
    "    # Add statistics lines\n",
    "    plt.axvline(stats['mean'], color='red', linestyle='-', linewidth=3,\n",
    "                label=f'Mean: {stats[\"mean\"]:.4f}')\n",
    "    plt.axvline(stats['median'], color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Median: {stats[\"median\"]:.4f}')\n",
    "    plt.axvline(theoretical_max, color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Theoretical: {theoretical_max:.4f}')\n",
    "\n",
    "    # Add percentile lines\n",
    "    plt.axvline(stats['percentiles']['90'], color='orange', linestyle=':', linewidth=2,\n",
    "                label=f'90th %ile: {stats[\"percentiles\"][\"90\"]:.4f}')\n",
    "    plt.axvline(stats['percentiles']['95'], color='purple', linestyle=':', linewidth=2,\n",
    "                label=f'95th %ile: {stats[\"percentiles\"][\"95\"]:.4f}')\n",
    "    plt.axvline(stats['percentiles']['99'], color='brown', linestyle=':', linewidth=2,\n",
    "                label=f'99th %ile: {stats[\"percentiles\"][\"99\"]:.4f}')\n",
    "\n",
    "    plt.xlabel('Maximum Attention Score (QK^T/√D)', fontsize=12)\n",
    "    plt.ylabel('Frequency Density', fontsize=12)\n",
    "    plt.title(f'Distribution of Maximum Attention Scores\\n'\n",
    "              f'Layer {stats[\"layer_idx\"]}, Head {stats[\"head_idx\"]}, '\n",
    "              f'Sequence Length {stats[\"seq_len\"]}, Scale {stats[\"scale\"]:.6f}', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add text box with statistics\n",
    "    textstr = f'''Statistics:\n",
    "    Samples: {len(max_values)}\n",
    "    Mean: {stats[\"mean\"]:.4f}\n",
    "    Std: {stats[\"std\"]:.4f}\n",
    "    Min: {stats[\"min\"]:.4f}\n",
    "    Max: {stats[\"max\"]:.4f}'''\n",
    "\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=9,\n",
    "             verticalalignment='top', bbox=props)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_multiple_heads(self, seq_len=512, num_samples=500, max_heads=8):\n",
    "    \"\"\"\n",
    "    Compare distributions across multiple heads\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    all_stats = []\n",
    "\n",
    "    for head_idx in range(min(max_heads, self.config.n_heads)):\n",
    "        print(f\"\\nAnalyzing Head {head_idx}...\")\n",
    "        stats = self.predict_max_distribution(\n",
    "            layer_idx=0, head_idx=head_idx,\n",
    "            seq_len=seq_len, num_samples=num_samples, plot=False\n",
    "        )\n",
    "        all_stats.append(stats)\n",
    "\n",
    "        ax = axes[head_idx]\n",
    "        max_values = stats['max_values']\n",
    "\n",
    "        ax.hist(max_values, bins=30, density=True, alpha=0.7,\n",
    "                color=plt.cm.Set3(head_idx), edgecolor='black', linewidth=0.5)\n",
    "        ax.axvline(stats['mean'], color='red', linestyle='--', linewidth=2)\n",
    "        ax.set_title(f'Head {head_idx}\\nMean: {stats[\"mean\"]:.3f}')\n",
    "        ax.set_xlabel('Max Score')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'Distribution Comparison Across Heads (Layer 0, T={seq_len})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_stats\n",
    "\n",
    "# Usage examples:\n",
    "stats = model.predict_max_distribution(layer_idx=0, head_idx=0, seq_len=512, num_samples=1000)\n",
    "all_stats = model.compare_multiple_heads(seq_len=512, num_samples=500)"
   ],
   "id": "7ebe6112c88b820b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_qk_distribution(model, model_config, num_samples=1000):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "\n",
    "    # 입력 분포 (LayerNorm 후라고 가정: 평균=0, 분산=1)\n",
    "    with torch.no_grad():\n",
    "        x_samples = torch.randn(num_samples, model_config.d_embed, device=device)  # [num_samples, d_embed]\n",
    "\n",
    "    scale = model_config.scale\n",
    "    theoretical_max = np.sqrt(2 * np.log(model_config.max_seq_len))\n",
    "\n",
    "    for layer_idx in range(model_config.n_layers):\n",
    "        attn_layer = model.blocks[layer_idx].attn\n",
    "\n",
    "        if model_config.rank is None:\n",
    "            # Multi Head Attention\n",
    "            qkv_weight = attn_layer.qkv_proj.weight  # [3*d_embed, d_embed]\n",
    "            wq_full = qkv_weight[:model_config.d_embed, :]      # [d_embed, d_embed]\n",
    "            wk_full = qkv_weight[model_config.d_embed:2*model_config.d_embed, :]  # [d_embed, d_embed]\n",
    "\n",
    "            layer_results = {}\n",
    "\n",
    "            for head_idx in range(model_config.n_heads):\n",
    "                # 헤드별 가중치 추출\n",
    "                head_start = head_idx * model_config.d_head\n",
    "                head_end = (head_idx + 1) * model_config.d_head\n",
    "\n",
    "                wq_head = wq_full[head_start:head_end, :]  # [d_head, d_embed]\n",
    "                wk_head = wk_full[head_start:head_end, :]  # [d_head, d_embed]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Q, K 계산\n",
    "                    q_samples = x_samples @ wq_head.T  # [num_samples, d_head]\n",
    "                    k_samples = x_samples @ wk_head.T  # [num_samples, d_head]\n",
    "\n",
    "                    # 실제 분산 계산 (CLT 기반)\n",
    "                    # Var[QK^T] = Σ(d=1 to d_head) Var[Q_d * K_d]\n",
    "                    # = Σ(d=1 to d_head) E[Q_d^2] * E[K_d^2] (독립성 가정)\n",
    "                    q_var_per_dim = q_samples.var(dim=0)  # [d_head]\n",
    "                    k_var_per_dim = k_samples.var(dim=0)  # [d_head]\n",
    "\n",
    "                    # 실제 분산 (정확한 계산)\n",
    "                    actual_variance = (q_var_per_dim * k_var_per_dim).sum().item()\n",
    "                    actual_std = np.sqrt(actual_variance)\n",
    "\n",
    "                    # 스케일링 후 분산\n",
    "                    scaled_variance = actual_variance * (scale ** 2)\n",
    "                    scaled_std = actual_std * scale\n",
    "\n",
    "                    # 최댓값 분포 근사 (Gumbel 분포 사용)\n",
    "                    # max(QK^T/√D) ≈ μ + σ * √(2 log(seq_len))\n",
    "                    predicted_max = scaled_std * np.sqrt(2 * np.log(model_config.max_seq_len))\n",
    "\n",
    "                    # 실제 계산으로 검증 (작은 샘플로)\n",
    "                    if head_idx == 0:  # 첫 번째 헤드만 실제 계산\n",
    "                        qk_scores = q_samples @ k_samples.T  # [num_samples, num_samples]\n",
    "                        qk_scaled = qk_scores * scale\n",
    "\n",
    "                        # Causal mask (상삼각 제거)\n",
    "                        tril_mask = torch.tril(torch.ones(num_samples, num_samples, device=device))\n",
    "                        qk_masked = qk_scaled.masked_fill(tril_mask == 0, float('-inf'))\n",
    "\n",
    "                        max_values = qk_masked.max(dim=-1).values  # [num_samples]\n",
    "                        max_values = max_values[max_values != float('-inf')]\n",
    "\n",
    "                        empirical_max_mean = max_values.mean().item()\n",
    "                        empirical_max_std = max_values.std().item()\n",
    "\n",
    "                    layer_results[head_idx] = {\n",
    "                        'actual_variance': actual_variance,\n",
    "                        'actual_std': actual_std,\n",
    "                        'scaled_variance': scaled_variance,\n",
    "                        'scaled_std': scaled_std,\n",
    "                        'predicted_max': predicted_max,\n",
    "                        'q_var_mean': q_var_per_dim.mean().item(),\n",
    "                        'k_var_mean': k_var_per_dim.mean().item(),\n",
    "                    }\n",
    "\n",
    "                    # 첫 번째 헤드는 실제 값도 저장\n",
    "                    if head_idx == 0:\n",
    "                        layer_results[head_idx].update({\n",
    "                            'empirical_max_mean': empirical_max_mean,\n",
    "                            'empirical_max_std': empirical_max_std,\n",
    "                        })\n",
    "\n",
    "            results[layer_idx] = layer_results\n",
    "\n",
    "        else:\n",
    "            # Multi Head Latent Attention (간단히 처리)\n",
    "            print(f\"Layer {layer_idx}: Multi Head Latent Attention (skipped)\")\n",
    "            continue\n",
    "\n",
    "    # 결과 출력\n",
    "    print_comprehensive_results(results, theoretical_max, scale)\n",
    "    return results\n",
    "\n",
    "def print_comprehensive_results(results, theoretical_max, scale):\n",
    "    \"\"\"결과를 보기 좋게 출력\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*20} COMPREHENSIVE QK^T DISTRIBUTION ANALYSIS {'='*20}\")\n",
    "    print(f\"Scale factor: {scale:.6f}\")\n",
    "    print(f\"Theoretical max (√(2log T)): {theoretical_max:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # 헤더\n",
    "    print(f\"{'Layer':<6} {'Head':<4} {'Actual_Std':<10} {'Scaled_Std':<10} {'Pred_Max':<9} {'Q_Var':<7} {'K_Var':<7} {'Status':<10}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    all_scaled_stds = []\n",
    "    all_pred_maxes = []\n",
    "\n",
    "    for layer_idx, layer_data in results.items():\n",
    "        for head_idx, head_data in layer_data.items():\n",
    "            actual_std = head_data['actual_std']\n",
    "            scaled_std = head_data['scaled_std']\n",
    "            pred_max = head_data['predicted_max']\n",
    "            q_var = head_data['q_var_mean']\n",
    "            k_var = head_data['k_var_mean']\n",
    "\n",
    "            all_scaled_stds.append(scaled_std)\n",
    "            all_pred_maxes.append(pred_max)\n",
    "\n",
    "            # 상태 판정\n",
    "            if scaled_std > 1.5:\n",
    "                status = \"🔴HIGH\"\n",
    "            elif scaled_std > 1.1:\n",
    "                status = \"🟡MED\"\n",
    "            else:\n",
    "                status = \"🟢OK\"\n",
    "\n",
    "            print(f\"{layer_idx:<6} {head_idx:<4} {actual_std:<10.4f} {scaled_std:<10.4f} {pred_max:<9.3f} {q_var:<7.3f} {k_var:<7.3f} {status:<10}\")\n",
    "\n",
    "    # 통계 요약\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"SUMMARY STATISTICS:\")\n",
    "    print(f\"  Scaled Std  - Mean: {np.mean(all_scaled_stds):.4f}, Std: {np.std(all_scaled_stds):.4f}\")\n",
    "    print(f\"  Predicted Max - Mean: {np.mean(all_pred_maxes):.4f}, Std: {np.std(all_pred_maxes):.4f}\")\n",
    "    print(f\"  Target: Scaled_Std ≈ 1.0, Predicted_Max ≈ {theoretical_max:.3f}\")\n",
    "\n",
    "    # 실제 검증 (첫 번째 레이어의 첫 번째 헤드)\n",
    "    if 0 in results and 0 in results[0]:\n",
    "        first_head = results[0][0]\n",
    "        if 'empirical_max_mean' in first_head:\n",
    "            emp_max = first_head['empirical_max_mean']\n",
    "            pred_max = first_head['predicted_max']\n",
    "            print(f\"\\nVERIFICATION (Layer 0, Head 0):\")\n",
    "            print(f\"  Empirical max mean: {emp_max:.4f}\")\n",
    "            print(f\"  Predicted max: {pred_max:.4f}\")\n",
    "            print(f\"  Ratio (emp/pred): {emp_max/pred_max:.4f}\")\n",
    "\n",
    "    # 문제 있는 레이어/헤드 식별\n",
    "    problematic = []\n",
    "    for layer_idx, layer_data in results.items():\n",
    "        for head_idx, head_data in layer_data.items():\n",
    "            if head_data['scaled_std'] > 1.5:\n",
    "                problematic.append((layer_idx, head_idx, head_data['scaled_std']))\n",
    "\n",
    "    if problematic:\n",
    "        print(f\"\\n⚠️  PROBLEMATIC HEADS (Scaled_Std > 1.5):\")\n",
    "        for layer_idx, head_idx, scaled_std in problematic:\n",
    "            print(f\"  Layer {layer_idx}, Head {head_idx}: {scaled_std:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ All heads have reasonable attention distributions!\")"
   ],
   "id": "34b74118d931d793"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results = analyze_qk_distribution(model, model_config, num_samples=1000)",
   "id": "5ff97cda780378ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_distribution_heatmap(results, config):\n",
    "    \"\"\"레이어/헤드별 분포를 히트맵으로 시각화\"\"\"\n",
    "\n",
    "    scaled_stds = np.zeros((config.n_layers, config.n_heads))\n",
    "    pred_maxes = np.zeros((config.n_layers, config.n_heads))\n",
    "\n",
    "    for layer_idx, layer_data in results.items():\n",
    "        for head_idx, head_data in layer_data.items():\n",
    "            scaled_stds[layer_idx, head_idx] = head_data['scaled_std']\n",
    "            pred_maxes[layer_idx, head_idx] = head_data['predicted_max']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Scaled Standard Deviation\n",
    "    im1 = ax1.imshow(scaled_stds, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax1.set_title('Scaled Standard Deviation (QK^T/√D)')\n",
    "    ax1.set_xlabel('Head Index')\n",
    "    ax1.set_ylabel('Layer Index')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "    # Predicted Maximum\n",
    "    im2 = ax2.imshow(pred_maxes, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax2.set_title('Predicted Maximum (QK^T/√D)')\n",
    "    ax2.set_xlabel('Head Index')\n",
    "    ax2.set_ylabel('Layer Index')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "3ee99f7867499696"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_distribution_heatmap(results, model.config)",
   "id": "bbf2ded8f6115b3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
