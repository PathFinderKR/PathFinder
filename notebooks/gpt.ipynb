{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "eba822f51503a573"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-29T08:51:41.857133Z",
     "start_time": "2025-05-29T08:51:39.274816Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional, Tuple, Type, Literal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import wandb"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "6d9c4037c51cdb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:51:42.029824Z",
     "start_time": "2025-05-29T08:51:42.019013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class TokenizerConfig:\n",
    "    vocab_path: str = \"char_vocab.json\"\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = -1\n",
    "    max_seq_len: int = 256\n",
    "    d_embed: int = 128\n",
    "    n_layers: int = 4\n",
    "    norm_eps: float = 1e-5\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Attention\n",
    "    attn_type: Literal[\"mha\", \"gqa\", \"mla\"] = \"mha\"\n",
    "    n_heads: int = 4\n",
    "    d_head: int = d_embed // n_heads\n",
    "    attn_bias: bool = False\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    d_latent: Optional[int] = None\n",
    "    ## Mixture of Attention Heads\n",
    "    moh: bool = False\n",
    "    n_activated_heads: Optional[int] = None\n",
    "    n_shared_heads: Optional[int] = None\n",
    "\n",
    "    # FeedForward\n",
    "    d_ff: int = d_embed * 4\n",
    "    mlp_bias: bool = False\n",
    "    activation: Type[nn.Module] = nn.GELU\n",
    "    d_ff_multiplier: Optional[float] = None\n",
    "    d_ff_multiple_of: int = 256\n",
    "    ## Mixture of Experts\n",
    "    moe: bool = True\n",
    "    n_experts: Optional[int] = 4\n",
    "    n_activated_experts: Optional[int] = 1\n",
    "    n_shared_experts: Optional[int] = None\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    local_dir: str = f\"datasets/Shakespeare/shakespeare.txt\"\n",
    "    val_size: float = 0.05\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    debug: bool = False\n",
    "    wandb_project: str = \"nanoGPT\"\n",
    "    run_name = f\"nanoGPT-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    output_dir: str = f\"checkpoints/nanoGPT\"\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # Training\n",
    "    per_device_train_batch_size: int = 512\n",
    "    per_device_eval_batch_size: int = 1024\n",
    "    num_train_epochs: int = 1\n",
    "    learning_rate: float = 2e-3\n",
    "    weight_decay: float = 0.1\n",
    "    optim: torch.optim.Optimizer = torch.optim.AdamW\n",
    "    betas: tuple[float, float] = (0.9, 0.95)\n",
    "    eps: float = 1e-8\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    gradient_accumulation_steps: int = 1024 // per_device_train_batch_size\n",
    "    eval_steps: int = 100\n",
    "    seed: int = 101\n",
    "    ## Precision\n",
    "    mixed_precision: bool = True\n",
    "    matmul_precision: Literal[\"highest\", \"high\", \"medium\"] = \"high\"\n",
    "\n",
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    max_new_tokens: int = 200\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "\n",
    "tokenizer_config = TokenizerConfig()\n",
    "model_config = ModelConfig()\n",
    "dataset_config = DatasetConfig()\n",
    "train_config = TrainConfig()\n",
    "generation_config = GenerationConfig()"
   ],
   "id": "be57ba8fc9aedcf6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weights & Biases",
   "id": "5ed87542e4fab7cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:51:42.895166Z",
     "start_time": "2025-05-29T08:51:42.061335Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))",
   "id": "604dc457c3aca161",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "d2c2b27f0a363231"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "37c47b3c933f39c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:51:42.984757Z",
     "start_time": "2025-05-29T08:51:42.970605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "set_seed(train_config.seed)"
   ],
   "id": "43f42a50ba95e818",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "dab21e872039a939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:51:43.065602Z",
     "start_time": "2025-05-29T08:51:43.061481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores"
   ],
   "id": "baef08c6ac2c7d4f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "1c5701db44321787"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:53:49.401700Z",
     "start_time": "2025-05-29T08:53:49.388946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_text(file_path: str, encoding: str = 'utf-8') -> str:\n",
    "    \"\"\"\n",
    "    Load and read text data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        encoding (str, optional): File encoding. Defaults to 'utf-8'.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the text file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(f\"Loaded text data from {file_path} (length: {len(text)} characters).\")\n",
    "    return text\n",
    "\n",
    "shakespeare_text = load_text(f\"../{dataset_config.local_dir}\")"
   ],
   "id": "d733f8aed190ecca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from ../datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.750603Z",
     "start_time": "2025-05-29T04:51:40.812524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(shakespeare_text[:1000])"
   ],
   "id": "9d577742c3f4cd56",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "395612ec5147789b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.751698Z",
     "start_time": "2025-05-29T04:51:40.870049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, vocab: Optional[Dict[str, int]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the character-level tokenizer.\n",
    "\n",
    "        Args:\n",
    "            vocab (dict, optional): A pre-defined vocabulary mapping. If None, it will be built from data.\n",
    "        \"\"\"\n",
    "        if vocab is not None:\n",
    "            self.char2idx = vocab\n",
    "            self.idx2char = {idx: char for char, idx in vocab.items()}\n",
    "            self.vocab_size = len(vocab)\n",
    "        else:\n",
    "            self.char2idx: Dict[str, int] = {}\n",
    "            self.idx2char: Dict[int, str] = {}\n",
    "            self.vocab_size: int = 0\n",
    "\n",
    "    def build_vocab(self, text: str):\n",
    "        \"\"\"\n",
    "        Build vocabulary from the provided text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text data to build the vocabulary from.\n",
    "        \"\"\"\n",
    "        unique_chars = sorted(set(text))\n",
    "        print(f\"Unique characters: {len(unique_chars)}\")\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
    "        self.vocab_size = len(self.char2idx)\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode a string into a tensor of integer token IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The encoded tensor.\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "        for char in text:\n",
    "            if char in self.char2idx:\n",
    "                ids.append(self.char2idx[char])\n",
    "            else:\n",
    "                ids.append(self.char2idx[\"?\"])\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Decode a tensor of integer token IDs into a string.\n",
    "\n",
    "        Args:\n",
    "            tokens (torch.Tensor): The tensor of token IDs.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        chars = []\n",
    "        for idx in tokens:\n",
    "            if idx in self.idx2char:\n",
    "                chars.append(self.idx2char[idx])\n",
    "            else:\n",
    "                chars.append(\"?\")\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def save_vocab(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Save the vocabulary to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to save the vocabulary file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.char2idx, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Vocabulary saved to {file_path}.\")\n",
    "\n",
    "    def load_vocab(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Load the vocabulary from a JSON file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the vocabulary file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.char2idx = json.load(f)\n",
    "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
    "        self.vocab_size = len(self.char2idx)\n",
    "        print(f\"Vocabulary loaded from {file_path}.\")"
   ],
   "id": "9cc83726df2fc865",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.752034Z",
     "start_time": "2025-05-29T04:51:40.929134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "char_tokenizer.save_vocab(file_path=tokenizer_config.vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "82f100c72b1ff4e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 65\n",
      "Vocabulary saved to char_vocab.json.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.756708Z",
     "start_time": "2025-05-29T04:51:40.991250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(f\"Vocabulary size: {char_tokenizer.vocab_size}\")\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "33ca255715696a09",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "56e97a156915cf36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.757709Z",
     "start_time": "2025-05-29T04:51:41.049155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_text(text: str, val_size: float) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Split text into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        text (str): The data to split.\n",
    "        val_size (float): Size of the validation set.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: Training and validation data.\n",
    "    \"\"\"\n",
    "    if val_size < 0 or val_size >= 1:\n",
    "        raise ValueError(f\"Invalid validation size: {val_size}\")\n",
    "\n",
    "    split_idx = int(len(text) * (1 - val_size))\n",
    "    return text[:split_idx], text[split_idx:]\n",
    "\n",
    "train_text, val_text = split_text(shakespeare_text, val_size=dataset_config.val_size)"
   ],
   "id": "b4bce7cbc68e8431",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.757904Z",
     "start_time": "2025-05-29T04:51:41.109379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=train_config.num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=train_config.num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ],
   "id": "9365bcc860bc94c0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.758051Z",
     "start_time": "2025-05-29T04:51:41.261290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][:5]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][:5]}\")"
   ],
   "id": "48cad8c822157759",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d70bd7cb9c06609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multi Head Self-Attention",
   "id": "78bf5cbc9a92270"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.758188Z",
     "start_time": "2025-05-29T04:51:41.318605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        assert config.d_embed % config.n_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.config = config\n",
    "        self.qkv_proj = nn.Linear(config.d_embed, 3 * config.d_embed, bias=config.attn_bias)\n",
    "        self.out_proj = nn.Linear(config.d_embed, config.d_embed, bias=config.attn_bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.flash = hasattr(F, \"scaled_dot_product_attention\")\n",
    "        if not self.flash:\n",
    "            print(\"Flash attention not available, using standard implementation.\")\n",
    "            self.scale = config.d_head ** -0.5\n",
    "            self.attn_dropout = nn.Dropout(config.dropout)\n",
    "            self.register_buffer(\n",
    "                \"mask\",\n",
    "                torch.tril(torch.ones(config.max_seq_len, config.max_seq_len)).view(1, 1, config.max_seq_len, config.max_seq_len)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Linear projection\n",
    "        q, k, v = self.qkv_proj(x).split(self.config.d_embed, dim=2)  # [batch_size, seq_len, d_embed]\n",
    "        q = q.view(batch_size, seq_len, self.config.n_heads, self.config.d_head).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_head]\n",
    "        k = k.view(batch_size, seq_len, self.config.n_heads, self.config.d_head).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_head]\n",
    "        v = v.view(batch_size, seq_len, self.config.n_heads, self.config.d_head).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_head]\n",
    "\n",
    "        # Casual self-attention\n",
    "        if self.flash:\n",
    "            attn = F.scaled_dot_product_attention(q, k, v, dropout_p=self.config.dropout if self.training else 0.0, is_causal=True)  # [batch_size, n_heads, seq_len, d_head]\n",
    "        else:\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale  # [batch_size, n_heads, seq_len, seq_len]\n",
    "            attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            attn = attn @ v  # [batch_size, n_heads, seq_len, d_head]\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, seq_len, self.config.d_embed)  # [batch_size, seq_len, d_embed]\n",
    "\n",
    "        # Output projection\n",
    "        attn = self.out_proj(attn)  # [batch_size, seq_len, d_embed]\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        return attn"
   ],
   "id": "6c99370e0415ee29",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feed Forward",
   "id": "fe29a70a5cdb64cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.758330Z",
     "start_time": "2025-05-29T04:51:41.379843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_embed, config.d_ff, bias=config.mlp_bias)\n",
    "        self.activation = config.activation()\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_embed, bias=config.mlp_bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # [batch_size, seq_len, d_ff]\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)  # [batch_size, seq_len, d_embed]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "id": "91561c42bb05fa07",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mixture of Experts",
   "id": "dc90e3c2787edbcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.758458Z",
     "start_time": "2025-05-29T05:18:47.859058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Expert module\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "4c2bff97b386df7f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.758600Z",
     "start_time": "2025-05-29T05:33:38.983299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, d_embed = 32, n_experts = 4, top_k= 4):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.gate = nn.Linear(d_embed, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, n_experts]\n",
    "        gating_probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, indices = torch.topk(gating_probs, self.top_k, dim=-1)  # [batch_size, seq_len, top_k], [batch_size, seq_len, top_k]\n",
    "        print(f\"Top-k probabilities shape: {top_probs.shape}, Indices shape: {indices.shape}\")\n",
    "        indices = indices.unsqueeze(-1).expand(-1, -1, -1, x.size(-1))  # [batch_size, seq_len, top_k, d_embed]\n",
    "        gating_probs = top_probs.unsqueeze(-1).expand(-1, -1, -1, x.size(-1))  # [batch_size, seq_len, top_k, d_embed]\n",
    "        return gating_probs, indices\n",
    "\n",
    "sample_x = torch.randn(batch, seq_len, embed)\n",
    "print(f\"Sample input shape: {sample_x.shape}\")\n",
    "\n",
    "router = TopkRouter(d_embed=32, n_experts=4, top_k=4)\n",
    "gating_output, indices = router(sample_x)"
   ],
   "id": "168654ee53c5bdc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([1, 4, 32])\n",
      "Top-k probabilities shape: torch.Size([1, 4, 4]), Indices shape: torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.758781Z",
     "start_time": "2025-05-29T05:24:03.341466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, d_embed = 32, n_experts = 4, top_k= 4):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.gate = nn.Linear(d_embed, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, n_experts]\n",
    "        gating_probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, indices = torch.topk(gating_probs, self.top_k, dim=-1)  # [batch_size, seq_len, top_k], [batch_size, seq_len, top_k]\n",
    "        print(f\"Top-k probabilities shape: {top_probs.shape}, Indices shape: {indices.shape}\")\n",
    "        indices = indices.unsqueeze(-1).expand(-1, -1, -1, x.size(-1))  # [batch_size, seq_len, top_k, d_embed]\n",
    "        gating_probs = top_probs.unsqueeze(-1).expand(-1, -1, -1, x.size(-1))  # [batch_size, seq_len, top_k, d_embed]\n",
    "        return gating_probs, indices\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = TopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "        print(final_output)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "sample_x = torch.randn(batch, seq_len, embed)\n",
    "print(f\"Sample input shape: {sample_x.shape}\")"
   ],
   "id": "b794c214c8e8c632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([1, 4, 32])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.758919Z",
     "start_time": "2025-05-29T05:24:05.260695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_k_gate = TopkRouter(n_embed=embed, num_experts=experts, top_k=activated)\n",
    "gating_output, indices = top_k_gate(sample_x)\n",
    "\n",
    "sparse_moe = SparseMoE(n_embed=embed, num_experts=experts, top_k=activated)\n",
    "final_output = sparse_moe(mh_output)\n",
    "print(\"Shape of the final output:\", final_output.shape)"
   ],
   "id": "eb29e70dc6c56365",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_logits shape: torch.Size([1, 4, 4]), indices shape: torch.Size([1, 4, 1])\n",
      "sparse_logits shape: torch.Size([2, 4, 4]), indices shape: torch.Size([2, 4, 1])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "Shape of the final output: torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.759068Z",
     "start_time": "2025-05-29T04:51:41.434271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_embed, config.d_ff, bias=config.mlp_bias)\n",
    "        self.activation = config.activation()\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_embed, bias=config.mlp_bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # [batch_size, seq_len, d_ff]\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)  # [batch_size, seq_len, d_embed]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        self.gate = nn.Linear(config.d_embed, config.n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, n_experts]\n",
    "        gating_probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, indices = torch.topk(gating_probs, self.top_k, dim=-1)  # [batch_size, seq_len, top_k], [batch_size, seq_len, top_k]\n",
    "        indices = indices.unsqueeze(-1).expand(-1, -1, -1, x.size(-1))  # [batch_size, seq_len, top_k, d_embed]\n",
    "        gating_probs = top_probs.unsqueeze(-1).expand(-1, -1, -1, x.size(-1))  # [batch_size, seq_len, top_k, d_embed]\n",
    "        return gating_probs, indices\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.router = Router(config)\n",
    "        self.experts = nn.ModuleList([Expert(config) for _ in range(config.n_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_probs, indices = self.router(x)\n",
    "        batch, seq_len, _ = x.size()\n",
    "\n",
    "        final = torch.zeros_like(x)\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_probs = gating_probs.view(-1, gating_probs.size(-1))\n",
    "        flat_idx = indices.view(-1, indices.size(-1))\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = (flat_idx == i).any(dim=-1)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            expert_in = flat_x[mask]\n",
    "            expert_out = expert(expert_in)\n",
    "            scores = flat_probs[mask, i].unsqueeze(1)\n",
    "            weighted = expert_out * scores\n",
    "            final.view(-1, final.size(-1))[mask] += weighted\n",
    "\n",
    "        return final"
   ],
   "id": "fd18c423f0ac9a8d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block",
   "id": "cecf0a41442bbc19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.759194Z",
     "start_time": "2025-05-29T04:51:41.495648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.d_embed, eps=config.norm_eps)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.norm2 = nn.LayerNorm(config.d_embed, eps=config.norm_eps)\n",
    "        if config.moe:\n",
    "            self.mlp = MoE(config)\n",
    "        else:\n",
    "            self.mlp = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ],
   "id": "e4ca7c7ef5da3397",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GPT",
   "id": "b4dd6274ace2d58c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.759333Z",
     "start_time": "2025-05-29T04:51:41.549842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_embed)\n",
    "        self.positional_encoding = nn.Embedding(config.max_seq_len, config.d_embed)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layers)])\n",
    "        self.norm = nn.LayerNorm(config.d_embed, eps=config.norm_eps)\n",
    "        self.lm_head = nn.Linear(config.d_embed, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def num_params(self):\n",
    "        unique = {p.data_ptr(): p for p in self.parameters()}\n",
    "        return sum(p.numel() for p in unique.values())\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        batch_size, seq_len = idx.size()\n",
    "\n",
    "        # Embedding\n",
    "        tok_embed = self.embedding(idx)  # [batch_size, seq_len, d_embed]\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device=device)  # [seq_len]\n",
    "        pos_embed = self.positional_encoding(pos)  # [seq_len, d_embed]\n",
    "        x = tok_embed + pos_embed  # [batch_size, seq_len, d_embed]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # [batch_size, seq_len, d_embed]\n",
    "\n",
    "        # Final normalization and linear layer\n",
    "        x = self.norm(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)  # [batch_size, seq_len, vocab_size]\n",
    "            logits = logits.view(-1, self.config.vocab_size)  # [batch_size * seq_len, vocab_size]\n",
    "            targets = targets.view(-1)  # [batch_size * seq_len]\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])  # [batch_size, 1, vocab_size]\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, idx, tokenizer, max_new_tokens: int, temperature: float = 1.0, top_k: int = 50):\n",
    "        if not (temperature > 0):\n",
    "            raise ValueError(\"temperature must be positive\")\n",
    "        self.eval()\n",
    "\n",
    "        # Generation loop\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Truncate if necessary\n",
    "            idx_cond = idx if idx.size(1) <= self.config.max_seq_len else idx[:, -self.config.max_seq_len:]\n",
    "            logits, _ = self(idx_cond)  # [batch_size, 1, vocab_size]\n",
    "\n",
    "            # Apply temperature and top-k filtering\n",
    "            logits = logits[:, -1, :] / temperature  # [batch_size, vocab_size]\n",
    "            if top_k is not None:\n",
    "                k_logits, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < k_logits[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)  # [batch_size, 1]\n",
    "            # Concatenate the next token to the input\n",
    "            idx = torch.cat((idx, next_idx), dim=1)  # [batch_size, seq_len + 1]\n",
    "\n",
    "            # Decode and print the next token\n",
    "            text = tokenizer.decode([next_idx[0].item()])\n",
    "            print(text, end='', flush=True)"
   ],
   "id": "252da1075abc399c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.759480Z",
     "start_time": "2025-05-29T04:51:41.605031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "#model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_params() / 1e6:.2f}M\")"
   ],
   "id": "afd4c978bd93f6c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (embedding): Embedding(65, 128)\n",
      "  (positional_encoding): Embedding(256, 128)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiHeadAttention(\n",
      "        (qkv_proj): Linear(in_features=128, out_features=384, bias=False)\n",
      "        (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MoE(\n",
      "        (router): Router(\n",
      "          (gate): Linear(in_features=128, out_features=4, bias=True)\n",
      "          (noise_linear): Linear(in_features=128, out_features=4, bias=True)\n",
      "        )\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x Expert(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=128, out_features=65, bias=False)\n",
      ")\n",
      "Number of parameters: 2.41M\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def testTemplate(customFunc, params, test_key):\n",
    "    start = time.time()\n",
    "    N, d, B, H = params\n",
    "    #compute pytorch unfused softmax\n",
    "    Q, K, V = createQKVSimple(N,d,B,H)\n",
    "    QKV = badSoftmax(Q,K,V)\n",
    "    end = time.time()\n",
    "    pytorch_time = end - start\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            #compute with Naive Unfused\n",
    "            start = time.time()\n",
    "            QKS1 = customFunc()\n",
    "            end = time.time()\n",
    "            manual_time = end - start\n",
    "\n",
    "    assert torch.allclose(QKV,QKS1, atol=1e-4), correctness_error_message\n",
    "    print(\"manual attention == pytorch attention\",torch.allclose(QKV,QKS1, atol=1e-4))\n",
    "    #print(\"Pytorch Execution Time:\", pytorch_time, \"\\n\")\n",
    "    print(\"Manual Execution Time: \", manual_time, \"\\n\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n",
    "    r = prof.key_averages()\n",
    "    for rr in r:\n",
    "        if rr.key == test_key:\n",
    "            key, cpu_time, mem_usage = rr.key, rr.cpu_time, rr.cpu_memory_usage\n",
    "            print (test_key+ \" statistics\")\n",
    "            print(\"cpu time: \", str(cpu_time / 1000.0) + \"ms\")\n",
    "            print(\"mem usage: \", mem_usage, \"bytes\")"
   ],
   "id": "9678b736e3a5844b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def part1Test(N, d, B, H):\n",
    "    print(\"Running Part 1 Test: Naive Unfused Attention\\n\")\n",
    "    Q,K,V = createQKVSimple(N,d,B,H)\n",
    "    attentionModuleStudent = CustomAttention(Q,K,V, B, H, N, d)\n",
    "    attentionModuleReference = CustomAttention(Q,K,V, B, H, N, d, True)\n",
    "    params = (N, d, B, H)\n",
    "    print(\"-----RUNNING REFERENCE IMPLEMENTATION-----\\n\")\n",
    "    testTemplate(attentionModuleStudent.myUnfusedAttention, params, \"REFERENCE - NAIVE ATTENTION\")\n",
    "    time.sleep(3)\n",
    "    print(\"-----RUNNING STUDENT IMPLEMENTATION-----\\n\")\n",
    "    testTemplate(attentionModuleReference.myUnfusedAttention, params, \"STUDENT - NAIVE ATTENTION\")\n"
   ],
   "id": "9881f52efc0a817b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "1d1d9a614384a0e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.761192Z",
     "start_time": "2025-05-29T04:51:41.821162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    raise ValueError(\"Debug mode is enabled. Stopping execution for debugging.\")"
   ],
   "id": "62a852f23434de2c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.761380Z",
     "start_time": "2025-05-29T04:51:41.878899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_config: TrainConfig,\n",
    "            model: nn.Module,\n",
    "            train_loader: DataLoader,\n",
    "            val_loader: DataLoader,\n",
    "            device: torch.device\n",
    "    ):\n",
    "        self.train_config = train_config\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "\n",
    "    def train(self):\n",
    "        wandb.init(\n",
    "            project=self.train_config.wandb_project,\n",
    "            name=self.train_config.run_name,\n",
    "            config=self.train_config.__dict__\n",
    "        )\n",
    "        wandb.watch(self.model, log=\"all\")\n",
    "\n",
    "        total_steps = (len(self.train_loader) * self.train_config.num_train_epochs // self.train_config.gradient_accumulation_steps)\n",
    "        warmup_steps = int(self.train_config.warmup_ratio * total_steps)\n",
    "\n",
    "        optimizer = self.train_config.optim(\n",
    "            self.model.parameters(),\n",
    "            lr=self.train_config.learning_rate,\n",
    "            weight_decay=self.train_config.weight_decay,\n",
    "            betas=self.train_config.betas,\n",
    "            eps=self.train_config.eps,\n",
    "            fused=True\n",
    "        )\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        progress_bar = tqdm(total=total_steps, desc=\"Training\")\n",
    "        step = 0\n",
    "\n",
    "        for epoch in range(self.train_config.num_train_epochs):\n",
    "            for batch_idx, batch in enumerate(self.train_loader):\n",
    "                self.model.train()\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                target_ids = batch[\"target_ids\"].to(self.device)\n",
    "\n",
    "                if self.train_config.mixed_precision:\n",
    "                    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                        outputs, loss = self.model(input_ids, target_ids)\n",
    "                else:\n",
    "                    outputs, loss = self.model(input_ids, target_ids)\n",
    "                loss = loss / self.train_config.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "\n",
    "                if (batch_idx + 1) % self.train_config.gradient_accumulation_steps == 0:\n",
    "                    grad_norm = clip_grad_norm_(self.model.parameters(), self.train_config.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    step += 1\n",
    "\n",
    "                    wandb.log({\n",
    "                        \"Train Loss\": loss.item() * self.train_config.gradient_accumulation_steps,\n",
    "                        \"Learning Rate\": scheduler.get_last_lr()[0],\n",
    "                        \"Grad Norm\": grad_norm,\n",
    "                        \"Epoch\": epoch + 1\n",
    "                    })\n",
    "                    progress_bar.set_postfix(\n",
    "                        loss=f\"{loss.item() * self.train_config.gradient_accumulation_steps:.4f}\",\n",
    "                        lr=f\"{scheduler.get_last_lr()[0]:.6f}\",\n",
    "                        grad_norm=f\"{grad_norm:.4f}\",\n",
    "                        epoch=epoch + 1\n",
    "                    )\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "                    if step % self.train_config.eval_steps == 0:\n",
    "                        self.validate()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.validate()  # Final validation\n",
    "        progress_bar.close()\n",
    "        wandb.finish()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for val_batch in self.val_loader:\n",
    "            val_input_ids = val_batch[\"input_ids\"].to(self.device)\n",
    "            val_target_ids = val_batch[\"target_ids\"].to(self.device)\n",
    "            if self.train_config.mixed_precision:\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    _, val_loss = self.model(val_input_ids, val_target_ids)\n",
    "            else:\n",
    "                _, val_loss = self.model(val_input_ids, val_target_ids)\n",
    "            total_val_loss += val_loss.item() * val_input_ids.size(0)\n",
    "            total_samples += val_input_ids.size(0)\n",
    "\n",
    "        wandb.log({\"Val Loss\": total_val_loss / total_samples})"
   ],
   "id": "87ed4448d92e66c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.761542Z",
     "start_time": "2025-05-29T04:51:41.937670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "        train_config=train_config,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "6ebc75004c7fa8c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/jwkim/projects/PathFinder/wandb/run-20250529_045141-iusl7li5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/iusl7li5' target=\"_blank\">nanoGPT-2025-05-29_04-51-39</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/iusl7li5' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/iusl7li5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 517/517 [03:43<00:00,  2.31it/s, epoch=1, grad_norm=0.0786, loss=1.9626, lr=0.000000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td></td></tr><tr><td>Grad Norm</td><td></td></tr><tr><td>Learning Rate</td><td></td></tr><tr><td>Train Loss</td><td></td></tr><tr><td>Val Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Grad Norm</td><td>0.07861</td></tr><tr><td>Learning Rate</td><td>0</td></tr><tr><td>Train Loss</td><td>1.96258</td></tr><tr><td>Val Loss</td><td>1.94662</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nanoGPT-2025-05-29_04-51-39</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/iusl7li5' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/iusl7li5</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250529_045141-iusl7li5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save",
   "id": "59b0d111404044f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.762088Z",
     "start_time": "2025-05-29T04:55:28.732074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not train_config.debug:\n",
    "    os.makedirs(train_config.output_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(train_config.output_dir, f\"{train_config.run_name}.pt\"))\n",
    "    print(f\"Model saved to {os.path.join(train_config.output_dir, f'{train_config.run_name}.pt')}\")"
   ],
   "id": "a5837c93ad4b3225",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/nanoGPT/nanoGPT-2025-05-29_04-51-39.pt\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "31f57a38b05be19b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:49:58.762243Z",
     "start_time": "2025-05-29T04:57:19.866338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "sample_prompt = \"To be, or not to be, that is the question\"\n",
    "print(sample_prompt)\n",
    "input_ids = char_tokenizer.encode(sample_prompt).to(device).unsqueeze(0)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\" Model Response:\")\n",
    "model.generate(\n",
    "    input_ids,\n",
    "    tokenizer=char_tokenizer,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k\n",
    ")\n",
    "print()\n",
    "print(\"=\" * 50)"
   ],
   "id": "c78bf4aa4a5bec27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User prompt: \n",
      "To be, or not to be, that is the question\n",
      "--------------------------------------------------\n",
      " Model Response:\n",
      "\n",
      "Best but garghered sway fard natim; it figh theall,\n",
      "Comsuck; murrd it by you friestlen?\n",
      "\n",
      "FROMEO:\n",
      "I him nowst shall havee this sprit:--\n",
      "\n",
      "ESers Vond:\n",
      "And Rand his then tin mare be my\n",
      "Murage tolly a gau\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
