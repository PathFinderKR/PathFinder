{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "eba822f51503a573"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from src.utils import set_seed, load_text, split_text\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.tokenizer import CharTokenizer\n",
    "from models.GPT2 import GPT\n",
    "from src.train import Trainer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "6d9c4037c51cdb88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    max_seq_len=256,\n",
    "    d_embed=128,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    d_head=32,\n",
    "    d_ff=512,\n",
    "    router_free=True,\n",
    "    n_experts=4,\n",
    "    n_activated_experts=1\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    model_name=\"nanoGPT\",\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=512,\n",
    "    gradient_accumulation_steps=512 // 256,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-3,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=False,\n",
    "    matmul_precision=\"high\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=1000\n",
    ")"
   ],
   "id": "6ab70e4c2df8b424",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "604dc457c3aca161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "d2c2b27f0a363231"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "37c47b3c933f39c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "43f42a50ba95e818",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "dab21e872039a939"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores"
   ],
   "id": "baef08c6ac2c7d4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "1c5701db44321787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "shakespeare_text = load_text(\"datasets/Shakespeare/shakespeare.txt\")",
   "id": "d733f8aed190ecca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(shakespeare_text[:1000])"
   ],
   "id": "9d577742c3f4cd56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "395612ec5147789b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "82f100c72b1ff4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(f\"Vocabulary size: {char_tokenizer.vocab_size}\")\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "33ca255715696a09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "56e97a156915cf36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "b4bce7cbc68e8431",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "9365bcc860bc94c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][:5]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][:5]}\")"
   ],
   "id": "48cad8c822157759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "ab879cf42408dc15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_params() / 1e6:.2f}M\")\n",
    "print(f\"Number of active parameters: {model.num_active_params() / 1e6:.2f}M\")"
   ],
   "id": "afd4c978bd93f6c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speedometer",
   "id": "a23d13fbb910e997"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fbab8f36f063efa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "1d1d9a614384a0e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    train_config=train_config,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "6ebc75004c7fa8c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "27b33a10e48ab9c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save model locally\n",
    "output_dir = f\"checkpoints/{train_config.model_name}/{train_config.run_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(\n",
    "    output_dir,\n",
    "    safe_serialization=True\n",
    ")\n",
    "# Push to Hugging Face Hub\n",
    "model.push_to_hub(\n",
    "    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    private=True,\n",
    "    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    ")"
   ],
   "id": "a7bb305371c6ca51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT(model_config)\n",
    "#  model = model.from_pretrained(output_dir).to(device)"
   ],
   "id": "2cfe471be088cf5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "31f57a38b05be19b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "cc2a13ad05736a95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"ðŸ¤– Model Response:\")\n",
    "print(response)"
   ],
   "id": "c78bf4aa4a5bec27",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
