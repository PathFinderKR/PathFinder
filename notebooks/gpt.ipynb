{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "9c5839c9a3cf64fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:38.853950Z",
     "start_time": "2025-10-27T03:53:35.974552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import wandb\n",
    "from src.utils import set_seed, load_text, split_text\n",
    "from src.config import ModelConfig, TrainConfig, GenerationConfig\n",
    "from src.train import Trainer\n",
    "from tokenizer.tokenizer import CharTokenizer\n",
    "from models.GPT import GPT"
   ],
   "id": "46f01dcbb360429a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:38.865746Z",
     "start_time": "2025-10-27T03:53:38.861328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + \"/..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")"
   ],
   "id": "8d59d28379b9627c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/pathfinder/projects/PathFinder\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "e1db9708d016dbff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:38.918987Z",
     "start_time": "2025-10-27T03:53:38.915126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config = ModelConfig(\n",
    "    vocab_size=-1,\n",
    "    max_seq_len=128,\n",
    "    d_embed=256,\n",
    "    n_layers=4,\n",
    "    attn_type=\"MLA\",\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    rank=32,\n",
    "    attn_bias=False,\n",
    "    d_ff=1024,\n",
    "    mlp_bias=False,\n",
    "    flash=True,\n",
    "    flash_decode=False\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    debug=False,\n",
    "    wandb_project=\"nanoGPT\",\n",
    "    model_name=\"nanoGPT\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    gradient_accumulation_steps=512 // 512,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    attn_decay=0.5,\n",
    "    eval_steps=100,\n",
    "    mixed_precision=True,\n",
    "    matmul_precision=\"medium\",\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=1.0,\n",
    "    top_k=50\n",
    ")"
   ],
   "id": "40c1b49185fe925c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:40.897729Z",
     "start_time": "2025-10-27T03:53:38.923525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "e1e0445647cd6c46",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/pathfinder/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "c58f70d689c96eb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "f274b536c84b9fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.020906Z",
     "start_time": "2025-10-27T03:53:41.014567Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(train_config.seed)",
   "id": "589c9877d81b1ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "41576077c1495528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.230322Z",
     "start_time": "2025-10-27T03:53:41.025232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(device)}\")\n",
    "torch.set_float32_matmul_precision(train_config.matmul_precision)  # Tensor Cores\n",
    "print(f\"MatMul Precision: {train_config.matmul_precision}\")"
   ],
   "id": "4480ebea1f838033",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4080 SUPER\n",
      "MatMul Precision: medium\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "3ce6a33cb70f30bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.246989Z",
     "start_time": "2025-10-27T03:53:41.235247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/shakespeare.txt\")\n",
    "shakespeare_text = load_text(dataset_path)"
   ],
   "id": "c8b140fbb518dd26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /home/pathfinder/projects/PathFinder/datasets/Shakespeare/shakespeare.txt (length: 1115394 characters).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.298128Z",
     "start_time": "2025-10-27T03:53:41.295147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    subset_shakespeare_text = shakespeare_text[:10000]\n",
    "    print(subset_shakespeare_text)\n",
    "    shakespeare_text = subset_shakespeare_text"
   ],
   "id": "2d80cd5188573c48",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "8d61430be1c7c5eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.315016Z",
     "start_time": "2025-10-27T03:53:41.302677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_tokenizer = CharTokenizer()\n",
    "char_tokenizer.build_vocab(text=shakespeare_text)\n",
    "vocab_path = os.path.join(PROJECT_ROOT, \"datasets/Shakespeare/vocab.json\")\n",
    "char_tokenizer.save_vocab(vocab_path)\n",
    "model_config.vocab_size = char_tokenizer.vocab_size"
   ],
   "id": "432c5fbe09cb4f09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 69\n",
      "Vocabulary saved to /home/pathfinder/projects/PathFinder/datasets/Shakespeare/vocab.json.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.377930Z",
     "start_time": "2025-10-27T03:53:41.375136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    print(\"Vocabulary:\", char_tokenizer.char2idx)"
   ],
   "id": "11c6e1486b065782",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "ca72c158757f0704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.385423Z",
     "start_time": "2025-10-27T03:53:41.381960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text, val_text = split_text(shakespeare_text, val_size=0.1)\n",
    "print(f\"Training text length: {len(train_text)} characters\")\n",
    "print(f\"Validation text length: {len(val_text)} characters\")"
   ],
   "id": "9c950bd3950a08db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 1003854 characters\n",
      "Validation text length: 111540 characters\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.571596Z",
     "start_time": "2025-10-27T03:53:41.435400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, max_seq_len: int):\n",
    "        self.encoded = tokenizer.encode(text)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_ids = self.encoded[idx:idx + self.max_seq_len]\n",
    "        target_ids = self.encoded[idx + 1:idx + self.max_seq_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    target_ids = torch.stack([item[1] for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        #\"attention_mask\": attention_mask,\n",
    "        \"target_ids\": target_ids\n",
    "    }\n",
    "\n",
    "train_dataset = TextDataset(train_text, char_tokenizer, model_config.max_seq_len)\n",
    "val_dataset = TextDataset(val_text, char_tokenizer, model_config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_config.per_device_eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "620a9b685b7bca7c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:41.580581Z",
     "start_time": "2025-10-27T03:53:41.577451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Sample input IDs: {sample_batch['input_ids'][0]}\")\n",
    "    print(f\"Sample target IDs: {sample_batch['target_ids'][0]}\")"
   ],
   "id": "a6a326932c2034e1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d680f9ad3de3eefb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:53:42.036156Z",
     "start_time": "2025-10-27T03:53:41.585052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = GPT(model_config).to(device)\n",
    "#model = torch.compile(model)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.get_num_params() / 1e6:.2f}M\")"
   ],
   "id": "2a82b103029ecc60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (token_embedding): Embedding(69, 256)\n",
      "  (positional_encoding): Embedding(128, 256)\n",
      "  (dropout): Dropout(p=0.01, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x Block(\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiHeadAttention(\n",
      "        (Wq): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (Wkv_down): Linear(in_features=256, out_features=32, bias=False)\n",
      "        (Wk_up): Linear(in_features=32, out_features=256, bias=False)\n",
      "        (Wv_up): Linear(in_features=32, out_features=256, bias=False)\n",
      "        (out_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): FeedForward(\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=False)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=256, out_features=69, bias=False)\n",
      ")\n",
      "Number of parameters: 2.77M\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "d016b0c6fe5f3be0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:55:55.572636Z",
     "start_time": "2025-10-27T03:53:42.040828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    master_process=True\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "57885df1d97b3537",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/pathfinder/projects/PathFinder/wandb/run-20251027_125342-beytuao4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/beytuao4' target=\"_blank\">nanoGPT-2025-10-27_12-53-38</a></strong> to <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/beytuao4' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/beytuao4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 981/981 [02:11<00:00,  7.47it/s, epoch=1, grad_norm=0.3991, loss=2.0191, lr=0.000000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "a1e4ea39315bf38daec51f19a2dad698"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>▄▂▂▂▂▁▁▁▂▂▁▂▁▂█▂▂▂▅▂▂▂▂▂▃▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>Learning Rate</td><td>▁▁▂▃▆███▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>█▇▅▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▆▅▄▃▂▂▁▁▁</td></tr><tr><td>Val Perplexity</td><td>█▆▅▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Grad Norm</td><td>0.39908</td></tr><tr><td>Learning Rate</td><td>0</td></tr><tr><td>Train Loss</td><td>2.01915</td></tr><tr><td>Val Loss</td><td>2.07878</td></tr><tr><td>Val Perplexity</td><td>7.99468</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nanoGPT-2025-10-27_12-53-38</strong> at: <a href='https://wandb.ai/pathfinderkr/nanoGPT/runs/beytuao4' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT/runs/beytuao4</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/nanoGPT' target=\"_blank\">https://wandb.ai/pathfinderkr/nanoGPT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>/home/pathfinder/projects/PathFinder/wandb/run-20251027_125342-beytuao4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "5fb3b0ae98408329"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:55:55.640124Z",
     "start_time": "2025-10-27T03:55:55.636979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not train_config.debug:\n",
    "    pass\n",
    "    #output_dir = os.path.join(PROJECT_ROOT, \"checkpoints\", train_config.model_name, train_config.run_name)\n",
    "    #os.makedirs(output_dir, exist_ok=True)\n",
    "    #try:\n",
    "    #    model.save_pretrained(\n",
    "    #        output_dir,\n",
    "    #        safe_serialization=True\n",
    "    #    )\n",
    "    #    print(\"Model saved successfully\")\n",
    "    #xcept Exception as e:\n",
    "    #    print(f\"Error saving model: {e}\")\n",
    "    # Push to Hugging Face Hub\n",
    "    #model.push_to_hub(\n",
    "    #    repo_id=f\"PathFinderKR/{train_config.model_name}-{train_config.run_name}\",\n",
    "    #    private=True,\n",
    "    #    use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    #)\n",
    "    #print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{train_config.model_name}-{train_config.run_name}\")"
   ],
   "id": "aa7fc811417bd46",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:55:55.647024Z",
     "start_time": "2025-10-27T03:55:55.643864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To load the model later, you can use:\n",
    "# model = GPT.from_pretrained(output_dir).to(device)"
   ],
   "id": "189947e80319f23e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "754c9ac783ec7ce0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:56:00.908554Z",
     "start_time": "2025-10-27T03:55:55.650700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"To be, or not to be, that is the question\"\n",
    "input_ids = char_tokenizer.encode(user_prompt).unsqueeze(0).to(device)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=generation_config.max_new_tokens,\n",
    "    temperature=generation_config.temperature,\n",
    "    top_k=generation_config.top_k,\n",
    "    tokenizer=char_tokenizer\n",
    ")\n",
    "response = char_tokenizer.decode(output[0].squeeze().cpu().numpy())"
   ],
   "id": "51651a0e0120012d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g of\n",
      "cockss, you by wim, excoow you that throd.\n",
      "\n",
      "DUKE VING Pas! Ro.\n",
      "\n",
      "CARICETIUS ICKE, I \u001B[91mReset KV cache\u001B[0m\n",
      "thoth iner soad ie chind of we northe.\n",
      "\n",
      "CLONIUS:\n",
      "Iby, Mower borcrtict.\n",
      "\n",
      "NICILA:\n",
      "Thoh noms, the's deaver our flerok!\n",
      "O, say, thim\u001B[91mReset KV cache\u001B[0m\n",
      " oules chice viss bet,\n",
      "As yould in nol Dedoung mough to abean show.\n",
      "\n",
      "AUCININIUS:\n",
      "Nloman, and kidy knose abery mowsge to I inmor\n",
      "\u001B[91mReset KV cache\u001B[0m\n",
      "\n",
      "RENGHARET:\n",
      "I this; nol you, kirse purke hemecer.\n",
      "\n",
      "Acomenow'd lor thas I was not Ro pensusy and by shane\n",
      "Whim do bucl beal and n\u001B[91mReset KV cache\u001B[0m\n",
      "tm, wil\n",
      "Hef anvime bealp.\n",
      "If why, faingh, to bumet as in rardvepath's hay.\n",
      "\n",
      "MISth OMERK:\n",
      "If to eact will. Bay, lood Due\n",
      "Thath an\u001B[91mReset KV cache\u001B[0m\n",
      "\n",
      "We pace,\n",
      "That ook deer. Wil arvoow, was amyine I ware:\n",
      "I dove the'll, a the for us tobl, and tis and nom'd ouid.\n",
      "\n",
      "LONGERE VTRT:\u001B[91mReset KV cache\u001B[0m\n",
      "\n",
      "thy if putrcaked, who willl fir ves,\n",
      "Fillong blom, Ogor deary bert thate fee,\n",
      "truny.\n",
      "\n",
      "DUCOLUS:\n",
      "I wes well pacunfy prusio,\n",
      "My I \u001B[91mReset KV cache\u001B[0m\n",
      "the so bearth and iten.\n",
      "Filk! I mive'd I gooust to ordre appt,\n",
      "Whos ler "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_log'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m user_prompt = \u001B[33m\"\u001B[39m\u001B[33mTo be, or not to be, that is the question\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m input_ids = char_tokenizer.encode(user_prompt).unsqueeze(\u001B[32m0\u001B[39m).to(device)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m output = model.generate(\n\u001B[32m      4\u001B[39m     input_ids,\n\u001B[32m      5\u001B[39m     use_cache=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m      6\u001B[39m     max_new_tokens=generation_config.max_new_tokens,\n\u001B[32m      7\u001B[39m     temperature=generation_config.temperature,\n\u001B[32m      8\u001B[39m     top_k=generation_config.top_k,\n\u001B[32m      9\u001B[39m     tokenizer=char_tokenizer\n\u001B[32m     10\u001B[39m )\n\u001B[32m     11\u001B[39m response = char_tokenizer.decode(output[\u001B[32m0\u001B[39m].squeeze().cpu().numpy())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/PathFinder/models/GPT.py:393\u001B[39m, in \u001B[36mGPT.generate\u001B[39m\u001B[34m(self, input_ids, use_cache, max_new_tokens, temperature, top_k, tokenizer)\u001B[39m\n\u001B[32m    390\u001B[39m     input_ids = input_ids[:, -\u001B[38;5;28mself\u001B[39m.config.max_seq_len:]\n\u001B[32m    392\u001B[39m \u001B[38;5;66;03m# ---------- Forward pass ----------------------------------------------------------------------------------\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m logits, _, kv_cache = \u001B[38;5;28mself\u001B[39m(\n\u001B[32m    394\u001B[39m     input_ids,\n\u001B[32m    395\u001B[39m     kv_cache=kv_cache \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    396\u001B[39m )                                                                              \u001B[38;5;66;03m# [batch_size, 1, vocab_size]\u001B[39;00m\n\u001B[32m    397\u001B[39m logits = logits[:, -\u001B[32m1\u001B[39m, :]                                                         \u001B[38;5;66;03m# [batch_size, vocab_size]\u001B[39;00m\n\u001B[32m    399\u001B[39m \u001B[38;5;66;03m# ---------- Temperature -----------------------------------------------------------------------------------\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1879\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1876\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1878\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1879\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1880\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1881\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1882\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1883\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n\u001B[32m   1884\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m _global_forward_hooks.items():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1840\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1838\u001B[39m     hook_result = hook(\u001B[38;5;28mself\u001B[39m, args, kwargs, result)\n\u001B[32m   1839\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1840\u001B[39m     hook_result = hook(\u001B[38;5;28mself\u001B[39m, args, result)\n\u001B[32m   1842\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m hook_result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1843\u001B[39m     result = hook_result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-env/lib/python3.12/site-packages/wandb/integration/torch/wandb_torch.py:113\u001B[39m, in \u001B[36mTorchHistory.add_log_parameters_hook.<locals>.<lambda>\u001B[39m\u001B[34m(mod, inp, outp)\u001B[39m\n\u001B[32m    110\u001B[39m log_track_params = log_track_init(log_freq)\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    112\u001B[39m     hook = module.register_forward_hook(\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m         \u001B[38;5;28;01mlambda\u001B[39;00m mod, inp, outp: parameter_log_hook(\n\u001B[32m    114\u001B[39m             mod, inp, outp, log_track_params\n\u001B[32m    115\u001B[39m         )\n\u001B[32m    116\u001B[39m     )\n\u001B[32m    117\u001B[39m     \u001B[38;5;28mself\u001B[39m._hook_handles[\u001B[33m\"\u001B[39m\u001B[33mparameters/\u001B[39m\u001B[33m\"\u001B[39m + prefix] = hook\n\u001B[32m    118\u001B[39m     module._wandb_hook_names.append(\u001B[33m\"\u001B[39m\u001B[33mparameters/\u001B[39m\u001B[33m\"\u001B[39m + prefix)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-env/lib/python3.12/site-packages/wandb/integration/torch/wandb_torch.py:108\u001B[39m, in \u001B[36mTorchHistory.add_log_parameters_hook.<locals>.parameter_log_hook\u001B[39m\u001B[34m(module, input_, output, log_track)\u001B[39m\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    107\u001B[39m     data = parameter\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m \u001B[38;5;28mself\u001B[39m.log_tensor_stats(data.cpu(), \u001B[33m\"\u001B[39m\u001B[33mparameters/\u001B[39m\u001B[33m\"\u001B[39m + prefix + name)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-env/lib/python3.12/site-packages/wandb/integration/torch/wandb_torch.py:254\u001B[39m, in \u001B[36mTorchHistory.log_tensor_stats\u001B[39m\u001B[34m(self, tensor, name)\u001B[39m\n\u001B[32m    251\u001B[39m     tensor = torch.Tensor(tensor_np)\n\u001B[32m    252\u001B[39m     bins = torch.Tensor(bins_np)\n\u001B[32m--> \u001B[39m\u001B[32m254\u001B[39m wandb.run._log(\n\u001B[32m    255\u001B[39m     {name: wandb.Histogram(np_histogram=(tensor.tolist(), bins.tolist()))},\n\u001B[32m    256\u001B[39m     commit=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    257\u001B[39m )\n",
      "\u001B[31mAttributeError\u001B[39m: 'NoneType' object has no attribute '_log'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"User prompt: \")\n",
    "print(user_prompt)\n",
    "print(\"-\" * 50)\n",
    "print(\"🤖 Model Response:\")\n",
    "print(response)"
   ],
   "id": "6e66a6c36144f057",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Profiling",
   "id": "df64b918376b3b29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if train_config.debug:\n",
    "    input_ids = torch.randint(0, model_config.vocab_size, (1, model_config.max_seq_len), device=device)\n",
    "    with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            model(input_ids)\n",
    "    print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cuda_time_total\", row_limit=20))"
   ],
   "id": "acc60cff49785962",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
