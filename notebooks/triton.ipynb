{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "8d04948fa637aeb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:22:37.166436Z",
     "start_time": "2025-06-13T08:22:35.873104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pytest\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "from src.utils import set_seed"
   ],
   "id": "753f7bc56b7cc5b6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "58e2843c60278081"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:22:37.447994Z",
     "start_time": "2025-06-13T08:22:37.339208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "DEVICE = driver.active.get_current_device()\n",
    "properties = driver.active.utils.get_device_properties(DEVICE)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "\n",
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "def supports_host_descriptor():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n",
    "def is_blackwell():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 10\n",
    "\n",
    "print(\n",
    "    f\"Device: {torch.cuda.get_device_name(device)}\\n\"\n",
    "    f\"Number of SM: {NUM_SM}\\n\"\n",
    "    f\"Number of registers: {NUM_REGS}\\n\"\n",
    "    f\"Size of SMEM: {SIZE_SMEM}\\n\"\n",
    "    f\"Warp size: {WARP_SIZE}\\n\"\n",
    "    f\"Target: {target}\"\n",
    ")"
   ],
   "id": "646d5ba250f3c96b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA RTX A6000\n",
      "Number of SM: 84\n",
      "Number of registers: 65536\n",
      "Size of SMEM: 101376\n",
      "Warp size: 32\n",
      "Target: GPUTarget(backend='cuda', arch=86, warp_size=32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:22:37.481581Z",
     "start_time": "2025-06-13T08:22:37.476508Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(42)",
   "id": "125099b90ac2c670",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vector Addition",
   "id": "b748cec59066ebe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:22:37.543768Z",
     "start_time": "2025-06-13T08:22:37.539952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@triton.jit\n",
    "def add_kernel(\n",
    "        x_ptr,  # *Pointer* to first input vector.\n",
    "        y_ptr,  # *Pointer* to second input vector.\n",
    "        output_ptr,  # *Pointer* to output vector.\n",
    "        n_elements,  # Size of the vector.\n",
    "        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "        # NOTE: `constexpr` so it can be used as a shape value.\n",
    "        ):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ],
   "id": "8e85baec0dabbc0b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:22:38.332112Z",
     "start_time": "2025-06-13T08:22:38.324791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # We need to preallocate the output.\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE\n",
    "    n_elements = output.numel()\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
    "    # running asynchronously at this point.\n",
    "    return output"
   ],
   "id": "930da7fd21e3fe27",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:22:39.097181Z",
     "start_time": "2025-06-13T08:22:38.821148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "size = 98432\n",
    "x = torch.rand(size, device=DEVICE)\n",
    "y = torch.rand(size, device=DEVICE)\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
   ],
   "id": "2a3e32461ca6942",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m y = torch.rand(size, device=DEVICE)\n\u001B[32m      4\u001B[39m output_torch = x + y\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m output_triton = \u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(output_torch)\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(output_triton)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 4\u001B[39m, in \u001B[36madd\u001B[39m\u001B[34m(x, y)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34madd\u001B[39m(x: torch.Tensor, y: torch.Tensor):\n\u001B[32m      2\u001B[39m     \u001B[38;5;66;03m# We need to preallocate the output.\u001B[39;00m\n\u001B[32m      3\u001B[39m     output = torch.empty_like(x)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m x.device == DEVICE \u001B[38;5;129;01mand\u001B[39;00m y.device == DEVICE \u001B[38;5;129;01mand\u001B[39;00m output.device == DEVICE\n\u001B[32m      5\u001B[39m     n_elements = output.numel()\n\u001B[32m      6\u001B[39m     \u001B[38;5;66;03m# The SPMD launch grid denotes the number of kernel instances that run in parallel.\u001B[39;00m\n\u001B[32m      7\u001B[39m     \u001B[38;5;66;03m# It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\u001B[39;00m\n\u001B[32m      8\u001B[39m     \u001B[38;5;66;03m# In this case, we use a 1D grid where the size is the number of blocks:\u001B[39;00m\n",
      "\u001B[31mAssertionError\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Softmax",
   "id": "7bf45175d3205247"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Naive Softmax",
   "id": "4e81bb5b165cc667"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-13T08:16:59.336179Z",
     "start_time": "2025-06-13T08:16:59.328661Z"
    }
   },
   "source": [
    "def naive_softmax(x):\n",
    "    \"\"\"Compute row-wise softmax of X using native pytorch\n",
    "\n",
    "    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n",
    "    this shift.\n",
    "    \"\"\"\n",
    "    # read  MN elements ; write M  elements\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z)\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None]\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:16:59.622214Z",
     "start_time": "2025-06-13T08:16:59.406308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test\n",
    "x = torch.randn(1823, 781, device=DEVICE)\n",
    "y = naive_softmax(x)\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "assert torch.allclose(y, torch.softmax(x, dim=1))"
   ],
   "id": "83bb7b8c841eed43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1823, 781])\n",
      "y shape: torch.Size([1823, 781])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fused Softmax",
   "id": "5e66643879883d73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:16:59.709022Z",
     "start_time": "2025-06-13T08:16:59.704527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)"
   ],
   "id": "10b0a02c2b93c883",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:16:59.731549Z",
     "start_time": "2025-06-13T08:16:59.728022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 8\n",
    "\n",
    "    # Number of software pipelining stages.\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # pre-compile kernel to get register usage and compute thread occupancy.\n",
    "    kernel = softmax_kernel.warmup(\n",
    "        y, x, x.stride(0), y.stride(0), n_rows, n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_stages=num_stages,\n",
    "        num_warps=num_warps,\n",
    "        grid=(1, )\n",
    "    )\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    size_smem = kernel.metadata.shared\n",
    "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    num_programs = NUM_SM * occupancy\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    # Create a number of persistent programs.\n",
    "    kernel[(num_programs, 1, 1)](\n",
    "        y, x, x.stride(0), y.stride(0), n_rows, n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_stages=num_stages\n",
    "    )\n",
    "    return y"
   ],
   "id": "246ab1fe7fbe0bc7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T08:17:00.234151Z",
     "start_time": "2025-06-13T08:16:59.788505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test\n",
    "x = torch.randn(1823, 781, device=DEVICE)\n",
    "y = softmax(x)\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "assert torch.allclose(y, torch.softmax(x, dim=1))"
   ],
   "id": "1dfde94c8c8a8777",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CompiledKernel.__getitem__.<locals>.runner() got an unexpected keyword argument 'BLOCK_SIZE'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Test\u001B[39;00m\n\u001B[32m      2\u001B[39m x = torch.randn(\u001B[32m1823\u001B[39m, \u001B[32m781\u001B[39m, device=DEVICE)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m y = \u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mx shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33my shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36msoftmax\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     34\u001B[39m num_programs = \u001B[38;5;28mmin\u001B[39m(num_programs, n_rows)\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# Create a number of persistent programs.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[43mkernel\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_programs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_cols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43mBLOCK_SIZE\u001B[49m\u001B[43m=\u001B[49m\u001B[43mBLOCK_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_stages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_stages\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "\u001B[31mTypeError\u001B[39m: CompiledKernel.__getitem__.<locals>.runner() got an unexpected keyword argument 'BLOCK_SIZE'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"Triton\",\n",
    "            \"Torch\",\n",
    "        ],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    stream = getattr(torch, DEVICE.type).Stream()\n",
    "    getattr(torch, DEVICE.type).set_stream(stream)\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: softmax(x))\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ],
   "id": "59c309d8026f035a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
