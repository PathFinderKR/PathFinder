{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lm_eval --model hf \\\n",
    "--model_args pretrained=openai-community/gpt2-medium,trust_remote_code=True,dtype=\"bfloat16\" \\\n",
    "--tasks hellaswag,wikitext \\\n",
    "--device cuda:0 \\\n",
    "--batch_size auto"
   ],
   "id": "7cb80eaa24e82976"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:44:15.736110Z",
     "start_time": "2025-06-11T08:44:14.328495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from models.GPT import GPT\n",
    "from src.config import ModelConfig"
   ],
   "id": "8552809e4224e378",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:44:15.836211Z",
     "start_time": "2025-06-11T08:44:15.832876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + \"/..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "model_name = \"PathFinder\"\n",
    "run_name = \"2025-06-10_05-57-46\"\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name, run_name)"
   ],
   "id": "142468b6d8ce9e08",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:47:55.745613Z",
     "start_time": "2025-06-11T08:47:52.839836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pathfinder_config = ModelConfig(\n",
    "        d_embed=1024,\n",
    "        n_layers=16,\n",
    "        n_heads=16,\n",
    "        d_head=64,\n",
    "        rank=32,\n",
    "        d_ff=4096,\n",
    "        beta_min=1/2,\n",
    "        beta_max=4,\n",
    "        cross_layer_attention=True\n",
    ") # 117M\n",
    "model = GPT(pathfinder_config)\n",
    "repo_id=f\"PathFinderKR/{model_name}-{run_name}\",\n",
    "print(repo_id)\n",
    "model = GPT.from_pretrained(checkpoint_path, local_files_only=True)\n",
    "model.push_to_hub(\n",
    "        repo_id=f\"PathFinderKR/{model_name}-{run_name}\",\n",
    "        private=True,\n",
    "        use_auth_token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "print(f\"Model pushed to Hugging Face Hub: PathFinderKR/{model_name}-{run_name}\")"
   ],
   "id": "4dfa0b215838a92c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PathFinderKR/PathFinder-2025-06-10_05-57-46',)\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'checkpoints/PathFinder/2025-06-10_05-57-46'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHFValidationError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m repo_id=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPathFinderKR/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(repo_id)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m model = \u001B[43mGPT\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m model.push_to_hub(\n\u001B[32m     17\u001B[39m         repo_id=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPathFinderKR/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     18\u001B[39m         private=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     19\u001B[39m         use_auth_token=os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mHUGGINGFACE_TOKEN\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     20\u001B[39m )\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel pushed to Hugging Face Hub: PathFinderKR/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/huggingface_hub/hub_mixin.py:511\u001B[39m, in \u001B[36mModelHubMixin.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001B[39m\n\u001B[32m    509\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    510\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m511\u001B[39m         config_file = \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    512\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    513\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconstants\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCONFIG_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    514\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    515\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    516\u001B[39m \u001B[43m            \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    517\u001B[39m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m            \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    522\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m HfHubHTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    523\u001B[39m         logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstants.CONFIG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not found on the HuggingFace Hub: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m arg_name, arg_value \u001B[38;5;129;01min\u001B[39;00m chain(\n\u001B[32m    102\u001B[39m     \u001B[38;5;28mzip\u001B[39m(signature.parameters, args),  \u001B[38;5;66;03m# Args values\u001B[39;00m\n\u001B[32m    103\u001B[39m     kwargs.items(),  \u001B[38;5;66;03m# Kwargs values\u001B[39;00m\n\u001B[32m    104\u001B[39m ):\n\u001B[32m    105\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mrepo_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfrom_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mto_id\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m         \u001B[43mvalidate_repo_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    108\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m arg_name == \u001B[33m\"\u001B[39m\u001B[33mtoken\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    109\u001B[39m         has_token = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/envs/torch-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001B[39m, in \u001B[36mvalidate_repo_id\u001B[39m\u001B[34m(repo_id)\u001B[39m\n\u001B[32m    151\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRepo id must be a string, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(repo_id)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m repo_id.count(\u001B[33m\"\u001B[39m\u001B[33m/\u001B[39m\u001B[33m\"\u001B[39m) > \u001B[32m1\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[32m    155\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mRepo id must be in the form \u001B[39m\u001B[33m'\u001B[39m\u001B[33mrepo_name\u001B[39m\u001B[33m'\u001B[39m\u001B[33m or \u001B[39m\u001B[33m'\u001B[39m\u001B[33mnamespace/repo_name\u001B[39m\u001B[33m'\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    156\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m. Use `repo_type` argument if needed.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    157\u001B[39m     )\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX.match(repo_id):\n\u001B[32m    160\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[32m    161\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mRepo id must use alphanumeric chars or \u001B[39m\u001B[33m'\u001B[39m\u001B[33m-\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m_\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m--\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33m..\u001B[39m\u001B[33m'\u001B[39m\u001B[33m are\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    162\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m forbidden, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m-\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m'\u001B[39m\u001B[33m cannot start or end the name, max length is 96:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    163\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    164\u001B[39m     )\n",
      "\u001B[31mHFValidationError\u001B[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'checkpoints/PathFinder/2025-06-10_05-57-46'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8d82b81783cbee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        self.gate = nn.Linear(config.d_embed, config.n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, n_experts]\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        scores, indices = torch.topk(scores, self.top_k, dim=-1)  # [batch_size, seq_len, top_k]\n",
    "        return scores, indices\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        if config.n_experts is None or config.n_activated_experts is None:\n",
    "            raise ValueError(\"n_experts and n_activated_experts must be specified for MoE\")\n",
    "        if config.n_experts < config.n_activated_experts:\n",
    "            raise ValueError(\"n_experts must be greater than or equal to n_activated_experts\")\n",
    "        self.router = Router(config)\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "        if config.n_shared_experts is not None:\n",
    "            self.shared_experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_shared_experts)])\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        scores, indices = self.router(x)\n",
    "\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "        scores_flat = scores.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "        indices_flat = indices.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (indices_flat == expert_idx)\n",
    "            selected_pos = mask.any(dim=-1)  # [batch * seq]\n",
    "\n",
    "            if selected_pos.any():\n",
    "                expert_input = x_flat[selected_pos]  # [top_k, d_embed]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                selected_mask_idx, selected_topk_idx = torch.where(mask)\n",
    "                gating_scores = scores_flat[selected_mask_idx, selected_topk_idx].unsqueeze(1)  # [top_k, 1]\n",
    "\n",
    "                weighted_output = expert_output * gating_scores  # [top_k, d_embed]\n",
    "                y_flat[selected_pos] += weighted_output  # [batch * seq, d_embed]\n",
    "\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = sum([expert(x_flat) for expert in self.shared_experts]) / len(self.shared_experts)\n",
    "            y_flat += shared_output  # [batch * seq, d_embed]\n",
    "\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n",
    "\n",
    "\n",
    "class RouterFreeMoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "\n",
    "        # Step 1: Expert selection\n",
    "        deltas = []\n",
    "        with torch.no_grad():\n",
    "            # if Device compute capabilities is 8.9 or higher, use fp8_autocast\n",
    "            if torch.cuda.get_device_capability()[0] >= 8 and torch.cuda.get_device_capability()[1] >= 9:\n",
    "                #ctx = fp8_autocast(enabled=True)\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            else:\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            with ctx:\n",
    "                for expert in self.experts:\n",
    "                    expert_output = expert(x_flat)  # [batch * seq, d_embed]\n",
    "                    delta = (expert_output - x_flat).norm(dim=-1)  # [batch * seq]\n",
    "                    deltas.append(delta)\n",
    "            deltas = torch.stack(deltas, dim=0)  # [n_experts, batch * seq]\n",
    "            best_expert_idx = deltas.argmax(dim=0)  # [batch * seq]\n",
    "\n",
    "        # Step 2: Apply the best expert\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (best_expert_idx == expert_idx)  # [batch * seq]\n",
    "            if mask.any():\n",
    "                selected_input = x_flat[mask]  # [num_selected, d_embed]\n",
    "                selected_output = expert(selected_input)  # [num_selected, d_embed]\n",
    "                y_flat[mask] = selected_output  # [batch * seq, d_embed]\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n"
   ],
   "id": "dc64b93a3a483252"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
