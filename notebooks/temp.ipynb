{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lm_eval --model hf \\\n",
    "--model_args pretrained=openai-community/gpt2-medium,trust_remote_code=True,dtype=\"bfloat16\" \\\n",
    "--tasks hellaswag,wikitext \\\n",
    "--device cuda:0 \\\n",
    "--batch_size auto"
   ],
   "id": "1f0bb9d0ce5e62f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8d82b81783cbee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        self.gate = nn.Linear(config.d_embed, config.n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, n_experts]\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        scores, indices = torch.topk(scores, self.top_k, dim=-1)  # [batch_size, seq_len, top_k]\n",
    "        return scores, indices\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        if config.n_experts is None or config.n_activated_experts is None:\n",
    "            raise ValueError(\"n_experts and n_activated_experts must be specified for MoE\")\n",
    "        if config.n_experts < config.n_activated_experts:\n",
    "            raise ValueError(\"n_experts must be greater than or equal to n_activated_experts\")\n",
    "        self.router = Router(config)\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "        if config.n_shared_experts is not None:\n",
    "            self.shared_experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_shared_experts)])\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        scores, indices = self.router(x)\n",
    "\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "        scores_flat = scores.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "        indices_flat = indices.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (indices_flat == expert_idx)\n",
    "            selected_pos = mask.any(dim=-1)  # [batch * seq]\n",
    "\n",
    "            if selected_pos.any():\n",
    "                expert_input = x_flat[selected_pos]  # [top_k, d_embed]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                selected_mask_idx, selected_topk_idx = torch.where(mask)\n",
    "                gating_scores = scores_flat[selected_mask_idx, selected_topk_idx].unsqueeze(1)  # [top_k, 1]\n",
    "\n",
    "                weighted_output = expert_output * gating_scores  # [top_k, d_embed]\n",
    "                y_flat[selected_pos] += weighted_output  # [batch * seq, d_embed]\n",
    "\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = sum([expert(x_flat) for expert in self.shared_experts]) / len(self.shared_experts)\n",
    "            y_flat += shared_output  # [batch * seq, d_embed]\n",
    "\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n",
    "\n",
    "\n",
    "class RouterFreeMoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "\n",
    "        # Step 1: Expert selection\n",
    "        deltas = []\n",
    "        with torch.no_grad():\n",
    "            # if Device compute capabilities is 8.9 or higher, use fp8_autocast\n",
    "            if torch.cuda.get_device_capability()[0] >= 8 and torch.cuda.get_device_capability()[1] >= 9:\n",
    "                #ctx = fp8_autocast(enabled=True)\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            else:\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            with ctx:\n",
    "                for expert in self.experts:\n",
    "                    expert_output = expert(x_flat)  # [batch * seq, d_embed]\n",
    "                    delta = (expert_output - x_flat).norm(dim=-1)  # [batch * seq]\n",
    "                    deltas.append(delta)\n",
    "            deltas = torch.stack(deltas, dim=0)  # [n_experts, batch * seq]\n",
    "            best_expert_idx = deltas.argmax(dim=0)  # [batch * seq]\n",
    "\n",
    "        # Step 2: Apply the best expert\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (best_expert_idx == expert_idx)  # [batch * seq]\n",
    "            if mask.any():\n",
    "                selected_input = x_flat[mask]  # [num_selected, d_embed]\n",
    "                selected_output = expert(selected_input)  # [num_selected, d_embed]\n",
    "                y_flat[mask] = selected_output  # [batch * seq, d_embed]\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n"
   ],
   "id": "dab0f1f2d4425773"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    def get_flops(self, x):\n",
    "        B, T = x.size()\n",
    "        D = self.config.d_embed\n",
    "\n",
    "        # ---------- Accelerator Intensity in Nvidia -------------------------------------------------------------------\n",
    "        # FLOPs = 52.22 TFLOPs\n",
    "        # Memory Bandwidth = 736.6GB/s\n",
    "        # TensorCore Accelerator Intensity = 70.65\n",
    "\n",
    "        # ---------- Matrix multiplication FLOPs and Memory read/writes ------------------------------------------------\n",
    "        # x = [B, T, D], W = [D, F]\n",
    "        # 1. Read x to SRAM\n",
    "        # bytes = 2 x B X T X D = 2BTD\n",
    "        # 2. Read W to SRAM\n",
    "        # bytes = 2 x D x F = 2DF\n",
    "        # 3. Compute Y = x @ W\n",
    "        # FLOPS = 2 x B x T x D x F = 2BTDF\n",
    "        # 4. Write Y to HBM\n",
    "        # bytes = 2BTF\n",
    "        # Arithmetic Intensity\n",
    "        # AI = 2BTDF / (2BTD + 2DF + 2BTF) = 4BTD / (5BT + 4D)\n",
    "        #    = BT\n",
    "\n",
    "        # ---------- Attention FLOPs and Memory read/writes ------------------------------------------------------------\n",
    "        # 1. Read x from HBM\n",
    "        # bytes =\n",
    "        # 2. Read Wq Wk Wv from HBM\n",
    "        # bytes =\n",
    "        # 3. Compute\n",
    "        #\n",
    "        #\n",
    "\n",
    "        ########## Multi Head Attention ################################################################################\n",
    "        if self.config.rank is None:\n",
    "            ## Prefill\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q, K, V from HBM\n",
    "            # bytes = 3 x (2 x B x T x D) = 6BTD\n",
    "            # 2. Compute Q @ K\n",
    "            # FLOPS = 2 x B x T x T x D = 2BT^2D\n",
    "            # 3. Compute A @ V\n",
    "            # FLOPS = 2 × B × T × T × D = 2BT^2D\n",
    "            # 4. Write attn_out\n",
    "            # bytes = 2 x B x T x D = 2BTD\n",
    "            ####################################\n",
    "            # attn bytes = 6BTD + 2BTD = 8BTD\n",
    "            # attn FLOPS = 2BT^2D + 2BT^2D = 4BT^2D\n",
    "            # attn AI = 4BT^2D / 8BTD = T/2\n",
    "            #         = T\n",
    "            attn_prefill_flops = 4 * B * T * T * D\n",
    "            attn_prefill_ai = T / 2\n",
    "\n",
    "            ## Decoding\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q, K, V from HBM\n",
    "            # bytes = 2 x B x 1 x D + 2 x (2 x B x S x D) = 2BD + 4BSD\n",
    "            # 2. Compute Q @ K\n",
    "            # FLOPS = 2 x B x 1 x S x D = 2BSD\n",
    "            # 3. Compute A @ V\n",
    "            # FLOPS = 2 × B × S × 1 × D = 2BSD\n",
    "            # 4. Write attn_out\n",
    "            # bytes = 2 x B x 1 x D = 2BD\n",
    "            ####################################\n",
    "            # attn bytes = 2BD + 4BSD + 2BD = 4BD(1 + S)\n",
    "            # attn FLOPS = 2BSD + 2BSD = 4BSD\n",
    "            # attn AI = 4BSD / 4BD(1 + S) = S / (1 + S) (ignore 1)\n",
    "            #         = 1\n",
    "            attn_decoding_flops = 4 * B * T * D\n",
    "            attn_decoding_ai = T / (1 + T)\n",
    "            # Why so low ai?\n",
    "            # FLOPS decreased by T, bytes remain the same\n",
    "\n",
    "        ########## Multi Head Latent Attention #########################################################################\n",
    "        else:\n",
    "            R = self.config.rank\n",
    "            ## Prefill\n",
    "            # 1. Read KV_latent from HBM\n",
    "            # bytes = 2 x (2 x B x T x R) = 4BTR\n",
    "            # 2. Read Wkv_up from HBM\n",
    "            # bytes = 2 x R x 2D = 4RD\n",
    "            # 3. Compute kv_latent @ Wkv_up\n",
    "            # FLOPS = 2 x B x T x R x 2D = 4BTRD\n",
    "            # 4. Write k, v to HBM\n",
    "            # bytes = 2 x (2 x B x T x D) = 4BTD\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q, K, V from HBM\n",
    "            # bytes = 3 x (2 x B x T x D) = 6BTD\n",
    "            # 2. Compute Q @ K\n",
    "            # FLOPS = 2 x B x T x T x D = 2BT^2D\n",
    "            # 3. Compute A @ V\n",
    "            # FLOPS = 2 × B × T × T × D = 2BT^2D\n",
    "            # 4. Write y\n",
    "            # bytes = 2 x B x T x D = 2BTD\n",
    "            ####################################\n",
    "            # attn bytes = 4BTR + 4RD + 4BTD + 6BTD + 2BTD = 4(RD + BTR + 3BTD)\n",
    "            # attn FLOPS = 4BTRD + 2BT^2D + 2BT^2D = 4BTD(R + T)\n",
    "            # attn AI = 4BTD(R + T) / 4(RD + BTR + 3BTD) = BTD(R + T) / RD + BTR + 3BTD\n",
    "            #         =\n",
    "            attn_prefill_flops = 4 * B * T * D * (R + T)\n",
    "            attn_prefill_ai = T / 2\n",
    "\n",
    "            ## Decoding\n",
    "            # 1. KV_latent from HBM\n",
    "            # bytes = 2 x (2 x B x S x R) = 4BSR\n",
    "            # 2. Read Wkv_up from HBM\n",
    "            # bytes = 2 x R x 2D = 4RD\n",
    "            # 3. Compute kv_latent @ Wkv_up\n",
    "            # FLOPS = 2 x B x S x R x 2D = 4BSRD\n",
    "            # 4. Write k, v to HBM\n",
    "            # bytes = 2 x (2 x B x S x D) = 4BSD\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q_latent, KV_latent from HBM\n",
    "            # bytes = 2 x B x S x R x D / d + 2 x B x S x R = 2BSR(D/d + 1)\n",
    "            # 2. Compute Q_latent @ K_latent\n",
    "            # FLOPS = 2 x B x 1 x S x R = 2BSR\n",
    "            # 3. Compute A @ KV_latent\n",
    "            # FLOPS = 2 x B x S x 1 x R = 2BSR\n",
    "            # 4. Write y\n",
    "            # bytes = 2 x B x 1 X R = 2BR\n",
    "            ####################################\n",
    "            # 5. Compute\n",
    "            # attn bytes =\n",
    "            # attn FLOPS =\n",
    "            # attn AI = 2S / (1 + S(D/d + 1))\n",
    "            #         =\n",
    "            attn_decoding_flops = 4 * B * T * D\n",
    "            attn_decoding_ai = T / (1 + T)\n",
    "        ################################################################################################################\n",
    "\n",
    "        ## KV cache\n",
    "        ##### MultiHeadAttention ###########\n",
    "        # size = 2 x (2 x B x S x D) = 4BSD\n",
    "        # AI = 1\n",
    "        ##### GroupedQueryAttention ########\n",
    "        # size = 2 x (2 x B x S x D / n_groups) = 4BSD / n_groups\n",
    "        # AI = G (group_size)\n",
    "        ##### MultiQueryAttention ##########\n",
    "        # size = 2 x (2 x B X S x d) = 4BSD / n_heads\n",
    "        # AI = n_heads\n",
    "        ##### MultiHeadLatentAttention #####\n",
    "        # size = 2 x (B x S x R) = 2BSR = 4BSD / (2D/R)\n",
    "        # AI = 2D/R\n",
    "\n",
    "        # ---------- FeedForward FLOPs and Memory read/writes ----------------------------------------------------------\n",
    "        # 1. Read x from HBM\n",
    "        # bytes = 2 x B x T x D = 2BTD\n",
    "        # 2. Read Wup, Wdown from HBM\n",
    "        # bytes = 2 x (2 x D x 4D) = 16D^2\n",
    "        # 3. Compute x @ Wup\n",
    "        # FLOPS = 2 x B x T x D x 4D = 8BTD^2\n",
    "        # 4. Compute x @ Wdown\n",
    "        # FLOPS = 2 x B x T x 4D x D = 8BTD^2\n",
    "        # 5. Write x to HBM\n",
    "        # bytes = 2 x B x T x D = 2BTD\n",
    "        ####################################\n",
    "        # FF bytes = 2BTD + 16D^2 + 2BTD = 4D(BT + 4D)\n",
    "        # FF FLOPS = 8BTD^2 + 8BTD^2 = 16BTD^2\n",
    "        # FF AI = 16BTD^2 / 4D(BT + 4D) = 4BTD / (BT + 4D) (ignore 2BT)\n",
    "        #       = BT\n",
    "        feedforward_flops = 16 * B * T * D * D\n",
    "        feedforward_ai = 4 * B * T * D / (B * T + 4 * D)\n",
    "\n",
    "        flops = {\n",
    "            'attn_prefill_flops': attn_prefill_flops, 'attn_prefill_ai': attn_prefill_ai,\n",
    "            'attn_decoding_flops': attn_decoding_flops, 'attn_decoding_ai': attn_decoding_ai,\n",
    "            'feedforward_flops': feedforward_flops, 'feedforward_ai': feedforward_ai\n",
    "        }\n",
    "\n",
    "        return flops"
   ],
   "id": "475eedbead1d0849"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
