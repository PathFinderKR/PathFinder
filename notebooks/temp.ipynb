{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lm_eval --model hf \\\n",
    "--model_args pretrained=openai-community/gpt2-medium,trust_remote_code=True,dtype=\"bfloat16\" \\\n",
    "--tasks hellaswag,wikitext \\\n",
    "--device cuda:0 \\\n",
    "--batch_size auto"
   ],
   "id": "1f0bb9d0ce5e62f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8d82b81783cbee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        self.gate = nn.Linear(config.d_embed, config.n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, n_experts]\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        scores, indices = torch.topk(scores, self.top_k, dim=-1)  # [batch_size, seq_len, top_k]\n",
    "        return scores, indices\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        if config.n_experts is None or config.n_activated_experts is None:\n",
    "            raise ValueError(\"n_experts and n_activated_experts must be specified for MoE\")\n",
    "        if config.n_experts < config.n_activated_experts:\n",
    "            raise ValueError(\"n_experts must be greater than or equal to n_activated_experts\")\n",
    "        self.router = Router(config)\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "        if config.n_shared_experts is not None:\n",
    "            self.shared_experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_shared_experts)])\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        scores, indices = self.router(x)\n",
    "\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "        scores_flat = scores.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "        indices_flat = indices.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (indices_flat == expert_idx)\n",
    "            selected_pos = mask.any(dim=-1)  # [batch * seq]\n",
    "\n",
    "            if selected_pos.any():\n",
    "                expert_input = x_flat[selected_pos]  # [top_k, d_embed]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                selected_mask_idx, selected_topk_idx = torch.where(mask)\n",
    "                gating_scores = scores_flat[selected_mask_idx, selected_topk_idx].unsqueeze(1)  # [top_k, 1]\n",
    "\n",
    "                weighted_output = expert_output * gating_scores  # [top_k, d_embed]\n",
    "                y_flat[selected_pos] += weighted_output  # [batch * seq, d_embed]\n",
    "\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = sum([expert(x_flat) for expert in self.shared_experts]) / len(self.shared_experts)\n",
    "            y_flat += shared_output  # [batch * seq, d_embed]\n",
    "\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n",
    "\n",
    "\n",
    "class RouterFreeMoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "\n",
    "        # Step 1: Expert selection\n",
    "        deltas = []\n",
    "        with torch.no_grad():\n",
    "            # if Device compute capabilities is 8.9 or higher, use fp8_autocast\n",
    "            if torch.cuda.get_device_capability()[0] >= 8 and torch.cuda.get_device_capability()[1] >= 9:\n",
    "                #ctx = fp8_autocast(enabled=True)\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            else:\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            with ctx:\n",
    "                for expert in self.experts:\n",
    "                    expert_output = expert(x_flat)  # [batch * seq, d_embed]\n",
    "                    delta = (expert_output - x_flat).norm(dim=-1)  # [batch * seq]\n",
    "                    deltas.append(delta)\n",
    "            deltas = torch.stack(deltas, dim=0)  # [n_experts, batch * seq]\n",
    "            best_expert_idx = deltas.argmax(dim=0)  # [batch * seq]\n",
    "\n",
    "        # Step 2: Apply the best expert\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (best_expert_idx == expert_idx)  # [batch * seq]\n",
    "            if mask.any():\n",
    "                selected_input = x_flat[mask]  # [num_selected, d_embed]\n",
    "                selected_output = expert(selected_input)  # [num_selected, d_embed]\n",
    "                y_flat[mask] = selected_output  # [batch * seq, d_embed]\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n"
   ],
   "id": "dab0f1f2d4425773"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    def get_flops(self, x):\n",
    "        B, T = x.size()\n",
    "        D = self.config.d_embed\n",
    "\n",
    "        # ---------- Accelerator Intensity in Nvidia -------------------------------------------------------------------\n",
    "        # FLOPs = 52.22 TFLOPs\n",
    "        # Memory Bandwidth = 736.6GB/s\n",
    "        # TensorCore Accelerator Intensity = 70.65\n",
    "\n",
    "        # ---------- Matrix multiplication FLOPs and Memory read/writes ------------------------------------------------\n",
    "        # x = [B, T, D], W = [D, F]\n",
    "        # 1. Read x to SRAM\n",
    "        # bytes = 2 x B X T X D = 2BTD\n",
    "        # 2. Read W to SRAM\n",
    "        # bytes = 2 x D x F = 2DF\n",
    "        # 3. Compute Y = x @ W\n",
    "        # FLOPS = 2 x B x T x D x F = 2BTDF\n",
    "        # 4. Write Y to HBM\n",
    "        # bytes = 2BTF\n",
    "        # Arithmetic Intensity\n",
    "        # AI = 2BTDF / (2BTD + 2DF + 2BTF) = 4BTD / (5BT + 4D)\n",
    "        #    = BT\n",
    "\n",
    "        # ---------- Attention FLOPs and Memory read/writes ------------------------------------------------------------\n",
    "        # 1. Read x from HBM\n",
    "        # bytes =\n",
    "        # 2. Read Wq Wk Wv from HBM\n",
    "        # bytes =\n",
    "        # 3. Compute\n",
    "        #\n",
    "        #\n",
    "\n",
    "        ########## Multi Head Attention ################################################################################\n",
    "        if self.config.rank is None:\n",
    "            ## Prefill\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q, K, V from HBM\n",
    "            # bytes = 3 x (2 x B x T x D) = 6BTD\n",
    "            # 2. Compute Q @ K\n",
    "            # FLOPS = 2 x B x T x T x D = 2BT^2D\n",
    "            # 3. Compute A @ V\n",
    "            # FLOPS = 2 × B × T × T × D = 2BT^2D\n",
    "            # 4. Write attn_out\n",
    "            # bytes = 2 x B x T x D = 2BTD\n",
    "            ####################################\n",
    "            # attn bytes = 6BTD + 2BTD = 8BTD\n",
    "            # attn FLOPS = 2BT^2D + 2BT^2D = 4BT^2D\n",
    "            # attn AI = 4BT^2D / 8BTD = T/2\n",
    "            #         = T\n",
    "            attn_prefill_flops = 4 * B * T * T * D\n",
    "            attn_prefill_ai = T / 2\n",
    "\n",
    "            ## Decoding\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q, K, V from HBM\n",
    "            # bytes = 2 x B x 1 x D + 2 x (2 x B x S x D) = 2BD + 4BSD\n",
    "            # 2. Compute Q @ K\n",
    "            # FLOPS = 2 x B x 1 x S x D = 2BSD\n",
    "            # 3. Compute A @ V\n",
    "            # FLOPS = 2 × B × S × 1 × D = 2BSD\n",
    "            # 4. Write attn_out\n",
    "            # bytes = 2 x B x 1 x D = 2BD\n",
    "            ####################################\n",
    "            # attn bytes = 2BD + 4BSD + 2BD = 4BD(1 + S)\n",
    "            # attn FLOPS = 2BSD + 2BSD = 4BSD\n",
    "            # attn AI = 4BSD / 4BD(1 + S) = S / (1 + S) (ignore 1)\n",
    "            #         = 1\n",
    "            attn_decoding_flops = 4 * B * T * D\n",
    "            attn_decoding_ai = T / (1 + T)\n",
    "            # Why so low ai?\n",
    "            # FLOPS decreased by T, bytes remain the same\n",
    "\n",
    "        ########## Multi Head Latent Attention #########################################################################\n",
    "        else:\n",
    "            R = self.config.rank\n",
    "            ## Prefill\n",
    "            # 1. Read KV_latent from HBM\n",
    "            # bytes = 2 x (2 x B x T x R) = 4BTR\n",
    "            # 2. Read Wkv_up from HBM\n",
    "            # bytes = 2 x R x 2D = 4RD\n",
    "            # 3. Compute kv_latent @ Wkv_up\n",
    "            # FLOPS = 2 x B x T x R x 2D = 4BTRD\n",
    "            # 4. Write k, v to HBM\n",
    "            # bytes = 2 x (2 x B x T x D) = 4BTD\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q, K, V from HBM\n",
    "            # bytes = 3 x (2 x B x T x D) = 6BTD\n",
    "            # 2. Compute Q @ K\n",
    "            # FLOPS = 2 x B x T x T x D = 2BT^2D\n",
    "            # 3. Compute A @ V\n",
    "            # FLOPS = 2 × B × T × T × D = 2BT^2D\n",
    "            # 4. Write y\n",
    "            # bytes = 2 x B x T x D = 2BTD\n",
    "            ####################################\n",
    "            # attn bytes = 4BTR + 4RD + 4BTD + 6BTD + 2BTD = 4(RD + BTR + 3BTD)\n",
    "            # attn FLOPS = 4BTRD + 2BT^2D + 2BT^2D = 4BTD(R + T)\n",
    "            # attn AI = 4BTD(R + T) / 4(RD + BTR + 3BTD) = BTD(R + T) / RD + BTR + 3BTD\n",
    "            #         =\n",
    "            attn_prefill_flops = 4 * B * T * D * (R + T)\n",
    "            attn_prefill_ai = T / 2\n",
    "\n",
    "            ## Decoding\n",
    "            # 1. KV_latent from HBM\n",
    "            # bytes = 2 x (2 x B x S x R) = 4BSR\n",
    "            # 2. Read Wkv_up from HBM\n",
    "            # bytes = 2 x R x 2D = 4RD\n",
    "            # 3. Compute kv_latent @ Wkv_up\n",
    "            # FLOPS = 2 x B x S x R x 2D = 4BSRD\n",
    "            # 4. Write k, v to HBM\n",
    "            # bytes = 2 x (2 x B x S x D) = 4BSD\n",
    "            ######### Flash Attention ##########\n",
    "            # 1. Read Q_latent, KV_latent from HBM\n",
    "            # bytes = 2 x B x S x R x D / d + 2 x B x S x R = 2BSR(D/d + 1)\n",
    "            # 2. Compute Q_latent @ K_latent\n",
    "            # FLOPS = 2 x B x 1 x S x R = 2BSR\n",
    "            # 3. Compute A @ KV_latent\n",
    "            # FLOPS = 2 x B x S x 1 x R = 2BSR\n",
    "            # 4. Write y\n",
    "            # bytes = 2 x B x 1 X R = 2BR\n",
    "            ####################################\n",
    "            # 5. Compute\n",
    "            # attn bytes =\n",
    "            # attn FLOPS =\n",
    "            # attn AI = 2S / (1 + S(D/d + 1))\n",
    "            #         =\n",
    "            attn_decoding_flops = 4 * B * T * D\n",
    "            attn_decoding_ai = T / (1 + T)\n",
    "        ################################################################################################################\n",
    "\n",
    "        ## KV cache\n",
    "        ##### MultiHeadAttention ###########\n",
    "        # size = 2 x (2 x B x S x D) = 4BSD\n",
    "        # AI = 1\n",
    "        ##### GroupedQueryAttention ########\n",
    "        # size = 2 x (2 x B x S x D / n_groups) = 4BSD / n_groups\n",
    "        # AI = G (group_size)\n",
    "        ##### MultiQueryAttention ##########\n",
    "        # size = 2 x (2 x B X S x d) = 4BSD / n_heads\n",
    "        # AI = n_heads\n",
    "        ##### MultiHeadLatentAttention #####\n",
    "        # size = 2 x (B x S x R) = 2BSR = 4BSD / (2D/R)\n",
    "        # AI = 2D/R\n",
    "\n",
    "        # ---------- FeedForward FLOPs and Memory read/writes ----------------------------------------------------------\n",
    "        # 1. Read x from HBM\n",
    "        # bytes = 2 x B x T x D = 2BTD\n",
    "        # 2. Read Wup, Wdown from HBM\n",
    "        # bytes = 2 x (2 x D x 4D) = 16D^2\n",
    "        # 3. Compute x @ Wup\n",
    "        # FLOPS = 2 x B x T x D x 4D = 8BTD^2\n",
    "        # 4. Compute x @ Wdown\n",
    "        # FLOPS = 2 x B x T x 4D x D = 8BTD^2\n",
    "        # 5. Write x to HBM\n",
    "        # bytes = 2 x B x T x D = 2BTD\n",
    "        ####################################\n",
    "        # FF bytes = 2BTD + 16D^2 + 2BTD = 4D(BT + 4D)\n",
    "        # FF FLOPS = 8BTD^2 + 8BTD^2 = 16BTD^2\n",
    "        # FF AI = 16BTD^2 / 4D(BT + 4D) = 4BTD / (BT + 4D) (ignore 2BT)\n",
    "        #       = BT\n",
    "        feedforward_flops = 16 * B * T * D * D\n",
    "        feedforward_ai = 4 * B * T * D / (B * T + 4 * D)\n",
    "\n",
    "        flops = {\n",
    "            'attn_prefill_flops': attn_prefill_flops, 'attn_prefill_ai': attn_prefill_ai,\n",
    "            'attn_decoding_flops': attn_decoding_flops, 'attn_decoding_ai': attn_decoding_ai,\n",
    "            'feedforward_flops': feedforward_flops, 'feedforward_ai': feedforward_ai\n",
    "        }\n",
    "\n",
    "        return flops"
   ],
   "id": "475eedbead1d0849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:04:20.208191Z",
     "start_time": "2025-06-18T08:04:01.056035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_N': 16, 'BLOCK_D': 64}, num_warps=2, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 32, 'BLOCK_D': 64}, num_warps=2, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 64, 'BLOCK_D': 64}, num_warps=4, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 128, 'BLOCK_D': 64}, num_warps=4, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 256, 'BLOCK_D': 64}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 512, 'BLOCK_D': 64}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 32, 'BLOCK_D': 128}, num_warps=4, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 64, 'BLOCK_D': 128}, num_warps=4, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 128, 'BLOCK_D': 128}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 256, 'BLOCK_D': 128}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 64, 'BLOCK_D': 256}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 128, 'BLOCK_D': 256}, num_warps=8, num_stages=2),\n",
    "    ],\n",
    "    key=['N', 'D'],  # 시퀀스 길이와 d_head에 따라 자동 최적화\n",
    ")\n",
    "@triton.jit\n",
    "def autotuned_flash_decode_kernel(\n",
    "        Q, K, V, Out,\n",
    "        stride_qb, stride_qh, stride_qd,\n",
    "        stride_kb, stride_kh, stride_kn, stride_kd,\n",
    "        stride_vb, stride_vh, stride_vn, stride_vd,\n",
    "        stride_ob, stride_oh, stride_od,\n",
    "        B, H, N, D,\n",
    "        scale,\n",
    "        BLOCK_N: tl.constexpr,\n",
    "        BLOCK_D: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton Autotuned Flash Decode Kernel\n",
    "    - 자동으로 최적 블록 크기 선택\n",
    "    - 다양한 하드웨어에서 최적 성능\n",
    "    \"\"\"\n",
    "    # 작업 ID\n",
    "    pid = tl.program_id(0)\n",
    "    batch_id = pid // H\n",
    "    head_id = pid % H\n",
    "\n",
    "    # Q 벡터 로드\n",
    "    q_offset = batch_id * stride_qb + head_id * stride_qh\n",
    "    d_range = tl.arange(0, BLOCK_D)\n",
    "    q = tl.load(Q + q_offset + d_range, mask=d_range < D)\n",
    "\n",
    "    # 누적 변수\n",
    "    acc = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    max_score = -float('inf')\n",
    "    sum_exp = 0.0\n",
    "\n",
    "    # KV 블록 처리\n",
    "    for start_n in tl.range(0, N, BLOCK_N):\n",
    "        end_n = tl.minimum(start_n + BLOCK_N, N)\n",
    "\n",
    "        # K 블록 로드\n",
    "        k_offset = batch_id * stride_kb + head_id * stride_kh + start_n * stride_kn\n",
    "        n_range = tl.arange(0, BLOCK_N)\n",
    "        k_ptrs = K + k_offset + n_range[:, None] * stride_kn + d_range[None, :]\n",
    "        k_mask = (n_range[:, None] < (end_n - start_n)) & (d_range[None, :] < D)\n",
    "        k_vals = tl.load(k_ptrs, mask=k_mask, other=0.0)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = tl.sum(q[None, :] * k_vals, axis=1) * scale\n",
    "        scores = tl.where(n_range < (end_n - start_n), scores, -float('inf'))\n",
    "\n",
    "        # Online softmax\n",
    "        block_max = tl.max(scores)\n",
    "        new_max = tl.maximum(max_score, block_max)\n",
    "\n",
    "        if max_score > -float('inf'):\n",
    "            exp_diff = tl.exp(max_score - new_max)\n",
    "            acc = acc * exp_diff\n",
    "            sum_exp = sum_exp * exp_diff\n",
    "\n",
    "        # Softmax weights\n",
    "        weights = tl.exp(scores - new_max)\n",
    "        weights = tl.where(n_range < (end_n - start_n), weights, 0.0)\n",
    "        block_sum = tl.sum(weights)\n",
    "\n",
    "        # V 블록 로드 및 누적\n",
    "        v_offset = batch_id * stride_vb + head_id * stride_vh + start_n * stride_vn\n",
    "        v_ptrs = V + v_offset + n_range[:, None] * stride_vn + d_range[None, :]\n",
    "        v_mask = (n_range[:, None] < (end_n - start_n)) & (d_range[None, :] < D)\n",
    "        v_vals = tl.load(v_ptrs, mask=v_mask, other=0.0)\n",
    "\n",
    "        weighted_v = tl.sum(weights[:, None] * v_vals, axis=0)\n",
    "        acc = acc + weighted_v\n",
    "        sum_exp = sum_exp + block_sum\n",
    "        max_score = new_max\n",
    "\n",
    "    # 출력\n",
    "    result = acc / tl.maximum(sum_exp, 1e-8)\n",
    "    out_offset = batch_id * stride_ob + head_id * stride_oh\n",
    "    tl.store(Out + out_offset + d_range, result, mask=d_range < D)\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_N': 32, 'BLOCK_D': 64, 'BLOCK_BH': 1}, num_warps=2, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 64, 'BLOCK_D': 64, 'BLOCK_BH': 1}, num_warps=4, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 128, 'BLOCK_D': 64, 'BLOCK_BH': 2}, num_warps=4, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 256, 'BLOCK_D': 64, 'BLOCK_BH': 2}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 512, 'BLOCK_D': 64, 'BLOCK_BH': 4}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 64, 'BLOCK_D': 128, 'BLOCK_BH': 1}, num_warps=4, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 128, 'BLOCK_D': 128, 'BLOCK_BH': 2}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 256, 'BLOCK_D': 128, 'BLOCK_BH': 4}, num_warps=8, num_stages=2),\n",
    "        triton.Config({'BLOCK_N': 128, 'BLOCK_D': 256, 'BLOCK_BH': 2}, num_warps=8, num_stages=2),\n",
    "    ],\n",
    "    key=['B', 'H', 'N', 'D'],  # 모든 차원을 고려한 최적화\n",
    ")\n",
    "@triton.jit\n",
    "def mega_autotuned_flash_kernel(\n",
    "        Q, K, V, Out,\n",
    "        stride_qb, stride_qh, stride_qd,\n",
    "        stride_kb, stride_kh, stride_kn, stride_kd,\n",
    "        stride_vb, stride_vh, stride_vn, stride_vd,\n",
    "        stride_ob, stride_oh, stride_od,\n",
    "        B, H, N, D,\n",
    "        scale,\n",
    "        BLOCK_N: tl.constexpr,\n",
    "        BLOCK_D: tl.constexpr,\n",
    "        BLOCK_BH: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Mega Autotuned Flash Kernel\n",
    "    - 배치/헤드 블로킹도 자동 최적화\n",
    "    - 대규모 워크로드 특화\n",
    "    \"\"\"\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # 블록당 여러 배치×헤드 처리\n",
    "    for local_idx in range(BLOCK_BH):\n",
    "        bh_id = pid * BLOCK_BH + local_idx\n",
    "        if bh_id >= B * H:\n",
    "            break\n",
    "\n",
    "        batch_id = bh_id // H\n",
    "        head_id = bh_id % H\n",
    "\n",
    "        # Q 로드\n",
    "        q_offset = batch_id * stride_qb + head_id * stride_qh\n",
    "        d_range = tl.arange(0, BLOCK_D)\n",
    "        q = tl.load(Q + q_offset + d_range, mask=d_range < D)\n",
    "\n",
    "        # Flash attention 계산\n",
    "        acc = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "        max_score = -float('inf')\n",
    "        sum_exp = 0.0\n",
    "\n",
    "        for start_n in tl.range(0, N, BLOCK_N):\n",
    "            end_n = tl.minimum(start_n + BLOCK_N, N)\n",
    "            block_size = end_n - start_n\n",
    "\n",
    "            # K 로드\n",
    "            k_offset = batch_id * stride_kb + head_id * stride_kh + start_n * stride_kn\n",
    "            n_range = tl.arange(0, BLOCK_N)\n",
    "            k_ptrs = K + k_offset + n_range[:, None] * stride_kn + d_range[None, :]\n",
    "            k_mask = (n_range[:, None] < block_size) & (d_range[None, :] < D)\n",
    "            k_vals = tl.load(k_ptrs, mask=k_mask, other=0.0)\n",
    "\n",
    "            # Scores\n",
    "            scores = tl.sum(q[None, :] * k_vals, axis=1) * scale\n",
    "            scores = tl.where(n_range < block_size, scores, -float('inf'))\n",
    "\n",
    "            # Softmax\n",
    "            block_max = tl.max(scores)\n",
    "            new_max = tl.maximum(max_score, block_max)\n",
    "\n",
    "            if max_score > -float('inf'):\n",
    "                scale_factor = tl.exp(max_score - new_max)\n",
    "                acc = acc * scale_factor\n",
    "                sum_exp = sum_exp * scale_factor\n",
    "\n",
    "            weights = tl.exp(scores - new_max)\n",
    "            weights = tl.where(n_range < block_size, weights, 0.0)\n",
    "            block_sum = tl.sum(weights)\n",
    "\n",
    "            # V 로드 및 누적\n",
    "            v_offset = batch_id * stride_vb + head_id * stride_vh + start_n * stride_vn\n",
    "            v_ptrs = V + v_offset + n_range[:, None] * stride_vn + d_range[None, :]\n",
    "            v_mask = (n_range[:, None] < block_size) & (d_range[None, :] < D)\n",
    "            v_vals = tl.load(v_ptrs, mask=v_mask, other=0.0)\n",
    "\n",
    "            weighted_v = tl.sum(weights[:, None] * v_vals, axis=0)\n",
    "            acc = acc + weighted_v\n",
    "            sum_exp = sum_exp + block_sum\n",
    "            max_score = new_max\n",
    "\n",
    "        # 출력 저장\n",
    "        result = acc / tl.maximum(sum_exp, 1e-8)\n",
    "        out_offset = batch_id * stride_ob + head_id * stride_oh\n",
    "        tl.store(Out + out_offset + d_range, result, mask=d_range < D)\n",
    "\n",
    "\n",
    "def autotuned_flash_attn_decode(q, k, v, scale=None):\n",
    "    \"\"\"Autotuned Flash Attention Decode\"\"\"\n",
    "    batch_size, n_heads, q_seq_len, d_head = q.shape\n",
    "    _, _, kv_seq_len, _ = k.shape\n",
    "\n",
    "    assert q_seq_len == 1, \"Decode mode requires q_seq_len=1\"\n",
    "\n",
    "    if scale is None:\n",
    "        scale = 1.0 / math.sqrt(d_head)\n",
    "\n",
    "    out = torch.empty_like(q)\n",
    "    grid = (batch_size * n_heads,)\n",
    "\n",
    "    autotuned_flash_decode_kernel[grid](\n",
    "        q, k, v, out,\n",
    "        q.stride(0), q.stride(1), q.stride(3),\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        out.stride(0), out.stride(1), out.stride(3),\n",
    "        batch_size, n_heads, kv_seq_len, d_head,\n",
    "        scale,\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def mega_autotuned_flash_attn_decode(q, k, v, scale=None):\n",
    "    \"\"\"Mega Autotuned Flash Attention - 대규모 배치 특화\"\"\"\n",
    "    batch_size, n_heads, q_seq_len, d_head = q.shape\n",
    "    _, _, kv_seq_len, _ = k.shape\n",
    "\n",
    "    if scale is None:\n",
    "        scale = 1.0 / math.sqrt(d_head)\n",
    "\n",
    "    out = torch.empty_like(q)\n",
    "\n",
    "    # 자동 그리드 크기 계산\n",
    "    total_bh = batch_size * n_heads\n",
    "    # Autotune이 BLOCK_BH를 결정하므로 충분히 큰 그리드 설정\n",
    "    grid_size = min(total_bh, 1024)  # 최대 1024 블록\n",
    "\n",
    "    mega_autotuned_flash_kernel[(grid_size,)](\n",
    "        q, k, v, out,\n",
    "        q.stride(0), q.stride(1), q.stride(3),\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        out.stride(0), out.stride(1), out.stride(3),\n",
    "        batch_size, n_heads, kv_seq_len, d_head,\n",
    "        scale,\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def naive_attention(q, k, v, scale=None):\n",
    "    \"\"\"PyTorch 기본 구현\"\"\"\n",
    "    if scale is None:\n",
    "        scale = 1.0 / math.sqrt(q.size(-1))\n",
    "\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(attn_weights, v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def comprehensive_scale_benchmark():\n",
    "    \"\"\"포괄적 스케일 벤치마크 - batch_size=1 포함\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"❌ CUDA 필요\")\n",
    "        return\n",
    "\n",
    "    # 다양한 스케일 테스트 (batch_size=1 포함)\n",
    "    test_configs = [\n",
    "        # Single batch 시나리오 (추론 서버)\n",
    "        {\"batch_size\": 1, \"n_heads\": 12, \"kv_seq_len\": 2048, \"d_head\": 64},\n",
    "        {\"batch_size\": 1, \"n_heads\": 32, \"kv_seq_len\": 8192, \"d_head\": 128},\n",
    "        {\"batch_size\": 1, \"n_heads\": 40, \"kv_seq_len\": 16384, \"d_head\": 128},\n",
    "        {\"batch_size\": 1, \"n_heads\": 64, \"kv_seq_len\": 32768, \"d_head\": 128},\n",
    "\n",
    "        # Small batch 시나리오\n",
    "        {\"batch_size\": 4, \"n_heads\": 96, \"kv_seq_len\": 4096, \"d_head\": 128},\n",
    "        {\"batch_size\": 8, \"n_heads\": 128, \"kv_seq_len\": 8192, \"d_head\": 128},\n",
    "\n",
    "        # Medium batch 시나리오\n",
    "        {\"batch_size\": 32, \"n_heads\": 64, \"kv_seq_len\": 4096, \"d_head\": 128},\n",
    "        {\"batch_size\": 64, \"n_heads\": 80, \"kv_seq_len\": 8192, \"d_head\": 128},\n",
    "\n",
    "        # Large batch 시나리오\n",
    "        {\"batch_size\": 128, \"n_heads\": 96, \"kv_seq_len\": 4096, \"d_head\": 128},\n",
    "        {\"batch_size\": 256, \"n_heads\": 64, \"kv_seq_len\": 2048, \"d_head\": 128},\n",
    "\n",
    "        # 초장문 컨텍스트\n",
    "        { \"batch_size\": 1, \"n_heads\": 32, \"kv_seq_len\": 128000, \"d_head\": 128},\n",
    "        { \"batch_size\": 2, \"n_heads\": 32, \"kv_seq_len\": 128000, \"d_head\": 128},\n",
    "        # 128K tokens\n",
    "    ]\n",
    "\n",
    "    print(\"🚀 포괄적 Autotuned Flash Attention 벤치마크\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    for config in test_configs:\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        n_heads = config[\"n_heads\"]\n",
    "        kv_seq_len = config[\"kv_seq_len\"]\n",
    "        d_head = config[\"d_head\"]\n",
    "\n",
    "        # 메모리 계산\n",
    "        kv_memory_gb = (batch_size * n_heads * kv_seq_len * d_head * 2 * 4) / (1024 ** 3)\n",
    "\n",
    "        print(f\"📊 B={batch_size}, H={n_heads}, KV_len={kv_seq_len:,}, D={d_head}\")\n",
    "        print(f\"💾 KV Cache: {kv_memory_gb:.2f} GB\")\n",
    "\n",
    "        # 메모리 제한 체크\n",
    "        if kv_memory_gb > 24:  # 24GB 제한\n",
    "            print(\"⚠️ 메모리 제한으로 스킵\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # 데이터 준비\n",
    "            dtype = torch.float16 if kv_memory_gb > 4 else torch.float32\n",
    "            q = torch.randn(batch_size, n_heads, 1, d_head, device=device, dtype=dtype)\n",
    "            k = torch.randn(batch_size, n_heads, kv_seq_len, d_head, device=device, dtype=dtype)\n",
    "            v = torch.randn(batch_size, n_heads, kv_seq_len, d_head, device=device, dtype=dtype)\n",
    "\n",
    "            scale = 1.0 / math.sqrt(d_head)\n",
    "\n",
    "            methods = []\n",
    "            warmup = 3\n",
    "            runs = 10\n",
    "\n",
    "            # 1. Naive PyTorch\n",
    "            try:\n",
    "                for _ in range(warmup):\n",
    "                    _ = naive_attention(q, k, v, scale)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                start = time.time()\n",
    "                for _ in range(runs):\n",
    "                    out_naive = naive_attention(q, k, v, scale)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                naive_time = (time.time() - start) / runs\n",
    "                methods.append((\"Naive PyTorch\", naive_time, out_naive))\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Naive 실패: {e}\")\n",
    "\n",
    "            # 2. PyTorch Flash Attention 2\n",
    "            try:\n",
    "                for _ in range(warmup):\n",
    "                    _ = F.scaled_dot_product_attention(q, k, v, scale=scale)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                start = time.time()\n",
    "                for _ in range(runs):\n",
    "                    out_fa2 = F.scaled_dot_product_attention(q, k, v, scale=scale)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                fa2_time = (time.time() - start) / runs\n",
    "                methods.append((\"PyTorch FA2\", fa2_time, out_fa2))\n",
    "            except Exception as e:\n",
    "                print(f\"❌ PyTorch FA2 실패: {e}\")\n",
    "\n",
    "            # 3. Autotuned Flash Decode\n",
    "            try:\n",
    "                print(\"🔧 Triton autotune 진행 중...\")\n",
    "                for _ in range(warmup):\n",
    "                    _ = autotuned_flash_attn_decode(q, k, v, scale)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                start = time.time()\n",
    "                for _ in range(runs):\n",
    "                    out_auto = autotuned_flash_attn_decode(q, k, v, scale)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                auto_time = (time.time() - start) / runs\n",
    "                methods.append((\"Autotuned Triton\", auto_time, out_auto))\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Autotuned 실패: {e}\")\n",
    "\n",
    "            # 4. Mega Autotuned (배치 크기 >= 4만)\n",
    "            if batch_size >= 4:\n",
    "                try:\n",
    "                    print(\"🔧 Mega autotune 진행 중...\")\n",
    "                    for _ in range(warmup):\n",
    "                        _ = mega_autotuned_flash_attn_decode(q, k, v, scale)\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                    start = time.time()\n",
    "                    for _ in range(runs):\n",
    "                        out_mega = mega_autotuned_flash_attn_decode(q, k, v, scale)\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                    mega_time = (time.time() - start) / runs\n",
    "                    methods.append((\"Mega Autotuned\", mega_time, out_mega))\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Mega Autotuned 실패: {e}\")\n",
    "\n",
    "            # 결과 출력\n",
    "            if methods:\n",
    "                print(f\"\\n{'구현':<18} {'시간(ms)':<10} {'속도향상':<10} {'처리량(M tok/s)':<15} {'정확도'}\")\n",
    "                print(\"-\" * 78)\n",
    "\n",
    "                baseline_time = methods[0][1]\n",
    "                baseline_output = methods[0][2]\n",
    "\n",
    "                for name, exec_time, output in methods:\n",
    "                    speedup = f\"{baseline_time / exec_time:.1f}x\"\n",
    "\n",
    "                    # 토큰 처리량 (M tokens/sec)\n",
    "                    total_tokens = batch_size * kv_seq_len\n",
    "                    throughput_m = (total_tokens / exec_time) / 1e6\n",
    "\n",
    "                    # 정확도 체크\n",
    "                    try:\n",
    "                        if torch.allclose(output, baseline_output, rtol=1e-2, atol=1e-2):\n",
    "                            accuracy = \"✅\"\n",
    "                        else:\n",
    "                            diff = torch.max(torch.abs(output - baseline_output)).item()\n",
    "                            accuracy = f\"⚠️{diff:.1e}\"\n",
    "                    except:\n",
    "                        accuracy = \"❓\"\n",
    "\n",
    "                    print(f\"{name:<18} {exec_time * 1000:<9.1f} {speedup:<10} {throughput_m:<14.1f} {accuracy}\")\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"❌ GPU 메모리 부족\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테스트 실패: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_scale_benchmark()"
   ],
   "id": "5b3bf330eda154e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 포괄적 Autotuned Flash Attention 벤치마크\n",
      "====================================================================================================\n",
      "📊 B=1, H=12, KV_len=2,048, D=64\n",
      "💾 KV Cache: 0.01 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      0.2       1.0x       9.8            ✅\n",
      "PyTorch FA2        0.3       0.8x       7.5            ✅\n",
      "Autotuned Triton   0.1       3.6x       35.0           ✅\n",
      "📊 B=1, H=32, KV_len=8,192, D=128\n",
      "💾 KV Cache: 0.25 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      0.4       1.0x       19.4           ✅\n",
      "PyTorch FA2        1.4       0.3x       6.0            ✅\n",
      "Autotuned Triton   0.2       2.0x       38.5           ⚠️6.1e-02\n",
      "📊 B=1, H=40, KV_len=16,384, D=128\n",
      "💾 KV Cache: 0.62 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      1.0       1.0x       16.0           ✅\n",
      "PyTorch FA2        2.7       0.4x       6.0            ✅\n",
      "Autotuned Triton   0.5       2.0x       32.2           ⚠️3.7e+00\n",
      "📊 B=1, H=64, KV_len=32,768, D=128\n",
      "💾 KV Cache: 2.00 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      3.3       1.0x       10.0           ✅\n",
      "PyTorch FA2        5.5       0.6x       6.0            ✅\n",
      "Autotuned Triton   1.6       2.1x       20.7           ⚠️3.9e+01\n",
      "📊 B=4, H=96, KV_len=4,096, D=128\n",
      "💾 KV Cache: 1.50 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "🔧 Mega autotune 진행 중...\n",
      "❌ Mega Autotuned 실패: at 24:12:\n",
      "    \"\"\"\n",
      "    Mega Autotuned Flash Kernel\n",
      "    - 배치/헤드 블로킹도 자동 최적화\n",
      "    - 대규모 워크로드 특화\n",
      "    \"\"\"\n",
      "    pid = tl.program_id(0)\n",
      "\n",
      "    # 블록당 여러 배치×헤드 처리\n",
      "    for local_idx in range(BLOCK_BH):\n",
      "        bh_id = pid * BLOCK_BH + local_idx\n",
      "        if bh_id >= B * H:\n",
      "            break\n",
      "            ^\n",
      "unsupported AST node type: Break\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      2.4       1.0x       6.8            ✅\n",
      "PyTorch FA2        3.4       0.7x       4.8            ✅\n",
      "Autotuned Triton   1.2       2.0x       13.7           ⚠️5.2e+01\n",
      "📊 B=8, H=128, KV_len=8,192, D=128\n",
      "💾 KV Cache: 8.00 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "🔧 Mega autotune 진행 중...\n",
      "❌ Mega Autotuned 실패: at 24:12:\n",
      "    \"\"\"\n",
      "    Mega Autotuned Flash Kernel\n",
      "    - 배치/헤드 블로킹도 자동 최적화\n",
      "    - 대규모 워크로드 특화\n",
      "    \"\"\"\n",
      "    pid = tl.program_id(0)\n",
      "\n",
      "    # 블록당 여러 배치×헤드 처리\n",
      "    for local_idx in range(BLOCK_BH):\n",
      "        bh_id = pid * BLOCK_BH + local_idx\n",
      "        if bh_id >= B * H:\n",
      "            break\n",
      "            ^\n",
      "unsupported AST node type: Break\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      6.9       1.0x       9.6            ✅\n",
      "PyTorch FA2        6.4       1.1x       10.3           ✅\n",
      "Autotuned Triton   3.2       2.2x       20.7           ⚠️nan\n",
      "📊 B=32, H=64, KV_len=4,096, D=128\n",
      "💾 KV Cache: 8.00 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "🔧 Mega autotune 진행 중...\n",
      "❌ Mega Autotuned 실패: at 24:12:\n",
      "    \"\"\"\n",
      "    Mega Autotuned Flash Kernel\n",
      "    - 배치/헤드 블로킹도 자동 최적화\n",
      "    - 대규모 워크로드 특화\n",
      "    \"\"\"\n",
      "    pid = tl.program_id(0)\n",
      "\n",
      "    # 블록당 여러 배치×헤드 처리\n",
      "    for local_idx in range(BLOCK_BH):\n",
      "        bh_id = pid * BLOCK_BH + local_idx\n",
      "        if bh_id >= B * H:\n",
      "            break\n",
      "            ^\n",
      "unsupported AST node type: Break\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      6.6       1.0x       19.9           ✅\n",
      "PyTorch FA2        6.4       1.0x       20.6           ✅\n",
      "Autotuned Triton   3.2       2.1x       41.2           ⚠️8.2e-02\n",
      "📊 B=64, H=80, KV_len=8,192, D=128\n",
      "💾 KV Cache: 40.00 GB\n",
      "⚠️ 메모리 제한으로 스킵\n",
      "📊 B=128, H=96, KV_len=4,096, D=128\n",
      "💾 KV Cache: 48.00 GB\n",
      "⚠️ 메모리 제한으로 스킵\n",
      "📊 B=256, H=64, KV_len=2,048, D=128\n",
      "💾 KV Cache: 32.00 GB\n",
      "⚠️ 메모리 제한으로 스킵\n",
      "📊 B=1, H=32, KV_len=128,000, D=128\n",
      "💾 KV Cache: 3.91 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      6.4       1.0x       20.1           ✅\n",
      "PyTorch FA2        21.2      0.3x       6.0            ✅\n",
      "Autotuned Triton   3.1       2.1x       41.4           ⚠️1.5e-02\n",
      "📊 B=2, H=32, KV_len=128,000, D=128\n",
      "💾 KV Cache: 7.81 GB\n",
      "🔧 Triton autotune 진행 중...\n",
      "\n",
      "구현                 시간(ms)     속도향상       처리량(M tok/s)    정확도\n",
      "------------------------------------------------------------------------------\n",
      "Naive PyTorch      6.4       1.0x       40.0           ✅\n",
      "PyTorch FA2        6.2       1.0x       41.4           ✅\n",
      "Autotuned Triton   3.1       2.1x       83.0           ⚠️nan\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "49dfb74c8b6b7000"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
