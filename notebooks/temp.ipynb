{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T12:01:29.763298Z",
     "start_time": "2025-06-08T12:01:29.760657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "291c9420c48ffce8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T12:03:07.516102Z",
     "start_time": "2025-06-08T12:03:07.513435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_embed = 768\n",
    "rank = 64"
   ],
   "id": "b90415e313126776",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T12:03:07.678657Z",
     "start_time": "2025-06-08T12:03:07.670104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Wq = nn.Linear(d_embed + 1, d_embed, bias=False)\n",
    "Wk_up = nn.Linear(rank, d_embed, bias=False)\n",
    "x = torch.randn(seq_len, d_embed + 1)\n",
    "\n",
    "print(f\"x shape: {x.shape}\"\n",
    "      f\"\\nWq shape: {Wq.weight.shape}\"\n",
    "      f\"\\nWk_up shape: {Wk_up.weight.shape}\"\n",
    "      )\n",
    "\n",
    "y = torch.chain_matmul(x, Wq.weight.T, Wk_up.weight)\n",
    "print(f\"y shape: {y.shape}\")"
   ],
   "id": "1f5eb210c2f537a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([10, 769])\n",
      "Wq shape: torch.Size([768, 769])\n",
      "Wk_up shape: torch.Size([768, 64])\n",
      "y shape: torch.Size([10, 64])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        self.gate = nn.Linear(config.d_embed, config.n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, n_experts]\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        scores, indices = torch.topk(scores, self.top_k, dim=-1)  # [batch_size, seq_len, top_k]\n",
    "        return scores, indices\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.top_k = config.n_activated_experts\n",
    "        if config.n_experts is None or config.n_activated_experts is None:\n",
    "            raise ValueError(\"n_experts and n_activated_experts must be specified for MoE\")\n",
    "        if config.n_experts < config.n_activated_experts:\n",
    "            raise ValueError(\"n_experts must be greater than or equal to n_activated_experts\")\n",
    "        self.router = Router(config)\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "        if config.n_shared_experts is not None:\n",
    "            self.shared_experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_shared_experts)])\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        scores, indices = self.router(x)\n",
    "\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "        scores_flat = scores.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "        indices_flat = indices.view(-1, self.top_k)  # [batch * seq, top_k]\n",
    "\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (indices_flat == expert_idx)\n",
    "            selected_pos = mask.any(dim=-1)  # [batch * seq]\n",
    "\n",
    "            if selected_pos.any():\n",
    "                expert_input = x_flat[selected_pos]  # [top_k, d_embed]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                selected_mask_idx, selected_topk_idx = torch.where(mask)\n",
    "                gating_scores = scores_flat[selected_mask_idx, selected_topk_idx].unsqueeze(1)  # [top_k, 1]\n",
    "\n",
    "                weighted_output = expert_output * gating_scores  # [top_k, d_embed]\n",
    "                y_flat[selected_pos] += weighted_output  # [batch * seq, d_embed]\n",
    "\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = sum([expert(x_flat) for expert in self.shared_experts]) / len(self.shared_experts)\n",
    "            y_flat += shared_output  # [batch * seq, d_embed]\n",
    "\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n",
    "\n",
    "\n",
    "class RouterFreeMoE(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([FeedForward(config) for _ in range(config.n_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        x_flat = x.view(-1, d_embed)  # [batch * seq, d_embed]\n",
    "\n",
    "        # Step 1: Expert selection\n",
    "        deltas = []\n",
    "        with torch.no_grad():\n",
    "            # if Device compute capabilities is 8.9 or higher, use fp8_autocast\n",
    "            if torch.cuda.get_device_capability()[0] >= 8 and torch.cuda.get_device_capability()[1] >= 9:\n",
    "                #ctx = fp8_autocast(enabled=True)\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            else:\n",
    "                ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "            with ctx:\n",
    "                for expert in self.experts:\n",
    "                    expert_output = expert(x_flat)  # [batch * seq, d_embed]\n",
    "                    delta = (expert_output - x_flat).norm(dim=-1)  # [batch * seq]\n",
    "                    deltas.append(delta)\n",
    "            deltas = torch.stack(deltas, dim=0)  # [n_experts, batch * seq]\n",
    "            best_expert_idx = deltas.argmax(dim=0)  # [batch * seq]\n",
    "\n",
    "        # Step 2: Apply the best expert\n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = (best_expert_idx == expert_idx)  # [batch * seq]\n",
    "            if mask.any():\n",
    "                selected_input = x_flat[mask]  # [num_selected, d_embed]\n",
    "                selected_output = expert(selected_input)  # [num_selected, d_embed]\n",
    "                y_flat[mask] = selected_output  # [batch * seq, d_embed]\n",
    "        y = y_flat.view(batch_size, seq_len, d_embed)\n",
    "        return y\n"
   ],
   "id": "dc64b93a3a483252"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
