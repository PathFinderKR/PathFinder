{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 모델 로딩\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# 2. LayerNorm 후 입력 샘플링\n",
    "def sample_normalized_x(seq_len, dim, batch_size):\n",
    "    return torch.randn(batch_size, seq_len, dim)  # LayerNorm 이후 가정: mean=0, std=1\n",
    "\n",
    "# 3. Attention logits 계산 (GPT-2의 Wq, Wk 사용)\n",
    "def get_attention_logits_max_gpt2(model, x, layer=0):\n",
    "    attn = model.h[layer].attn  # GPT-2 block의 attention\n",
    "    Wq = attn.c_attn.weight.data[:, :model.config.n_embd]           # Q part\n",
    "    Wk = attn.c_attn.weight.data[:, model.config.n_embd:2*model.config.n_embd]  # K part\n",
    "\n",
    "    # Q, K 계산\n",
    "    Q = x @ Wq  # [B, T, D]\n",
    "    K = x @ Wk  # [B, T, D]\n",
    "\n",
    "    # Head 분리\n",
    "    num_heads = attn.num_heads\n",
    "    head_dim = model.config.n_embd // num_heads\n",
    "\n",
    "    Q = Q.view(x.size(0), x.size(1), num_heads, head_dim).transpose(1, 2)  # [B, H, T, D]\n",
    "    K = K.view(x.size(0), x.size(1), num_heads, head_dim).transpose(1, 2)  # [B, H, T, D]\n",
    "\n",
    "    # Attention logits\n",
    "    attn_logits = Q @ K.transpose(-2, -1) / (head_dim ** 0.5)  # [B, H, T, T]\n",
    "    max_logits = attn_logits.max(dim=-1).values  # [B, H, T]\n",
    "\n",
    "    return max_logits.flatten().detach().cpu().numpy()\n",
    "\n",
    "# 4. 시뮬레이션 및 분포 시각화\n",
    "def simulate_and_plot_gpt2():\n",
    "    seq_len = 10\n",
    "    dim = model.config.n_embd\n",
    "    batch_size = 64\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(100):  # 반복 샘플링\n",
    "        x = sample_normalized_x(seq_len, dim, batch_size)\n",
    "        max_logits = get_attention_logits_max_gpt2(model, x, layer=0)\n",
    "        samples.extend(max_logits)\n",
    "\n",
    "    plt.hist(samples, bins=100, density=True)\n",
    "    plt.title(\"GPT-2 Max Attention Logits (Pre-Softmax)\")\n",
    "    plt.xlabel(\"Max Attention Logit\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "simulate_and_plot_gpt2()"
   ],
   "id": "6996034ab1e2160b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a1c0aa5bc2bdc035"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
